{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uber Driver Data Analysis\n",
    "\n",
    "Works with SAR and Portal data.\n",
    "\n",
    "Note that this notebook is not final, and more documentation is being added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing the required libraries with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jupyter numpy pandas portion geopy openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from configparser import ConfigParser\n",
    "from typing import Callable, Any\n",
    "\n",
    "import geopy.distance\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_facets = ['duration_h', 'distance_km', 'uber_paid']\n",
    "all_time_properties: dict[str, Callable[[Timestamp], Any]] = {\n",
    "    'day_of_week': lambda d: d.day_name(),\n",
    "    'day_type': lambda d: 'weekday' if d.weekday() < 5 else 'weekend',\n",
    "    'time_of_day': lambda d: 'AM' if d.hour < 12 else 'PM',\n",
    "    'night': lambda d: 'night' if d.hour <= 6 or 23 < d.hour else 'day',\n",
    "    'sunday': lambda d: 'sunday' if d.day_name() == 'Sunday' else 'weekday'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        table: Table['t1': Timestamp, 't2': Timestamp, 't3': Timestamp],\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    :param: table: a table having a number N > 1 of time-columns and L of entries.\n",
    "    :param: columns: a list of n time-column names present in {table}.\n",
    "    :param: extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    :return: periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Usage:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(table.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lifetime_trips(zf: ZipFile, timezone: str, pattern: str = '*Driver Lifetime Trips.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, zf,\n",
    "                       ['request_timestamp_utc', 'begintrip_timestamp_utc', 'dropoff_timestamp_utc', 'status',\n",
    "                        'request_to_begin_distance_miles', 'trip_distance_miles', 'original_fare_local'])\n",
    "    table = table[table.status == 'completed'].drop(columns='status')\n",
    "    table.replace({r'\\N': np.nan}, inplace=True)\n",
    "    for col in ['request_timestamp_utc', 'begintrip_timestamp_utc', 'dropoff_timestamp_utc']:\n",
    "        table[col] = pd.to_datetime(table[col], utc=True).dt.tz_convert(timezone)\n",
    "    for col in ['request_to_begin_distance_miles', 'original_fare_local']:\n",
    "        table[col] = table[col].astype(float)\n",
    "    table = time_tuples_to_periods(\n",
    "        table,\n",
    "        columns=['request_timestamp_utc', 'begintrip_timestamp_utc', 'dropoff_timestamp_utc'],\n",
    "        extra_info=[\n",
    "            lambda r: {'status': 'P2', 'distance_km': mile2km(r['request_to_begin_distance_miles']), 'file': r['file']},\n",
    "            lambda r: {'status': 'P3', 'distance_km': mile2km(r['trip_distance_miles']), 'file': r['file'],\n",
    "                       'uber_paid': r['original_fare_local']}])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_on_off(zf: ZipFile, timezone: str, pattern: str = '*Driver Online Offline.csv',\n",
    "                birdeye_coefficient: float = 1.5) -> PeriodTable:\n",
    "    table = find_table(pattern, zf,\n",
    "                       ['begin_timestamp_utc', 'end_timestamp_utc', 'earner_state',\n",
    "                        'begin_lat', 'begin_lng', 'end_lat', 'end_lng'])\n",
    "    table.rename(columns={'begin_timestamp_utc': 'begin', 'end_timestamp_utc': 'end',\n",
    "                          'earner_state': 'status'}, inplace=True)\n",
    "    table = table.replace({r'\\N': np.nan,\n",
    "                           'ontrip': 'P3', 'on trip': 'P3',\n",
    "                           'enroute': 'P2', 'en route': 'P2',\n",
    "                           'open': 'P1', 'offline': 'P0'})\n",
    "    # TODO decide if this should be kept given the new logic\n",
    "    table = table[table.status != 'P3']\n",
    "    gps_cols = ['begin_lat', 'begin_lng', 'end_lat', 'end_lng']\n",
    "    for col in gps_cols:\n",
    "        table[col] = table[col].astype(float)\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col], utc=True).dt.tz_convert(timezone)\n",
    "    table = table.dropna()\n",
    "    table['distance_km'] = table.apply(\n",
    "        lambda r: birdeye_coefficient * geopy.distance.geodesic((r['begin_lat'], r['begin_lng']),\n",
    "                                                                (r['end_lat'], r['end_lng'])).km, axis=1)\n",
    "    table.drop(columns=gps_cols, inplace=True)\n",
    "    return table.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sar(zip_path: Path, timezone: str = 'Europe/Zurich') -> Table:\n",
    "    with ZipFile(zip_path) as zf:\n",
    "        print(f'Zip open')\n",
    "        lifetime_trips = load_lifetime_trips(zf, timezone)\n",
    "        on_off = load_on_off(zf, timezone)\n",
    "    print(f'Zip closed')\n",
    "    return pd.concat([lifetime_trips, on_off]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_portal(zip_path: Path, timezone: str = 'Europe/Zurich'):\n",
    "    with ZipFile(zip_path) as zf:\n",
    "        print(f'Zip open')\n",
    "        table = find_table('*driver_lifetime_trips*.csv', zf,\n",
    "                           ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                            'Local Dropoff Timestamp',\n",
    "                            'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    print(f'Zip closed')\n",
    "    table = table[table['Status'] == 'completed']\n",
    "    for col in ['Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp']:\n",
    "        table[col] = pd.to_datetime(table[col]).dt.tz_convert(timezone)\n",
    "    table = time_tuples_to_periods(\n",
    "        table, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp'],\n",
    "        extra_info=[lambda r: {'status': 'P2'},\n",
    "                    lambda r: {'status': 'P3', 'distance_km': mile2km(r['Trip Distance (miles)']),\n",
    "                               'uber_paid': r['Local Original Fare']}])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver-specific logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: Table,\n",
    "        percentage_df_path: Optional[str | Path] = None\n",
    ") -> Table:\n",
    "    filtered = pd.DataFrame(index=daily.index)\n",
    "    filtered['datetime'] = pd.to_datetime(daily['date'])\n",
    "    duration_P1_cols = list(filter(lambda col: all(f in col for f in ['duration_h', 'P1']), daily.columns))\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    if percentage_df_path is not None:\n",
    "        percentage = pd.read_csv(percentage_df_path)\n",
    "        percentage['Uber'] /= 100\n",
    "        for c in duration_P1_cols:\n",
    "            for i, row in percentage.iterrows():\n",
    "                filtered[c] = np.where(\n",
    "                    (filtered.datetime.dt.year == row.year) & (filtered.datetime.dt.month == row.month),\n",
    "                    daily[c] * row['Uber'], daily[c])\n",
    "\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [dt.date(2020, 11, 26), *date_range((2020, 12, 21), (2020, 12, 25)),\n",
    "                     *date_range((2021, 2, 1), (2021, 2, 12)), *date_range((2021, 8, 16), (2021, 8, 28)),\n",
    "                     *date_range((2021, 9, 20), (2021, 10, 3)), *date_range((2021, 11, 25), (2021, 12, 12)),\n",
    "                     *date_range((2022, 4, 25), (2022, 5, 13))]\n",
    "    duration_P1_weekday_AM_cols = list(\n",
    "        filter(lambda col: all(f in col for f in ['duration_h', 'P1', 'AM', 'weekday']), daily.columns))\n",
    "    filtered.loc[\n",
    "        daily[~filtered.datetime.apply(lambda d: d.date()).isin(dates_to_keep)].index, duration_P1_weekday_AM_cols] = 0\n",
    "    return filtered[duration_P1_cols].rename(columns={col: f'{col}(filtered)' for col in duration_P1_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method merges intervals according to the following logic:\n",
    "(NOTE: this is out of date and the first case is now weighted differently)\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Here, the blue status has a higher priority than the orange one (e.g. blue = P3 > P2 = orange).\n",
    "\n",
    "Note that the sum and max logic could be customized per attribute (possible for the sum part but not the max one at the moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_logic(a: dict['begin', 'end', '...'], b: dict['begin', 'end', '...'], agg=max) -> dict['begin', 'end', '...']:\n",
    "    \"\"\"\n",
    "    Combines numerical values from two different intervals in a \"fair\" way by using the original values where the intervals don't\n",
    "    overlap, and using an aggregation of both values at the intersection (by default the aggregation is the maximum).\n",
    "    \"\"\"\n",
    "    # TODO change function name, not clear\n",
    "    ab, ae, bb, be = [ts.astimezone(dt.timezone.utc) for ts in [a['begin'], a['end'], b['begin'], b['end']]]\n",
    "    d_union = (be - ab).seconds\n",
    "    d_inter = (ae - bb).seconds\n",
    "    da = (ae - ab).seconds - d_inter\n",
    "    db = (be - bb).seconds - d_inter\n",
    "\n",
    "    weighted = {k: (a[k] * da + agg(a[k], b[k]) * d_inter + b[k] * db) / d_union\n",
    "                for k in a.keys() if k in all_facets} if d_union != 0 else {}\n",
    "    return {'begin': a['begin'], 'end': b['end'], **weighted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Merges overlapping entries \"efficiently\".\n",
    "    :param table: the table whose intervals should be merged\n",
    "    :return: a table with merged intervals\n",
    "    \"\"\"\n",
    "    status_in_order: list['str'] = ['P3', 'P2', 'P1', 'unavailable', 'P0']\n",
    "    priority: dict[str, int] = dict(zip(status_in_order, range(len(status_in_order), 0, -1)))\n",
    "    table = table.sort_values(['begin', 'end'])\n",
    "    groups: list[dict] = []\n",
    "    group: dict = table.iloc[0].to_dict()\n",
    "    for i, row in table.iloc[1:].iterrows():\n",
    "        # Group and row overlap\n",
    "        if row['begin'].astimezone(dt.timezone.utc) <= group['end'].astimezone(dt.timezone.utc):\n",
    "            # TODO make this part more understandable\n",
    "            if (row['begin'], row['end']) == (group['begin'], group['end']):  # Exactly the same interval\n",
    "                group.update(merge_logic(group, row.to_dict()))\n",
    "                if priority[row['status']] > priority[group['status']]:\n",
    "                    group['status'] = row['status']\n",
    "            else:\n",
    "                if row['status'] == group['status']:\n",
    "                    group.update(merge_logic(group, row.to_dict()))\n",
    "                else:  # different statuses\n",
    "                    if priority[row['status']] > priority[group['status']]:\n",
    "                        group['end'] = row['begin']\n",
    "                    else:\n",
    "                        row['begin'] = group['end']\n",
    "                    groups.append(group)\n",
    "                    group = row.to_dict()\n",
    "        else:\n",
    "            groups.append(group)\n",
    "            group = row.to_dict()\n",
    "\n",
    "    return pd.DataFrame.from_records(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hours(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many hours periods in as many intervals as hours covered by the interval.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def f(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_tz = begin.tzinfo\n",
    "        begin = begin.astimezone(dt.timezone.utc)\n",
    "        end = end.astimezone(dt.timezone.utc)\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        rows = []\n",
    "        if begin.hour != end.hour or begin.day != end.day:\n",
    "            rows.append(scaled_interval(begin, begin.replace(minute=59, second=59), rest, og_duration))\n",
    "            for hours in range(1, math.floor((end - begin) / dt.timedelta(hours=1)) - 1):\n",
    "                mid = begin + dt.timedelta(hours=hours)\n",
    "                rows.append(scaled_interval(mid.replace(minute=0, second=0),\n",
    "                                            mid.replace(minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(minute=0, second=0), end, rest, og_duration))\n",
    "        else:\n",
    "            rows.append({'begin': begin, 'end': end, **rest})\n",
    "        return [{**{k: ts.astimezone(og_tz) for k, ts in select(d, keep=['begin', 'end']).items()},\n",
    "                 **select(d, drop=['begin', 'end'])} for d in rows]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in f(**d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: PeriodTable,\n",
    "        save_folder: Path,\n",
    "        interval_logic: Optional[Callable[[PeriodTable], PeriodTable]] = merge_overlapping_intervals,\n",
    "        filtering_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        time_properties: Optional[dict[str, Callable[[Timestamp], Any]]] = None,\n",
    "        name: str = 'analysis',\n",
    "        facets: list[str] = all_facets,\n",
    "        compute_most_lucrative_months: bool = False,\n",
    "        save_periods: bool = True,\n",
    "):\n",
    "    # Apply interval logic if specified\n",
    "    if interval_logic is not None:\n",
    "        periods = interval_logic(periods)\n",
    "    # Split intervals spanning many hours\n",
    "    periods = split_hours(periods)\n",
    "    # Compute the duration of each split period\n",
    "    periods['duration_h'] = (periods.end.dt.tz_convert('UTC') - periods.begin.dt.tz_convert('UTC')) / dt.timedelta(\n",
    "        hours=1)\n",
    "\n",
    "    # Compute these datetime properties since they will be the same for begin and end (thanks to split_hours)\n",
    "    periods['hour'] = periods.end.apply(lambda d: f'{d.hour}-{(d + dt.timedelta(hours=1)).hour}')\n",
    "    periods['date'] = periods.end.dt.date.astype(str)\n",
    "    periods['week'] = periods.end.apply(find_week_limits)\n",
    "    periods['month'] = periods.end.apply(lambda d: f'{d.month:02d}. {d.month_name()}')\n",
    "    periods['year'] = periods.end.dt.year\n",
    "\n",
    "    pivoting_cols = ['status']\n",
    "    if time_properties:\n",
    "        for k, f in time_properties.items():\n",
    "            periods[k] = periods.end.apply(f)\n",
    "        pivoting_cols += list(time_properties.keys())\n",
    "\n",
    "    # No use in them being actual dates anymore, and we need strings to write to excel\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = periods[col].dt.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "    # Pivot table so that each there is a single line per interval and per granularity of interest\n",
    "    # TODO Note: this should be pivot and not pivot_table, but there are (begin, end, status) duplicates in the data\n",
    "    # TODO which shouldn't happen, so the duplicates are merged by averaging other values but it doesn't make sense.\n",
    "    # We have to fillNa first otherwise the margins are not computed correctly\n",
    "    periods = periods.fillna(0).pivot_table(index=['begin', 'end', 'hour', 'date', 'week', 'month', 'year'],\n",
    "                                            columns=pivoting_cols, values=facets, aggfunc='max',\n",
    "                                            margins=True, margins_name='total').reset_index()\n",
    "    # TODO remove this line if margins=True is NOT given to the above pivot_table call\n",
    "    periods.drop(periods.index[-1], inplace=True)\n",
    "    # Next 2 lines remove columns that have only 0/nans\n",
    "    periods[periods == 0] = np.nan\n",
    "    periods.drop(columns=periods.columns[periods.isna().all()], inplace=True)\n",
    "\n",
    "    # Add filtered columns if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        periods = pd.concat([periods, filtering_logic(periods)], axis=1)\n",
    "\n",
    "    agg_dict = {c: sum for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    hourly = periods.groupby(['year', 'month', 'week', 'date', 'hour']).agg(agg_dict).reset_index()\n",
    "    daily = hourly.groupby(['year', 'month', 'week', 'date']).agg(agg_dict).reset_index()\n",
    "    weekly = daily.groupby(['week']).agg(agg_dict).reset_index()\n",
    "    monthly = daily.groupby(['year', 'month']).agg(agg_dict).reset_index()\n",
    "    yearly = monthly.groupby(['year']).agg(agg_dict).reset_index()\n",
    "    total = yearly.agg(agg_dict).to_frame().T\n",
    "\n",
    "    hourly.drop(columns=['year', 'month', 'week'], inplace=True)\n",
    "    daily.drop(columns=['year', 'month'], inplace=True)\n",
    "\n",
    "    tabs = {'hourly': hourly, 'daily': daily, 'weekly': weekly, 'monthly': monthly,\n",
    "            'yearly': yearly, 'total': total}\n",
    "\n",
    "    save_folder.mkdir(parents=True, exist_ok=True)\n",
    "    if save_periods:\n",
    "        periods.to_csv(save_folder / f'{name}_periods.csv')\n",
    "    if compute_most_lucrative_months:\n",
    "        print('Computing lucrative months')\n",
    "        df = monthly.copy()\n",
    "        df = df.sort_values([('uber_paid', 'total')], ascending=False)\n",
    "        tabs['most_lucrative_months'] = df\n",
    "        request_months = list(df.iloc[:10].sort_values(['year', 'month']).apply(\n",
    "            lambda r: f'{french_months[int(r.month[0].split(\".\")[0])]} {r.year[0]}', axis=1))\n",
    "        request_months += list(\n",
    "            filter(lambda s: s not in request_months, [f'{french_months[i]} 2020' for i in range(3, 12 + 1)]))\n",
    "        tabs['sar_request_text'] = pd.DataFrame(\n",
    "            [{'text': sar_text.replace('{REPLACE_HERE}', ', '.join(request_months))}])\n",
    "\n",
    "    print('Writing Excel file')\n",
    "    save_excel(save_folder / f'{name}_results.xlsx', tabs)\n",
    "    print('Done!')\n",
    "    return tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_config():\n",
    "    config = ConfigParser()\n",
    "    config.read(filenames='config.ini')\n",
    "    for section in re.split(',\\s*', config.get('global', 'run')):\n",
    "        in_path = config.get(section, 'input_zipfile')\n",
    "        out_path = config.get(section, 'output_folder')\n",
    "        timezone = config.get(section, 'timezone')\n",
    "        facets = re.split(',\\s*', config.get(section, 'facets'))\n",
    "        time_properties = select(all_time_properties, re.split(',\\s*', config.get(section, 'time_properties')))\n",
    "\n",
    "        match config.get(section, 'type'):\n",
    "            case 'SAR':\n",
    "                loader = load_sar\n",
    "            case 'Portal':\n",
    "                loader = load_portal\n",
    "            case other:\n",
    "                raise ValueError(f'[{section}].type must be specified and be one of SAR or Portal. Found \"{other}\"')\n",
    "\n",
    "        print(f\"{'-' * 20} Running {section} {'-' * 20}\")\n",
    "        periods = loader(in_path, timezone)\n",
    "        pipeline(periods, save_folder=Path(out_path), name=section,\n",
    "                 facets=facets, time_properties=time_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_zips_in_folder(data_folder: Path = Path(os.getcwd()) / 'data', ignore: list[str] = []):\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith('.zip'):\n",
    "            name = filename.split('.zip')[0]\n",
    "            if filename in ignore:\n",
    "                continue\n",
    "            if 'SAR' in name:\n",
    "                loader = load_sar\n",
    "            elif 'Portal' in name:\n",
    "                loader = load_portal\n",
    "            else:\n",
    "                print(name)\n",
    "                continue\n",
    "\n",
    "            print(f\"{'-' * 20} Running {filename} {'-' * 20}\")\n",
    "            periods = loader(data_folder / filename)\n",
    "            compute_most_lucrative_months = 'Portal' in name\n",
    "            pipeline(periods, save_folder=data_folder / 'results', name=name,\n",
    "                     facets=all_facets, time_properties=select(all_time_properties, ['sunday', 'night']),\n",
    "                     compute_most_lucrative_months=compute_most_lucrative_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
