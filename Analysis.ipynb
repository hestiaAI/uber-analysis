{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Uber Driver Data Analysis\n",
    "\n",
    "(New version)\n",
    "\n",
    "Works with SAR and Portal data.\n",
    "\n",
    "Note that this notebook is not final, and more documentation is being added."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing the required libraries with pip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install numpy pandas portion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Tuple\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import portion as P"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "class Table(pd.DataFrame):\n",
    "    \"\"\"Some Python magic to be able to type hint things like df: Table['begin': datetime, 'end': datetime]\"\"\"\n",
    "    __class_getitem__ = classmethod(type(list[str]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "IntervalTable = Table['begin': datetime, 'end': datetime]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'\n",
    "raw_folder = data_folder / 'raw'\n",
    "processed_folder = data_folder / 'processed'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def find_file(pattern: str, folder: Path) -> Path:\n",
    "    matches = glob.glob(pattern, root_dir=folder)\n",
    "    if len(matches) == 0:\n",
    "        raise ValueError(f'Could not find file {pattern} in {folder}')\n",
    "    elif len(matches) > 1:\n",
    "        print(f'Found many matches for {pattern} in {folder}. Using the first.')\n",
    "    return folder / matches[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def find_dataframe(pattern: str, folder: Path, usecols: list[str]) -> pd.DataFrame:\n",
    "    file = find_file(pattern, folder)\n",
    "    return pd.read_csv(file, usecols=usecols)[usecols].assign(file=file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def df_to_interval(df: IntervalTable) -> P.interval:\n",
    "    \"\"\"Converts a DataFrame with columns 'begin' and 'end' into a Portion interval, merging entries that overlap.\"\"\"\n",
    "    return P.Interval(*[P.closed(row['begin'], row['end']) for row in df.to_dict('records')])\n",
    "\n",
    "\n",
    "def interval_to_df(interval: P.interval) -> IntervalTable:\n",
    "    \"\"\"Converts a Portion interval into a dataframe with columns 'begin' and 'end'.\"\"\"\n",
    "    return pd.DataFrame([{'begin': begin, 'end': end} for _, begin, end, _ in P.to_data(interval)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(df: IntervalTable, agg_dict: Optional[dict] = None) -> IntervalTable:\n",
    "    agg_dict = agg_dict or {}\n",
    "\n",
    "    def handle_groups(df: IntervalTable) -> IntervalTable:\n",
    "        intervals = df[['begin', 'end']].sort_values(['begin', 'end'])\n",
    "        group = 0\n",
    "        group_end = intervals.end.iloc[0]\n",
    "\n",
    "        def find_group(row):\n",
    "            nonlocal group, group_end\n",
    "            if row.begin <= group_end:\n",
    "                group_end = max(row.end, group_end)\n",
    "            else:\n",
    "                group += 1\n",
    "                group_end = row.end\n",
    "            return group\n",
    "\n",
    "        groups = intervals.apply(find_group, axis=1)\n",
    "        return df.groupby(groups).agg({'begin': 'min', 'end': 'max', **agg_dict}).reset_index(drop=True)\n",
    "\n",
    "    return df.groupby('status').apply(handle_groups).reset_index(level=1, drop=True).reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def union(dfl: IntervalTable, dfr: IntervalTable, agg_dict: Optional[dict] = None) -> IntervalTable:\n",
    "    dfl = dfl.sort_values(['begin', 'end'])\n",
    "    dfr = dfr.sort_values(['begin', 'end'])\n",
    "    return dfl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def subtract(dfl: IntervalTable, dfr: IntervalTable) -> IntervalTable:\n",
    "    res = dfl.copy()\n",
    "    for (il, rl), (ir, rr) in product(dfl.iterrows(), dfr.iterrows()):\n",
    "        if rr.begin <= rl.begin and rl.end <= rr.end:  # The left interval was contained in the right so we discard it\n",
    "            res.drop(il, inplace=True)\n",
    "        elif rr.begin <= rl.begin or rl.end <= rr.end:  # The left-right intersection is non-empty\n",
    "            res.loc[il, 'begin'] = max(rl.begin, rr.begin)\n",
    "            res.loc[il, 'end'] = min(rl.end, rr.end)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "                         begin                       end status  distance_km  \\\n0    2022-05-29 10:12:49+00:00 2022-05-29 10:18:57+00:00     P2          NaN   \n1    2022-05-29 10:18:57+00:00 2022-05-29 10:27:47+00:00     P3     2.052856   \n2    2022-05-28 12:04:08+00:00 2022-05-28 12:11:05+00:00     P2          NaN   \n3    2022-05-28 12:11:05+00:00 2022-05-28 12:22:45+00:00     P3     3.368586   \n4    2022-05-22 11:19:27+00:00 2022-05-22 11:23:43+00:00     P2          NaN   \n...                        ...                       ...    ...          ...   \n9973 2017-11-02 15:18:23+00:00 2017-11-02 15:28:43+00:00     P3     4.524063   \n9974 2017-11-02 14:55:09+00:00 2017-11-02 15:00:16+00:00     P2          NaN   \n9975 2017-11-02 15:00:16+00:00 2017-11-02 15:04:32+00:00     P3     1.132339   \n9976 2017-11-02 14:27:40+00:00 2017-11-02 14:36:01+00:00     P2          NaN   \n9977 2017-11-02 14:36:01+00:00 2017-11-02 14:48:40+00:00     P3     2.929388   \n\n      uber_pay  \n0          NaN  \n1        25.34  \n2          NaN  \n3        27.32  \n4          NaN  \n...        ...  \n9973     14.24  \n9974       NaN  \n9975      6.32  \n9976       NaN  \n9977     12.07  \n\n[9978 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>begin</th>\n      <th>end</th>\n      <th>status</th>\n      <th>distance_km</th>\n      <th>uber_pay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-05-29 10:12:49+00:00</td>\n      <td>2022-05-29 10:18:57+00:00</td>\n      <td>P2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-05-29 10:18:57+00:00</td>\n      <td>2022-05-29 10:27:47+00:00</td>\n      <td>P3</td>\n      <td>2.052856</td>\n      <td>25.34</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-05-28 12:04:08+00:00</td>\n      <td>2022-05-28 12:11:05+00:00</td>\n      <td>P2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-05-28 12:11:05+00:00</td>\n      <td>2022-05-28 12:22:45+00:00</td>\n      <td>P3</td>\n      <td>3.368586</td>\n      <td>27.32</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-05-22 11:19:27+00:00</td>\n      <td>2022-05-22 11:23:43+00:00</td>\n      <td>P2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9973</th>\n      <td>2017-11-02 15:18:23+00:00</td>\n      <td>2017-11-02 15:28:43+00:00</td>\n      <td>P3</td>\n      <td>4.524063</td>\n      <td>14.24</td>\n    </tr>\n    <tr>\n      <th>9974</th>\n      <td>2017-11-02 14:55:09+00:00</td>\n      <td>2017-11-02 15:00:16+00:00</td>\n      <td>P2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9975</th>\n      <td>2017-11-02 15:00:16+00:00</td>\n      <td>2017-11-02 15:04:32+00:00</td>\n      <td>P3</td>\n      <td>1.132339</td>\n      <td>6.32</td>\n    </tr>\n    <tr>\n      <th>9976</th>\n      <td>2017-11-02 14:27:40+00:00</td>\n      <td>2017-11-02 14:36:01+00:00</td>\n      <td>P2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9977</th>\n      <td>2017-11-02 14:36:01+00:00</td>\n      <td>2017-11-02 14:48:40+00:00</td>\n      <td>P3</td>\n      <td>2.929388</td>\n      <td>12.07</td>\n    </tr>\n  </tbody>\n</table>\n<p>9978 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_portal(raw_folder / 'portal' / 'Driver')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "24890121"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4989 ** 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subtract(df[df.status == 'P2'], df[df.status == 'P3'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.53 s, sys: 39.5 ms, total: 2.57 s\n",
      "Wall time: 2.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "     status                     begin                       end  distance_km  \\\n0        P2 2017-11-02 14:27:40+00:00 2017-11-02 14:36:01+00:00     0.000000   \n1        P2 2017-11-02 14:55:09+00:00 2017-11-02 15:00:16+00:00     0.000000   \n2        P2 2017-11-02 15:11:51+00:00 2017-11-02 15:18:23+00:00     0.000000   \n3        P2 2017-11-02 15:53:50+00:00 2017-11-02 15:58:23+00:00     0.000000   \n4        P2 2017-11-03 12:30:20+00:00 2017-11-03 12:37:22+00:00     0.000000   \n...     ...                       ...                       ...          ...   \n9940     P3 2022-05-22 10:30:12+00:00 2022-05-22 10:38:14+00:00     3.838262   \n9941     P3 2022-05-22 11:04:37+00:00 2022-05-22 11:16:14+00:00     8.338381   \n9942     P3 2022-05-22 11:23:43+00:00 2022-05-22 11:41:01+00:00     7.053083   \n9943     P3 2022-05-28 12:11:05+00:00 2022-05-28 12:22:45+00:00     3.368586   \n9944     P3 2022-05-29 10:18:57+00:00 2022-05-29 10:27:47+00:00     2.052856   \n\n      uber_pay  \n0         0.00  \n1         0.00  \n2         0.00  \n3         0.00  \n4         0.00  \n...        ...  \n9940     36.00  \n9941     34.58  \n9942     35.85  \n9943     27.32  \n9944     25.34  \n\n[9945 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>status</th>\n      <th>begin</th>\n      <th>end</th>\n      <th>distance_km</th>\n      <th>uber_pay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P2</td>\n      <td>2017-11-02 14:27:40+00:00</td>\n      <td>2017-11-02 14:36:01+00:00</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P2</td>\n      <td>2017-11-02 14:55:09+00:00</td>\n      <td>2017-11-02 15:00:16+00:00</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P2</td>\n      <td>2017-11-02 15:11:51+00:00</td>\n      <td>2017-11-02 15:18:23+00:00</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P2</td>\n      <td>2017-11-02 15:53:50+00:00</td>\n      <td>2017-11-02 15:58:23+00:00</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P2</td>\n      <td>2017-11-03 12:30:20+00:00</td>\n      <td>2017-11-03 12:37:22+00:00</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9940</th>\n      <td>P3</td>\n      <td>2022-05-22 10:30:12+00:00</td>\n      <td>2022-05-22 10:38:14+00:00</td>\n      <td>3.838262</td>\n      <td>36.00</td>\n    </tr>\n    <tr>\n      <th>9941</th>\n      <td>P3</td>\n      <td>2022-05-22 11:04:37+00:00</td>\n      <td>2022-05-22 11:16:14+00:00</td>\n      <td>8.338381</td>\n      <td>34.58</td>\n    </tr>\n    <tr>\n      <th>9942</th>\n      <td>P3</td>\n      <td>2022-05-22 11:23:43+00:00</td>\n      <td>2022-05-22 11:41:01+00:00</td>\n      <td>7.053083</td>\n      <td>35.85</td>\n    </tr>\n    <tr>\n      <th>9943</th>\n      <td>P3</td>\n      <td>2022-05-28 12:11:05+00:00</td>\n      <td>2022-05-28 12:22:45+00:00</td>\n      <td>3.368586</td>\n      <td>27.32</td>\n    </tr>\n    <tr>\n      <th>9944</th>\n      <td>P3</td>\n      <td>2022-05-29 10:18:57+00:00</td>\n      <td>2022-05-29 10:27:47+00:00</td>\n      <td>2.052856</td>\n      <td>25.34</td>\n    </tr>\n  </tbody>\n</table>\n<p>9945 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_overlapping_intervals(df, {'distance_km': 'sum', 'uber_paid': 'sum'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specific to SAR data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def make_status_intervals(df: IntervalTable['status': str]) -> dict[str, P.interval]:\n",
    "    return {s: df_to_interval(df[df.status == s]) for s in df.status.unique()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def interval_merge_logic(lt: dict[str, P.interval], oo: dict[str, P.interval]) -> dict[str, pd.DataFrame]:\n",
    "    P3 = lt['P3'] | oo['P3']\n",
    "    P2 = (lt['P2'] | oo['P2']) - P3\n",
    "    P1 = oo['P1'] - (P2 | P3)\n",
    "    return {'P1': P1, 'P2': P2, 'P3': P3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main_interval_logic(lt: dict[str, P.interval], oo: dict[str, P.interval], P0_has_priority=False) -> pd.DataFrame:\n",
    "    \"\"\"Consider the following ordering of priorities: P3 > P2 > P1. P0_has_priority determines if P0 is on the left or right of inequalities.\"\"\"\n",
    "    if P0_has_priority:\n",
    "        for d in [lt, oo]:\n",
    "            for k in d.keys():\n",
    "                if k != 'P0':\n",
    "                    d[k] = d[k] - oo['P0']\n",
    "    lt['P2'] = lt['P2'] - lt['P3']\n",
    "    oo['P2'] = oo['P2'] - oo['P3']\n",
    "    oo['P1'] = oo['P1'] - (oo['P2'] | oo['P3'])\n",
    "    intervals = interval_merge_logic(lt, oo)\n",
    "    return pd.concat([interval_to_df(i).assign(status=f'{s} consistent') for s, i in intervals.items()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One processing function per file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_lifetime_trips(folder: Path, pattern: str = '*Driver Lifetime Trips.csv') -> pd.DataFrame:\n",
    "    df = find_dataframe(pattern, folder,\n",
    "                        ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status'])\n",
    "    df = df[df.status.isin(['completed', 'fare_split'])].drop(columns='status')\n",
    "    df.columns = ['request', 'begintrip', 'dropoff']\n",
    "    df = time_tuples_to_periods(df, columns=df.columns,\n",
    "                                extra_info=[lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_on_off(folder: Path, pattern: str = '*Driver Online Offline.csv') -> pd.DataFrame:\n",
    "    df = find_dataframe(pattern, folder,\n",
    "                        ['begin_timestamp_local', 'end_timestamp_local', 'earner_state'])\n",
    "    df.columns = ['begin', 'end', 'status']\n",
    "    df = df.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_dispatches(path: str | Path) -> pd.DataFrame:\n",
    "    # TODO (not finished)\n",
    "    df = pd.read_csv(path, usecols=['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                                    'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                                    'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_trip_status(path: str | Path) -> pd.DataFrame:\n",
    "    pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_distance_traveled(path: str | Path) -> pd.DataFrame:\n",
    "    pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_sar(\n",
    "        folder: Path,\n",
    "        interval_logic: Callable[[dict[str, pd.DataFrame], dict[str, pd.DataFrame]], pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    lifetime_trips = process_lifetime_trips(folder)\n",
    "    on_off = process_on_off(folder)\n",
    "    return interval_logic(*[make_status_intervals(df) for df in [lifetime_trips, on_off]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specific to Portal data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def process_portal(folder: Path):\n",
    "    df = find_dataframe('*driver_lifetime_trips*.csv', folder,\n",
    "                        ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "                         'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    df = df[df['Status'].isin(['completed', 'fare_split'])]\n",
    "    df = time_tuples_to_periods(df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                                             'Local Dropoff Timestamp'],\n",
    "                                extra_info=[lambda r: {'status': 'P2'},\n",
    "                                            lambda r: {'status': 'P3',\n",
    "                                                       'distance_km': r['Trip Distance (miles)'] * 1.60934,\n",
    "                                                       'uber_paid': r['Local Original Fare']}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Common to SAR and Portal data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        df: pd.DataFrame,\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    Args:\n",
    "        - df: a table having a number N > 1 of time-columns and L of entries.\n",
    "        - columns: a list of n time-column names.\n",
    "        - extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    Returns:\n",
    "        - periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Ex:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(df.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_in_half_days(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Makes sure that intervals spanning many days or many morning/afternoon periods are split\"\"\"\n",
    "\n",
    "    # TODO write doc and give a better name\n",
    "    def rec(begin: datetime.datetime, end: datetime.datetime, **rest) -> list[dict]:\n",
    "        rows = []\n",
    "        if begin.day != end.day:\n",
    "            rows.append({'begin': begin, 'end': begin.replace(hour=23, minute=59, second=59), **rest})\n",
    "            for days in range(end.day - begin.day - 1):\n",
    "                inbetween = (begin + datetime.timedelta(days=days))\n",
    "                rows.append({'begin': inbetween.replace(hour=0, minute=0, second=0),\n",
    "                             'end': inbetween.replace(hour=23, minute=59, second=59), **rest})\n",
    "            rows.append({'begin': end.replace(hour=0, minute=0, second=0), 'end': end, **rest})\n",
    "            return [e for r in rows for e in rec(**r)]\n",
    "        elif begin.hour < 12 <= end.hour:\n",
    "            rows.append({'begin': begin, 'end': end.replace(hour=11, minute=59, second=59), **rest})\n",
    "            rows.append({'begin': end.replace(hour=12, minute=0, second=0), 'end': end, **rest})\n",
    "            return rows\n",
    "        else:\n",
    "            return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in df.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: pd.DataFrame,\n",
    "        interval_logic: Optional[Callable[[pd.DataFrame], pd.DataFrame]] = None,\n",
    "        filtering_logic: Optional[Callable[[pd.DataFrame], pd.DataFrame]] = None,\n",
    "        name: str = 'time_per_status',\n",
    "        facets: list[str] = ('duration',)\n",
    ") -> Tuple[pd.DataFrame, ...]:\n",
    "    # Make sure that each interval is contained within a single 12-hour time period (AM or PM) by splitting when necessary\n",
    "    splitted = split_in_half_days(periods)\n",
    "\n",
    "    # Now we can compute all of these time properties since they will be the same for begin and end\n",
    "    splitted['date'] = splitted.end.dt.date\n",
    "    splitted['day_of_week'] = splitted.end.dt.day_name()\n",
    "    splitted['day_type'] = (splitted.end.dt.weekday < 5).replace({True: 'week day', False: 'weekend'})\n",
    "    splitted['time_of_day'] = (splitted.end.dt.hour < 12).replace({True: 'AM', False: 'PM'})\n",
    "    splitted['week'] = splitted.end.dt.isocalendar().week\n",
    "    splitted['year'] = splitted.end.dt.year\n",
    "    splitted['duration'] = splitted.end - splitted.begin\n",
    "\n",
    "    # Group entries by day and time period\n",
    "    daily = splitted.groupby(['date', 'year', 'week', 'day_of_week',\n",
    "                              'day_type', 'time_of_day', 'status']).agg({f: 'sum' for f in facets}).reset_index()\n",
    "    daily['date'] = pd.to_datetime(daily.date)\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        filtered = filtering_logic(daily)\n",
    "        filtered['status'] = filtered.status + '|filtered'\n",
    "        daily = pd.concat([daily, filtered])\n",
    "\n",
    "    processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "    daily.to_csv(processed_folder / f'{name}_daily.csv', index=False)\n",
    "    weekly = daily.groupby(['year', 'week', 'status']).agg({f: 'sum' for f in facets}).reset_index()\n",
    "    weekly.to_csv(processed_folder / f'{name}_weekly.csv', index=False)\n",
    "    yearly = weekly.groupby(['year', 'status']).agg({f: 'sum' for f in facets}).reset_index()\n",
    "    yearly.to_csv(processed_folder / f'{name}_yearly.csv', index=False)\n",
    "    total = yearly.groupby(['status']).agg({f: 'sum' for f in facets}).reset_index()\n",
    "    total.to_csv(processed_folder / f'{name}_total.csv', index=False)\n",
    "    return daily, weekly, yearly, total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tying everything together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: pd.DataFrame,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'percentage.csv'\n",
    ") -> pd.DataFrame:\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    P1 = daily.loc[daily.status.str.contains('P1')].copy()\n",
    "    for i, row in percentage.iterrows():\n",
    "        if row['Uber'] == 0 and P1[(P1.date.dt.year == row.year) & (\n",
    "                P1.date.dt.month == row.month)].duration.sum() > datetime.timedelta(0):\n",
    "            print(\n",
    "                f'bad specification for {row.year}/{row.month}: activity found even though specified percentage was 0')\n",
    "        P1['duration'] = np.where(\n",
    "            (P1.date.dt.year == row.year) & (P1.date.dt.month == row.month),\n",
    "            P1['duration'] * row['Uber'],\n",
    "            P1['duration'])\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26),\n",
    "                     *pd.date_range(datetime.date(2020, 12, 21), datetime.date(2020, 12, 25)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 2, 1), datetime.date(2021, 2, 12)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 8, 16), datetime.date(2021, 8, 28)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 9, 20), datetime.date(2021, 10, 3)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 11, 25), datetime.date(2021, 12, 12)).values,\n",
    "                     *pd.date_range(datetime.date(2022, 4, 25), datetime.date(2022, 5, 13)).values]\n",
    "    P1.drop(P1[(P1.time_of_day == 'AM') &\n",
    "               (P1.day_type == 'week day') &\n",
    "               ~P1.date.apply(lambda d: d.date()).isin(dates_to_keep)].index,\n",
    "            inplace=True)\n",
    "    return P1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = pipeline(process_sar(raw_folder / 'new', interval_logic=main_interval_logic),\n",
    "             name='sar',\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             facets=['duration'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = pipeline(process_portal(raw_folder / 'portal' / 'Driver'),\n",
    "             name='portal',\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             facets=['duration', 'distance_km', 'uber_paid'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
