{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Uber Driver Data Analysis\n",
    "\n",
    "## Data processing pipeline\n",
    "The data processing pipeline consists of 2 parts, that should be executed in order.\n",
    "The explanation of each part can be found below, in later cells.\n",
    "\n",
    "## File structure\n",
    "### Required input files\n",
    "- `data`\n",
    "  - `raw`\n",
    "    - `02 - Driver Lifetime Trips.csv`\n",
    "    - `05 - Driver Online OffLine.csv`\n",
    "    - `08 - Driver Dispatches Offered and Accepted.csv`\n",
    "\n",
    "### Created output files\n",
    "- `data`\n",
    "  - `processed`\n",
    "    - `02-events_df.csv`\n",
    "    - `02-period_df.csv`\n",
    "    - `05-events_df.csv`\n",
    "    - `05-period_df.csv`\n",
    "    - `08-period_df.csv`\n",
    "    - `08-events_df.csv`\n",
    "    - `fusion_df.csv`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import uuid\n",
    "from functools import reduce\n",
    "from pathlib import Path, PurePath\n",
    "from typing import TypedDict, Callable, Optional, Tuple\n",
    "import portion as P\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ncls import NCLS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_folder = PurePath(os.getcwd()) / 'data'\n",
    "raw_folder = data_folder / 'raw'\n",
    "processed_folder = data_folder / 'processed'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SeriesMapping = Callable[[pd.Series], pd.Series]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Config(TypedDict):\n",
    "    key: str\n",
    "    filename: str\n",
    "    datetime_columns: list[str]\n",
    "    duration_columns: dict[str, str]\n",
    "    columns_to_merge: list[list[str]]\n",
    "    value_names: list[str]\n",
    "    label_names: list[str]\n",
    "\n",
    "\n",
    "configs: list[Config] = [\n",
    "    {\n",
    "        'key': '02',\n",
    "        'filename': '02 - Driver Lifetime Trips.csv',\n",
    "        'datetime_columns': [\n",
    "            'request_timestamp_local',\n",
    "            'request_timestamp_utc',\n",
    "            'begintrip_timestamp_local',\n",
    "            'begintrip_timestamp_utc',\n",
    "            'dropoff_timestamp_local',\n",
    "            'dropoff_timestamp_utc',\n",
    "            'rewindtrip_timestamp_local',\n",
    "            'rewindtrip_timestamp_utc'\n",
    "        ],\n",
    "        'duration_columns': {\n",
    "            'request_to_begin_duration_seconds': 'second',\n",
    "            'trip_duration_seconds': 'second',\n",
    "            'fare_duration_minutes': 'minute',\n",
    "            'wait_duration_minutes': 'minute'\n",
    "        },\n",
    "        'columns_to_merge': [\n",
    "            [\n",
    "                'request_timestamp_utc',\n",
    "                'begintrip_timestamp_utc',\n",
    "                'dropoff_timestamp_utc'\n",
    "            ],\n",
    "            [\n",
    "                'request_lat',\n",
    "                'begintrip_lat',\n",
    "                'dropoff_lat'\n",
    "            ],\n",
    "            [\n",
    "                'request_lng',\n",
    "                'begintrip_lng',\n",
    "                'dropoff_lng'\n",
    "            ]\n",
    "        ],\n",
    "        'value_names': [\n",
    "            'event_timestamp_utc',\n",
    "            'event_lat',\n",
    "            'event_lng'\n",
    "        ],\n",
    "        'label_names': [\n",
    "            'request',\n",
    "            'begintrip',\n",
    "            'dropoff'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'key': '05',\n",
    "        'filename': '05 - Driver Online Offline.csv',\n",
    "        'datetime_columns': [\n",
    "            'begin_timestamp',\n",
    "            'end_timestamp',\n",
    "            'begin_timestamp_local',\n",
    "            'end_timestamp_local'\n",
    "        ],\n",
    "        'duration_columns': {\n",
    "            'duration_seconds': 'second'\n",
    "        },\n",
    "        'columns_to_merge': [\n",
    "            [\n",
    "                'begin_timestamp',\n",
    "                'end_timestamp'\n",
    "            ],\n",
    "            [\n",
    "                'begin_lat',\n",
    "                'end_lat'\n",
    "            ],\n",
    "            [\n",
    "                'begin_lng',\n",
    "                'end_lng'\n",
    "            ]\n",
    "        ],\n",
    "        'value_names': [\n",
    "            'event_timestamp_utc',\n",
    "            'event_lat',\n",
    "            'event_lng'\n",
    "        ],\n",
    "        'label_names': [\n",
    "            'start',\n",
    "            'end'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'key': '08',\n",
    "        'filename': '08 - Driver Dispatches Offered and Accepted.csv',\n",
    "        'datetime_columns': [\n",
    "            'start_timestamp_utc',\n",
    "            'end_timestamp_utc',\n",
    "            'start_timestamp_local',\n",
    "            'end_timestamp_local'\n",
    "        ],\n",
    "        'duration_columns': {\n",
    "            'minutes_online': 'minute',\n",
    "            'minutes_active': 'minute',\n",
    "            'minutes_on_trip': 'minute'\n",
    "        },\n",
    "        'columns_to_merge': [\n",
    "            [\n",
    "                'start_timestamp_utc',\n",
    "                'end_timestamp_utc'\n",
    "            ],\n",
    "            [\n",
    "                'start_timestamp_local',\n",
    "                'end_timestamp_local'\n",
    "            ]\n",
    "        ],\n",
    "        'value_names': [\n",
    "            'event_timestamp_utc',\n",
    "            'event_timestamp_local'\n",
    "        ],\n",
    "        'label_names': [\n",
    "            'start',\n",
    "            'end'\n",
    "        ]\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1\n",
    "### Pivoting events by type\n",
    "\n",
    "This pipeline is applied to the three raw files described above.\n",
    "\n",
    "For each file, the main operation is the pivoting of columns according to the type of the event in the row.\n",
    "Each event is thus given an id and some extra information such as its type.\n",
    "\n",
    "The data is then split into two tables:\n",
    "- `period`: the same as the original table but extended with an id for each event type and with a period id\n",
    "- `events`: the list of events and their properties\n",
    "\n",
    "Emmanuel's description:\n",
    "> Separates \"events\" and \"periods\" in Uber data files, with UUIDs matching events to start or end limits of intervals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def replace_NaN(df: pd.DataFrame, NaN_expressions: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Replaces all occurrences of {NaN_expressions} by {np.nan} in {df}\"\"\"\n",
    "    for NaN_expression in NaN_expressions:\n",
    "        df = df.replace({NaN_expression: np.nan})\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_mappings(\n",
    "        df: pd.DataFrame,\n",
    "        column_mappings: dict[str, SeriesMapping]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Applies functions in {columns_mappings} to {df}\"\"\"\n",
    "    remaining = list(set(df.columns) - set(column_mappings.keys()))\n",
    "    return pd.concat([df.transform(column_mappings), df[remaining]], axis=1)\n",
    "\n",
    "\n",
    "def load_data(\n",
    "        file_path: Path,\n",
    "        column_mappings: Optional[dict[str, SeriesMapping]] = None,\n",
    "        NaN_expressions: Optional[list[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = replace_NaN(df, (NaN_expressions or []) + ['NaN', 'NA', 'N/A', r'\\N'])\n",
    "    if column_mappings is not None:\n",
    "        df = apply_mappings(df, column_mappings)\n",
    "    return df\n",
    "\n",
    "\n",
    "def duration_mapping(shorthand: str) -> SeriesMapping:\n",
    "    return lambda s: pd.to_timedelta(s.astype(float), unit=shorthand)\n",
    "\n",
    "\n",
    "datetime64: SeriesMapping = lambda s: s.astype('datetime64[ns]')\n",
    "get_duration: dict[str, SeriesMapping] = {'second': duration_mapping('s'), 'minute': duration_mapping('m')}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def pivot_events(\n",
    "        df: pd.DataFrame,\n",
    "        columns_to_merge: list[list[str]],\n",
    "        value_names: list[str],\n",
    "        label_names: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    assert len(set([len(c) for c in\n",
    "                    columns_to_merge])) == 1, f'extract_1_events: columns_to_merge must have lines of equal lengths'\n",
    "    assert len(value_names) == len(\n",
    "        columns_to_merge), f'extract_1_events: value_names must have as many items as number of lines in columns_to_merge'\n",
    "    assert len(label_names) == len(columns_to_merge[\n",
    "                                       0]), f'extract_1_events: label_names must have as many items as the number of items in each line of columns_to_merge'\n",
    "\n",
    "    df_no_index = df.reset_index()\n",
    "    dfs = []\n",
    "    for (column_to_merge, value_name) in zip(columns_to_merge, value_names):\n",
    "        # the pivoting of events\n",
    "        pivoted = df_no_index.melt(id_vars='index', value_vars=column_to_merge, var_name='event_type',\n",
    "                                   value_name=value_name).rename(columns={'index': 'period_id'})\n",
    "        # e.g. replaces instances of {begin_timestamp} with {begin} (since it is in column {timestamp})\n",
    "        for (column, label) in zip(column_to_merge, label_names):\n",
    "            pivoted = pivoted.replace(column, label)\n",
    "        dfs.append(pivoted)\n",
    "\n",
    "    merged = reduce(lambda l, r: pd.merge(l, r, how='left', on=['period_id', 'event_type']), dfs)\n",
    "\n",
    "    merged['event_UUID'] = [uuid.uuid4() for _ in merged.index]\n",
    "    merged['period_UUID'] = [uuid.uuid4() for _ in df.index] * len(label_names)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def extend_event_info(df: pd.DataFrame, events_df: pd.DataFrame, label_names: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()  # to avoid in-place operations\n",
    "    for label in label_names:\n",
    "        df[label + '_event_UUID'] = events_df[events_df['event_type'] == label]['event_UUID'].tolist()\n",
    "    df['period_UUID'] = events_df[events_df['event_type'] == label_names[0]]['period_UUID'].tolist()\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def first_pipeline(\n",
    "        key: str,\n",
    "        filename: str,\n",
    "        datetime_columns: list[str],\n",
    "        duration_columns: dict[str, str],\n",
    "        columns_to_merge: list[list[str]],\n",
    "        value_names: list[str],\n",
    "        label_names: list[str]\n",
    ") -> (pd.DataFrame, pd.DataFrame):\n",
    "    datetime_columns = {c: datetime64 for c in datetime_columns}\n",
    "    duration_columns = {k: get_duration[v] for k, v in duration_columns.items()}\n",
    "\n",
    "    filepath = raw_folder / filename\n",
    "\n",
    "    data_df = load_data(filepath, {**datetime_columns, **duration_columns})\n",
    "\n",
    "    events_df = pivot_events(data_df, columns_to_merge, value_names, label_names)\n",
    "    period_df = extend_event_info(data_df, events_df, label_names)\n",
    "\n",
    "    print(filename)\n",
    "    display(data_df.head(1))\n",
    "    print(f'{key}-events_df.csv')\n",
    "    display(events_df.head(1))\n",
    "    print(f'{key}-period_df.csv')\n",
    "    display(period_df.head(1))\n",
    "\n",
    "    processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "    events_df.to_csv(processed_folder / f'{key}-events_df.csv', index=False)\n",
    "    period_df.to_csv(processed_folder / f'{key}-period_df.csv', index=False)\n",
    "\n",
    "    return events_df, period_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 02 - Driver Lifetime Trips.csv\n",
    "For this file, turns rows of\n",
    "```\n",
    "[request_time, request_lng, request_lat,\n",
    " begintrip_time, begintrip_lng, begintrip_lat,\n",
    " dropoff_time, dropoff_lng, dropoff_lat]\n",
    "```\n",
    "into rows of\n",
    "`[event_id, event_type, event_time, event_lng, event_lat]`\n",
    "where `event_type` is one of `[request, begintrip, dropoff]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 - Driver Lifetime Trips.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "  request_timestamp_local request_timestamp_utc begintrip_timestamp_local  \\\n0     2017-11-02 14:27:40   2017-11-02 13:27:40       2017-11-02 14:36:01   \n\n  begintrip_timestamp_utc dropoff_timestamp_local dropoff_timestamp_utc  \\\n0     2017-11-02 13:36:01     2017-11-02 14:48:40   2017-11-02 13:48:40   \n\n  rewindtrip_timestamp_local rewindtrip_timestamp_utc  \\\n0                        NaT                      NaT   \n\n  request_to_begin_duration_seconds trip_duration_seconds  ...  \\\n0                   0 days 00:08:21       0 days 00:12:40  ...   \n\n  has_driver_upfront_fare is_cash_trip wait_time_fare_local is_on_time  \\\n0                   False        False                  NaN        NaN   \n\n  earnings_boost_usd  wait_time_fare_usd service_fee_usd trip_distance_miles  \\\n0                NaN                 NaN             NaN            1.820242   \n\n  rounding_down_amount_local  long_distance_surcharge_local  \n0                        0.0                            NaN  \n\n[1 rows x 89 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>request_timestamp_local</th>\n      <th>request_timestamp_utc</th>\n      <th>begintrip_timestamp_local</th>\n      <th>begintrip_timestamp_utc</th>\n      <th>dropoff_timestamp_local</th>\n      <th>dropoff_timestamp_utc</th>\n      <th>rewindtrip_timestamp_local</th>\n      <th>rewindtrip_timestamp_utc</th>\n      <th>request_to_begin_duration_seconds</th>\n      <th>trip_duration_seconds</th>\n      <th>...</th>\n      <th>has_driver_upfront_fare</th>\n      <th>is_cash_trip</th>\n      <th>wait_time_fare_local</th>\n      <th>is_on_time</th>\n      <th>earnings_boost_usd</th>\n      <th>wait_time_fare_usd</th>\n      <th>service_fee_usd</th>\n      <th>trip_distance_miles</th>\n      <th>rounding_down_amount_local</th>\n      <th>long_distance_surcharge_local</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-02 14:27:40</td>\n      <td>2017-11-02 13:27:40</td>\n      <td>2017-11-02 14:36:01</td>\n      <td>2017-11-02 13:36:01</td>\n      <td>2017-11-02 14:48:40</td>\n      <td>2017-11-02 13:48:40</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>0 days 00:08:21</td>\n      <td>0 days 00:12:40</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.820242</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 89 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-events_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "   period_id event_type event_timestamp_utc  event_lat event_lng  \\\n0          0    request 2017-11-02 13:27:40  46.191392  6.153364   \n\n                             event_UUID                           period_UUID  \n0  988b4af7-6e80-4a39-aa69-5a7aa18b76b6  41e7cec2-4f31-4fb2-af12-93970582678d  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>period_id</th>\n      <th>event_type</th>\n      <th>event_timestamp_utc</th>\n      <th>event_lat</th>\n      <th>event_lng</th>\n      <th>event_UUID</th>\n      <th>period_UUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>request</td>\n      <td>2017-11-02 13:27:40</td>\n      <td>46.191392</td>\n      <td>6.153364</td>\n      <td>988b4af7-6e80-4a39-aa69-5a7aa18b76b6</td>\n      <td>41e7cec2-4f31-4fb2-af12-93970582678d</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-period_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "  request_timestamp_local request_timestamp_utc begintrip_timestamp_local  \\\n0     2017-11-02 14:27:40   2017-11-02 13:27:40       2017-11-02 14:36:01   \n\n  begintrip_timestamp_utc dropoff_timestamp_local dropoff_timestamp_utc  \\\n0     2017-11-02 13:36:01     2017-11-02 14:48:40   2017-11-02 13:48:40   \n\n  rewindtrip_timestamp_local rewindtrip_timestamp_utc  \\\n0                        NaT                      NaT   \n\n  request_to_begin_duration_seconds trip_duration_seconds  ...  \\\n0                   0 days 00:08:21       0 days 00:12:40  ...   \n\n  earnings_boost_usd wait_time_fare_usd service_fee_usd trip_distance_miles  \\\n0                NaN                NaN             NaN            1.820242   \n\n  rounding_down_amount_local  long_distance_surcharge_local  \\\n0                        0.0                            NaN   \n\n                     request_event_UUID                  begintrip_event_UUID  \\\n0  988b4af7-6e80-4a39-aa69-5a7aa18b76b6  7b9a07f1-fec1-471b-8a2a-f0c086c3be38   \n\n                     dropoff_event_UUID                           period_UUID  \n0  b1a62f65-f4e1-4f80-9012-5599148ad89b  41e7cec2-4f31-4fb2-af12-93970582678d  \n\n[1 rows x 93 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>request_timestamp_local</th>\n      <th>request_timestamp_utc</th>\n      <th>begintrip_timestamp_local</th>\n      <th>begintrip_timestamp_utc</th>\n      <th>dropoff_timestamp_local</th>\n      <th>dropoff_timestamp_utc</th>\n      <th>rewindtrip_timestamp_local</th>\n      <th>rewindtrip_timestamp_utc</th>\n      <th>request_to_begin_duration_seconds</th>\n      <th>trip_duration_seconds</th>\n      <th>...</th>\n      <th>earnings_boost_usd</th>\n      <th>wait_time_fare_usd</th>\n      <th>service_fee_usd</th>\n      <th>trip_distance_miles</th>\n      <th>rounding_down_amount_local</th>\n      <th>long_distance_surcharge_local</th>\n      <th>request_event_UUID</th>\n      <th>begintrip_event_UUID</th>\n      <th>dropoff_event_UUID</th>\n      <th>period_UUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-02 14:27:40</td>\n      <td>2017-11-02 13:27:40</td>\n      <td>2017-11-02 14:36:01</td>\n      <td>2017-11-02 13:36:01</td>\n      <td>2017-11-02 14:48:40</td>\n      <td>2017-11-02 13:48:40</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>0 days 00:08:21</td>\n      <td>0 days 00:12:40</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.820242</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>988b4af7-6e80-4a39-aa69-5a7aa18b76b6</td>\n      <td>7b9a07f1-fec1-471b-8a2a-f0c086c3be38</td>\n      <td>b1a62f65-f4e1-4f80-9012-5599148ad89b</td>\n      <td>41e7cec2-4f31-4fb2-af12-93970582678d</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 93 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _ = first_pipeline(**configs[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 05 - Driver Online Offline.csv\n",
    "For this file, turns rows of\n",
    "```\n",
    "[begin_timestamp, end_timestamp,\n",
    " begin_lat, end_lat,\n",
    " begin_lng, end_lng]\n",
    "```\n",
    "into rows of\n",
    "`[event_id, event_type, event_timestamp_utc, event_lng, event_lat]`\n",
    "where `event_type` is one of `[start, end]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05 - Driver Online Offline.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "      begin_timestamp       end_timestamp begin_timestamp_local  \\\n0 2017-11-01 15:36:41 2017-11-01 15:36:49   2017-11-01 16:36:41   \n\n  end_timestamp_local duration_seconds  end_lng  \\\n0 2017-11-01 16:36:49  0 days 00:00:08  6.13493   \n\n                           vehicle_uuid    end_lat  begin_lat status  \\\n0  63d9a727-1ee4-4cc0-998c-69d321dd8028  46.177765  46.177765   open   \n\n   begin_lng  city_id  \n0    6.13493      266  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>begin_timestamp</th>\n      <th>end_timestamp</th>\n      <th>begin_timestamp_local</th>\n      <th>end_timestamp_local</th>\n      <th>duration_seconds</th>\n      <th>end_lng</th>\n      <th>vehicle_uuid</th>\n      <th>end_lat</th>\n      <th>begin_lat</th>\n      <th>status</th>\n      <th>begin_lng</th>\n      <th>city_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-01 15:36:41</td>\n      <td>2017-11-01 15:36:49</td>\n      <td>2017-11-01 16:36:41</td>\n      <td>2017-11-01 16:36:49</td>\n      <td>0 days 00:00:08</td>\n      <td>6.13493</td>\n      <td>63d9a727-1ee4-4cc0-998c-69d321dd8028</td>\n      <td>46.177765</td>\n      <td>46.177765</td>\n      <td>open</td>\n      <td>6.13493</td>\n      <td>266</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-events_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "   period_id event_type event_timestamp_utc  event_lat event_lng  \\\n0          0      start 2017-11-01 15:36:41  46.177765   6.13493   \n\n                             event_UUID                           period_UUID  \n0  eea8b59c-709e-428e-bfce-00e49ad3bd66  84da5ddb-1716-4ef6-960b-68174852019b  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>period_id</th>\n      <th>event_type</th>\n      <th>event_timestamp_utc</th>\n      <th>event_lat</th>\n      <th>event_lng</th>\n      <th>event_UUID</th>\n      <th>period_UUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>start</td>\n      <td>2017-11-01 15:36:41</td>\n      <td>46.177765</td>\n      <td>6.13493</td>\n      <td>eea8b59c-709e-428e-bfce-00e49ad3bd66</td>\n      <td>84da5ddb-1716-4ef6-960b-68174852019b</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-period_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "      begin_timestamp       end_timestamp begin_timestamp_local  \\\n0 2017-11-01 15:36:41 2017-11-01 15:36:49   2017-11-01 16:36:41   \n\n  end_timestamp_local duration_seconds  end_lng  \\\n0 2017-11-01 16:36:49  0 days 00:00:08  6.13493   \n\n                           vehicle_uuid    end_lat  begin_lat status  \\\n0  63d9a727-1ee4-4cc0-998c-69d321dd8028  46.177765  46.177765   open   \n\n   begin_lng  city_id                      start_event_UUID  \\\n0    6.13493      266  eea8b59c-709e-428e-bfce-00e49ad3bd66   \n\n                         end_event_UUID                           period_UUID  \n0  e90a3355-dc6e-463d-a67b-74408d55bb40  84da5ddb-1716-4ef6-960b-68174852019b  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>begin_timestamp</th>\n      <th>end_timestamp</th>\n      <th>begin_timestamp_local</th>\n      <th>end_timestamp_local</th>\n      <th>duration_seconds</th>\n      <th>end_lng</th>\n      <th>vehicle_uuid</th>\n      <th>end_lat</th>\n      <th>begin_lat</th>\n      <th>status</th>\n      <th>begin_lng</th>\n      <th>city_id</th>\n      <th>start_event_UUID</th>\n      <th>end_event_UUID</th>\n      <th>period_UUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-01 15:36:41</td>\n      <td>2017-11-01 15:36:49</td>\n      <td>2017-11-01 16:36:41</td>\n      <td>2017-11-01 16:36:49</td>\n      <td>0 days 00:00:08</td>\n      <td>6.13493</td>\n      <td>63d9a727-1ee4-4cc0-998c-69d321dd8028</td>\n      <td>46.177765</td>\n      <td>46.177765</td>\n      <td>open</td>\n      <td>6.13493</td>\n      <td>266</td>\n      <td>eea8b59c-709e-428e-bfce-00e49ad3bd66</td>\n      <td>e90a3355-dc6e-463d-a67b-74408d55bb40</td>\n      <td>84da5ddb-1716-4ef6-960b-68174852019b</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _ = first_pipeline(**configs[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 08 - Driver Dispatches Offered and Accepted.csv\n",
    "For this file, turns rows of\n",
    "```\n",
    "[start_timestamp_utc, end_timestamp_utc\n",
    " start_timestamp_local, end_timestamp_local]\n",
    "```\n",
    "into rows of\n",
    "`[event_id, event_type, event_timestamp_utc, event_timestamp_local]`\n",
    "where `event_type` is one of `[start, end]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08 - Driver Dispatches Offered and Accepted.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "  start_timestamp_utc   end_timestamp_utc start_timestamp_local  \\\n0 2021-11-18 12:00:00 2021-11-18 13:00:00   2021-11-18 13:00:00   \n\n  end_timestamp_local  minutes_online minutes_active minutes_on_trip  \\\n0 2021-11-18 14:00:00 0 days 01:00:00         0 days          0 days   \n\n   driver_adjusted_fares                             partner_uuids  \\\n0                    0.0  [\"f0699b53-7acb-48ba-9cea-e872a1de9fb9\"]   \n\n   rider_cancellations  ... driver_cancellations  rejections  flow_type  \\\n0                    0  ...                    0           0      UberX   \n\n  accepts  expireds  dispatches  city_id  completed_trips  trip_fares  \\\n0       0         0           0      266                0         0.0   \n\n                              vehicle_uuids  \n0  [\"63d9a727-1ee4-4cc0-998c-69d321dd8028\"]  \n\n[1 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start_timestamp_utc</th>\n      <th>end_timestamp_utc</th>\n      <th>start_timestamp_local</th>\n      <th>end_timestamp_local</th>\n      <th>minutes_online</th>\n      <th>minutes_active</th>\n      <th>minutes_on_trip</th>\n      <th>driver_adjusted_fares</th>\n      <th>partner_uuids</th>\n      <th>rider_cancellations</th>\n      <th>...</th>\n      <th>driver_cancellations</th>\n      <th>rejections</th>\n      <th>flow_type</th>\n      <th>accepts</th>\n      <th>expireds</th>\n      <th>dispatches</th>\n      <th>city_id</th>\n      <th>completed_trips</th>\n      <th>trip_fares</th>\n      <th>vehicle_uuids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-11-18 12:00:00</td>\n      <td>2021-11-18 13:00:00</td>\n      <td>2021-11-18 13:00:00</td>\n      <td>2021-11-18 14:00:00</td>\n      <td>0 days 01:00:00</td>\n      <td>0 days</td>\n      <td>0 days</td>\n      <td>0.0</td>\n      <td>[\"f0699b53-7acb-48ba-9cea-e872a1de9fb9\"]</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>UberX</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>266</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[\"63d9a727-1ee4-4cc0-998c-69d321dd8028\"]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 21 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-events_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "   period_id event_type event_timestamp_utc event_timestamp_local  \\\n0          0      start 2021-11-18 12:00:00   2021-11-18 13:00:00   \n\n                             event_UUID                           period_UUID  \n0  3e43d97e-39d9-438c-9365-35e29f9fe212  7573c8bc-b0da-4e43-9809-bfff49aec160  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>period_id</th>\n      <th>event_type</th>\n      <th>event_timestamp_utc</th>\n      <th>event_timestamp_local</th>\n      <th>event_UUID</th>\n      <th>period_UUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>start</td>\n      <td>2021-11-18 12:00:00</td>\n      <td>2021-11-18 13:00:00</td>\n      <td>3e43d97e-39d9-438c-9365-35e29f9fe212</td>\n      <td>7573c8bc-b0da-4e43-9809-bfff49aec160</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-period_df.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "  start_timestamp_utc   end_timestamp_utc start_timestamp_local  \\\n0 2021-11-18 12:00:00 2021-11-18 13:00:00   2021-11-18 13:00:00   \n\n  end_timestamp_local  minutes_online minutes_active minutes_on_trip  \\\n0 2021-11-18 14:00:00 0 days 01:00:00         0 days          0 days   \n\n   driver_adjusted_fares                             partner_uuids  \\\n0                    0.0  [\"f0699b53-7acb-48ba-9cea-e872a1de9fb9\"]   \n\n   rider_cancellations  ... accepts  expireds  dispatches city_id  \\\n0                    0  ...       0         0           0     266   \n\n   completed_trips  trip_fares                             vehicle_uuids  \\\n0                0         0.0  [\"63d9a727-1ee4-4cc0-998c-69d321dd8028\"]   \n\n                       start_event_UUID                        end_event_UUID  \\\n0  3e43d97e-39d9-438c-9365-35e29f9fe212  853539f5-daca-4e8d-8566-45ee7a5a2d00   \n\n                            period_UUID  \n0  7573c8bc-b0da-4e43-9809-bfff49aec160  \n\n[1 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start_timestamp_utc</th>\n      <th>end_timestamp_utc</th>\n      <th>start_timestamp_local</th>\n      <th>end_timestamp_local</th>\n      <th>minutes_online</th>\n      <th>minutes_active</th>\n      <th>minutes_on_trip</th>\n      <th>driver_adjusted_fares</th>\n      <th>partner_uuids</th>\n      <th>rider_cancellations</th>\n      <th>...</th>\n      <th>accepts</th>\n      <th>expireds</th>\n      <th>dispatches</th>\n      <th>city_id</th>\n      <th>completed_trips</th>\n      <th>trip_fares</th>\n      <th>vehicle_uuids</th>\n      <th>start_event_UUID</th>\n      <th>end_event_UUID</th>\n      <th>period_UUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-11-18 12:00:00</td>\n      <td>2021-11-18 13:00:00</td>\n      <td>2021-11-18 13:00:00</td>\n      <td>2021-11-18 14:00:00</td>\n      <td>0 days 01:00:00</td>\n      <td>0 days</td>\n      <td>0 days</td>\n      <td>0.0</td>\n      <td>[\"f0699b53-7acb-48ba-9cea-e872a1de9fb9\"]</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>266</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>[\"63d9a727-1ee4-4cc0-998c-69d321dd8028\"]</td>\n      <td>3e43d97e-39d9-438c-9365-35e29f9fe212</td>\n      <td>853539f5-daca-4e8d-8566-45ee7a5a2d00</td>\n      <td>7573c8bc-b0da-4e43-9809-bfff49aec160</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 24 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _ = first_pipeline(**configs[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "### Merging time intervals\n",
    "\n",
    "*The understanding of this part is in progress.*\n",
    "\n",
    "There is some form of time-interval merging, but it is not clear to what end yet.\n",
    "\n",
    "Emmanuel's description:\n",
    "> merges Period files to find intersections between periods events recorded automaticaly by the Uber application (\"Driver Online Offline\") and trips performed by the human driver (\"Driver Lifetime Trips\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def fix_timestamp(df: pd.DataFrame, col_to_replace: str, col_rescue: str):\n",
    "    # If {col_to_replace} is null (NaT), replace with {col_rescue} of the next row\n",
    "    df[col_to_replace] = np.where(df[col_to_replace].isnull(),\n",
    "                                  df[col_rescue].shift(-1),\n",
    "                                  df[col_to_replace])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_convert_dates(filepath: PurePath, datetime_columns: Optional[list[str]] = None):\n",
    "    df = pd.read_csv(filepath)\n",
    "    for i in (datetime_columns or []):\n",
    "        df[i] = pd.to_datetime(df[i])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def merge_over_time_intervals(df1, df2, start1, end1, start2, end2):\n",
    "    df1['start_unixts'] = df1[start1].view('int64')\n",
    "    df1['end_unixts'] = df1[end1].view('int64')\n",
    "    df2['start_unixts'] = df2[start2].view('int64')\n",
    "    df2['end_unixts'] = df2[end2].view('int64')\n",
    "\n",
    "    ncls = NCLS(df1['start_unixts'], df1['end_unixts'], df1.index.values)\n",
    "\n",
    "    x1, x2 = ncls.all_overlaps_both(df2['start_unixts'].values, df2['end_unixts'].values, df2.index.values)\n",
    "\n",
    "    df1 = df1.reindex(x2).reset_index(drop=True)\n",
    "    df2 = df2.reindex(x1).reset_index(drop=True)\n",
    "\n",
    "    df = df1.join(df2, rsuffix='2')\n",
    "\n",
    "    df.drop(['start_unixts', 'end_unixts'], axis=1, inplace=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def second_pipeline() -> pd.DataFrame:\n",
    "    trips_df = load_and_convert_dates(processed_folder / '02-period_df.csv',\n",
    "                                      datetime_columns=next(c for c in configs if c['key'] == '02')['datetime_columns'])\n",
    "    app_connection_full_df = load_and_convert_dates(processed_folder / '05-period_df.csv',\n",
    "                                                    datetime_columns=next(c for c in configs if c['key'] == '05')[\n",
    "                                                        'datetime_columns'])\n",
    "\n",
    "    app_connection_full_df = fix_timestamp(app_connection_full_df, 'end_timestamp', 'begin_timestamp')\n",
    "    app_connection_full_df = app_connection_full_df.dropna(subset=['begin_timestamp', 'end_timestamp'])\n",
    "    trips_df = fix_timestamp(trips_df, 'begintrip_timestamp_utc', 'begintrip_timestamp_utc')\n",
    "    trips_df = fix_timestamp(trips_df, 'dropoff_timestamp_utc', 'begintrip_timestamp_utc')\n",
    "\n",
    "    merged = merge_over_time_intervals(app_connection_full_df, trips_df,\n",
    "                                       'begin_timestamp', 'end_timestamp',\n",
    "                                       'request_timestamp_utc', 'dropoff_timestamp_utc')\n",
    "\n",
    "    merged.to_csv(processed_folder / 'fusion_df.csv', index=False)\n",
    "\n",
    "    return merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "fusion_df = second_pipeline()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Time lost analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def df_to_interval(df: pd.DataFrame, status: str) -> P.interval:\n",
    "    return P.Interval(*[P.closed(row['begin'], row['end']) for row in df[df.status == status].to_dict('records')])\n",
    "\n",
    "\n",
    "def interval_to_df(interval: P.interval, status: str) -> pd.DataFrame:\n",
    "    return pd.DataFrame([{'begin': begin, 'end': end, 'status': status} for _, begin, end, _ in P.to_data(interval)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def split(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Makes sure that intervals spanning many days or many morning/afternoon periods are split\"\"\"\n",
    "\n",
    "    def rec(begin: datetime.datetime, end: datetime.datetime, **rest) -> list[dict]:\n",
    "        rows = []\n",
    "        if begin.day != end.day:\n",
    "            rows.append({'begin': begin, 'end': begin.replace(hour=23, minute=59, second=59), **rest})\n",
    "            for days in range(begin.day + 1, end.day):\n",
    "                inbetween = (begin + datetime.timedelta(days=days))\n",
    "                rows.append({'begin': inbetween.replace(hour=0, minute=0, second=0),\n",
    "                             'end': inbetween.replace(hour=23, minute=59, second=59), **rest})\n",
    "            rows.append({'begin': end.replace(hour=0, minute=0, second=0),\n",
    "                         'end': end, **rest})\n",
    "            return [e for r in rows for e in rec(**r)]\n",
    "        elif begin.hour < 12 <= end.hour:\n",
    "            rows.append({'begin': begin, 'end': end.replace(hour=11, minute=59, second=59), **rest})\n",
    "            rows.append({'begin': end.replace(hour=12, minute=0, second=0), 'end': end, **rest})\n",
    "            return rows\n",
    "        else:\n",
    "            return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in df.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "dates_to_keep = [datetime.date(2020, 11, 26),\n",
    "                 *pd.date_range(datetime.date(2020, 12, 21), datetime.date(2020, 12, 25)).values,\n",
    "                 *pd.date_range(datetime.date(2021, 2, 1), datetime.date(2021, 2, 12)).values,\n",
    "                 *pd.date_range(datetime.date(2021, 8, 16), datetime.date(2021, 8, 28)).values,\n",
    "                 *pd.date_range(datetime.date(2021, 9, 20), datetime.date(2021, 10, 3)).values,\n",
    "                 *pd.date_range(datetime.date(2021, 11, 25), datetime.date(2021, 12, 12)).values,\n",
    "                 *pd.date_range(datetime.date(2022, 4, 25), datetime.date(2022, 5, 13)).values]\n",
    "\n",
    "\n",
    "def run_pipeline_3(agg: pd.DataFrame, name: str = 'time_per_status') -> Tuple[pd.DataFrame, ...]:\n",
    "    agg = split(agg)\n",
    "    # now we can compute all of these time properties since they will be the same for begin and end\n",
    "    agg['date'] = agg.begin.dt.date\n",
    "    agg['day_of_week'] = agg.begin.dt.day_name()\n",
    "    agg['day_type'] = (agg.begin.dt.weekday < 5).replace({True: 'week day', False: 'weekend'})\n",
    "    agg['time_of_day'] = (agg.begin.dt.hour < 12).replace({True: 'AM', False: 'PM'})\n",
    "    agg['duration'] = agg.end - agg.begin\n",
    "    grouped = agg.groupby(['date', 'day_of_week', 'day_type', 'time_of_day', 'status'])['duration'].sum().reset_index()\n",
    "    grouped.to_csv(processed_folder / f'{name}.csv', index=False)\n",
    "    grouped['date'] = pd.to_datetime(grouped.date)\n",
    "\n",
    "    percentage = pd.read_csv(data_folder / 'percentage.csv')\n",
    "    percentage['Uber'] /= 100\n",
    "    waiting = grouped.loc[grouped.status == 'waiting'].copy()\n",
    "    for i, row in percentage.iterrows():\n",
    "        if row['Uber'] == 0 and waiting[(waiting.date.dt.year == row.year) & (\n",
    "                waiting.date.dt.month == row.month)].duration.sum() > datetime.timedelta(0):\n",
    "            print(\n",
    "                f'bad specification for {row.year}/{row.month}: activity found even though specified percentage was 0')\n",
    "        waiting['duration_seconds'] = np.where(\n",
    "            (waiting.date.dt.year == row.year) & (waiting.date.dt.month == row.month),\n",
    "            waiting['duration'] * row['Uber'],\n",
    "            waiting['duration'])\n",
    "    waiting.drop(waiting[(waiting.time_of_day == 'AM') &\n",
    "                         (waiting.day_type == 'week day') &\n",
    "                         ~waiting.date.apply(lambda d: d.date()).isin(dates_to_keep)].index,\n",
    "                 inplace=True)\n",
    "    waiting.to_csv(processed_folder / f'{name}_waiting.csv', index=False)\n",
    "    concat = pd.concat([grouped, waiting.replace({'waiting': 'waiting (filter)'})])\n",
    "    per_year = concat.groupby([concat.date.dt.year, 'status'])['duration'].sum().reset_index()\n",
    "    per_year.to_csv(processed_folder / f'{name}_total.csv', index=False)\n",
    "    return grouped, concat, per_year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing data from `02-period_df.csv` into a format of (begin, end, status)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "df_02 = pd.read_csv(processed_folder / '02-period_df.csv',\n",
    "                    usecols=['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local',\n",
    "                             'status'])\n",
    "df_02 = df_02[df_02.status.isin(['completed', 'fare_split'])].drop(columns='status')\n",
    "df_02.columns = ['request', 'begintrip', 'dropoff']\n",
    "\n",
    "# This maps each record of (request, begintrip, dropoff) into two records: (request, begintrip, en route) and (begintrip, dropoff, on trip)\n",
    "df_02 = pd.DataFrame(df_02.apply(lambda r: [{'begin': r['request'], 'end': r['begintrip'], 'status': 'en route'},\n",
    "                                            {'begin': r['begintrip'], 'end': r['dropoff'], 'status': 'on trip'}],\n",
    "                                 axis=1).explode().to_list())\n",
    "for c in ['begin', 'end']:\n",
    "    df_02[c] = pd.to_datetime(df_02[c])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing data from `05-period_df.csv` into a format of (begin, end, status)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "df_05 = pd.read_csv(processed_folder / '05-period_df.csv',\n",
    "                    usecols=['begin_timestamp_local', 'end_timestamp_local', 'status'])\n",
    "df_05.columns = ['begin', 'end', 'status']\n",
    "for c in ['begin', 'end']:\n",
    "    df_05[c] = pd.to_datetime(df_05[c])\n",
    "df_05.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternative 1: ignore any status duration that happens when the driver is offline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "offline_05 = df_to_interval(df_05, 'offline')\n",
    "on_trip_05 = df_to_interval(df_05, 'on trip')\n",
    "en_route_05 = df_to_interval(df_05, 'en route') - on_trip_05\n",
    "open_05 = df_to_interval(df_05, 'open')\n",
    "\n",
    "on_trip_02 = df_to_interval(df_02, 'on trip') - offline_05\n",
    "en_route_02 = df_to_interval(df_02, 'en route') - on_trip_02 - offline_05"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternative 2: ignore the offline status completely"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "on_trip_02 = df_to_interval(df_02, 'on trip')\n",
    "en_route_02 = df_to_interval(df_02, 'en route') - on_trip_02\n",
    "\n",
    "on_trip_05 = df_to_interval(df_05, 'on trip')\n",
    "en_route_05 = df_to_interval(df_05, 'en route') - on_trip_05\n",
    "open_05 = df_to_interval(df_05, 'open')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "on_trip = on_trip_02 | on_trip_05\n",
    "en_route = (en_route_02 | en_route_05) - on_trip\n",
    "waiting = open_05 - (en_route | on_trip)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "merged = pd.concat(\n",
    "    [interval_to_df(on_trip, 'on trip'),\n",
    "     interval_to_df(en_route, 'en route'),\n",
    "     interval_to_df(waiting, 'waiting')])\n",
    "grouped, concat, per_year = run_pipeline_3(merged, name='time_per_status')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
