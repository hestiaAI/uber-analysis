{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Uber Driver Data Analysis\n",
    "\n",
    "(New version)\n",
    "\n",
    "Works with SAR and Portal data.\n",
    "\n",
    "Note that this notebook is not final, and more documentation is being added."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import portion as P"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'\n",
    "raw_folder = data_folder / 'raw'\n",
    "processed_folder = data_folder / 'processed'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def find_file(pattern: str, folder: Path) -> Path:\n",
    "    matches = glob.glob(pattern, root_dir=folder)\n",
    "    if len(matches) == 0:\n",
    "        raise ValueError(f'Could not find file {pattern} in {folder}')\n",
    "    elif len(matches) > 1:\n",
    "        print(f'Found many matches for {pattern} in {folder}. Using the first.')\n",
    "    return folder / matches[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specific to SAR data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def make_status_intervals(df: pd.DataFrame) -> dict[str, P.interval]:\n",
    "    return {s: df_to_interval(df[df.status == s]) for s in df.status.unique()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def interval_merge_logic(lt: dict[str, P.interval], oo: dict[str, P.interval]) -> dict[str, pd.DataFrame]:\n",
    "    P3 = lt['P3'] | oo['P3']\n",
    "    P2 = (lt['P2'] | oo['P2']) - P3\n",
    "    P1 = oo['P1'] - (P2 | P3)\n",
    "    return {'P1': P1, 'P2': P2, 'P3': P3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def main_interval_logic(lt: dict[str, P.interval], oo: dict[str, P.interval], P0_has_priority=False) -> pd.DataFrame:\n",
    "    \"\"\"Consider the following ordering of priorities: P3 > P2 > P1. P0_has_priority determines if P0 is on the left or right of inequalities.\"\"\"\n",
    "    if P0_has_priority:\n",
    "        for d in [lt, oo]:\n",
    "            for k in d.keys():\n",
    "                if k != 'P0':\n",
    "                    d[k] = d[k] - oo['P0']\n",
    "    lt['P2'] = lt['P2'] - lt['P3']\n",
    "    oo['P2'] = oo['P2'] - oo['P3']\n",
    "    oo['P1'] = oo['P1'] - (oo['P2'] | oo['P3'])\n",
    "    intervals = interval_merge_logic(lt, oo)\n",
    "    return pd.concat([interval_to_df(i).assign(status=f'{s} consistent') for s, i in intervals.items()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### One processing function per file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def process_lifetime_trips(path: str | Path) -> pd.DataFrame:\n",
    "    usecols = ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status']\n",
    "    df = pd.read_csv(path, usecols=usecols)[usecols]\n",
    "    df = df[df.status.isin(['completed', 'fare_split'])].drop(columns='status')\n",
    "    df.columns = ['request', 'begintrip', 'dropoff']\n",
    "    df = time_tuples_to_periods(df, columns=df.columns,\n",
    "                                extra_info=[lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def process_on_off(path: str | Path) -> pd.DataFrame:\n",
    "    usecols = ['begin_timestamp_local', 'end_timestamp_local', 'earner_state']\n",
    "    df = pd.read_csv(path, usecols=usecols)[usecols]\n",
    "    df.columns = ['begin', 'end', 'status']\n",
    "    df = df.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def process_dispatches(path: str | Path) -> pd.DataFrame:\n",
    "    # TODO (not finished)\n",
    "    df = pd.read_csv(path, usecols=['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                                    'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                                    'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def process_trip_status(path: str | Path) -> pd.DataFrame:\n",
    "    pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def process_distance_traveled(path: str | Path) -> pd.DataFrame:\n",
    "    pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def process_sar(\n",
    "        folder: Path,\n",
    "        interval_logic: Callable[[dict[str, pd.DataFrame], dict[str, pd.DataFrame]], pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    lifetime_trips = process_lifetime_trips(find_file('*Driver Lifetime Trips.csv', folder))\n",
    "    on_off = process_on_off(find_file('*Driver Online Offline.csv', folder))\n",
    "    return interval_logic(*[make_status_intervals(df) for df in [lifetime_trips, on_off]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specific to Portal data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def process_portal(folder: Path):\n",
    "    usecols = ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "               'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare']\n",
    "    df = pd.read_csv(find_file('*driver_lifetime_trips*.csv', folder), usecols=usecols)[usecols]\n",
    "    df = df[df['Status'].isin(['completed', 'fare_split'])]\n",
    "    df = time_tuples_to_periods(df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                                             'Local Dropoff Timestamp'],\n",
    "                                extra_info=[lambda r: {'status': 'P2'},\n",
    "                                            lambda r: {'status': 'P3',\n",
    "                                                       'distance_km': r['Trip Distance (miles)'] * 1.60934,\n",
    "                                                       'uber_pay': r['Local Original Fare']}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Common to SAR and Portal data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        df: pd.DataFrame,\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    Args:\n",
    "        - df: a table having a number N > 1 of time-columns and L of entries.\n",
    "        - columns: a list of n time-column names.\n",
    "        - extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    Returns:\n",
    "        - periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Ex:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(df.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def df_to_interval(df: pd.DataFrame) -> P.interval:\n",
    "    \"\"\"Converts a DataFrame with columns 'begin' and 'end' into a Portion interval, merging entries that overlap.\"\"\"\n",
    "    return P.Interval(*[P.closed(row['begin'], row['end']) for row in df.to_dict('records')])\n",
    "\n",
    "\n",
    "def interval_to_df(interval: P.interval) -> pd.DataFrame:\n",
    "    \"\"\"Converts a Portion interval into a dataframe with columns 'begin' and 'end'.\"\"\"\n",
    "    return pd.DataFrame([{'begin': begin, 'end': end} for _, begin, end, _ in P.to_data(interval)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def split_in_half_days(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Makes sure that intervals spanning many days or many morning/afternoon periods are split\"\"\"\n",
    "\n",
    "    # TODO write doc and give a better name\n",
    "    def rec(begin: datetime.datetime, end: datetime.datetime, **rest) -> list[dict]:\n",
    "        rows = []\n",
    "        if begin.day != end.day:\n",
    "            rows.append({'begin': begin, 'end': begin.replace(hour=23, minute=59, second=59), **rest})\n",
    "            for days in range(end.day - begin.day - 1):\n",
    "                inbetween = (begin + datetime.timedelta(days=days))\n",
    "                rows.append({'begin': inbetween.replace(hour=0, minute=0, second=0),\n",
    "                             'end': inbetween.replace(hour=23, minute=59, second=59), **rest})\n",
    "            rows.append({'begin': end.replace(hour=0, minute=0, second=0), 'end': end, **rest})\n",
    "            return [e for r in rows for e in rec(**r)]\n",
    "        elif begin.hour < 12 <= end.hour:\n",
    "            rows.append({'begin': begin, 'end': end.replace(hour=11, minute=59, second=59), **rest})\n",
    "            rows.append({'begin': end.replace(hour=12, minute=0, second=0), 'end': end, **rest})\n",
    "            return rows\n",
    "        else:\n",
    "            return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in df.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: pd.DataFrame,\n",
    "        filtering_logic: Optional[Callable[[pd.DataFrame], pd.DataFrame]] = None,\n",
    "        name: str = 'time_per_status',\n",
    "        facets: list[str] = ['duration']\n",
    ") -> Tuple[pd.DataFrame, ...]:\n",
    "    # Make sure that each interval is contained within a single 12-hour time period (AM or PM) by splitting when necessary\n",
    "    splitted = split_in_half_days(periods)\n",
    "\n",
    "    # Now we can compute all of these time properties since they will be the same for begin and end\n",
    "    splitted['date'] = splitted.end.dt.date\n",
    "    splitted['day_of_week'] = splitted.end.dt.day_name()\n",
    "    splitted['day_type'] = (splitted.end.dt.weekday < 5).replace({True: 'week day', False: 'weekend'})\n",
    "    splitted['time_of_day'] = (splitted.end.dt.hour < 12).replace({True: 'AM', False: 'PM'})\n",
    "    splitted['week'] = splitted.end.dt.isocalendar().week\n",
    "    splitted['year'] = splitted.end.dt.year\n",
    "    splitted['duration'] = splitted.end - splitted.begin\n",
    "\n",
    "    # Group entries by day and time period\n",
    "    daily = splitted.groupby(['date', 'year', 'week', 'day_of_week',\n",
    "                              'day_type', 'time_of_day', 'status']).aggregate({f: 'sum' for f in facets}).reset_index()\n",
    "    daily['date'] = pd.to_datetime(daily.date)\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        filtered = filtering_logic(daily)\n",
    "        filtered['status'] = filtered.status + '|filtered'\n",
    "        daily = pd.concat([daily, filtered])\n",
    "\n",
    "    processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "    daily.to_csv(processed_folder / f'{name}_daily.csv', index=False)\n",
    "    weekly = daily.groupby(['year', 'week', 'status']).aggregate({f: 'sum' for f in facets}).reset_index()\n",
    "    weekly.to_csv(processed_folder / f'{name}_weekly.csv', index=False)\n",
    "    yearly = weekly.groupby(['year', 'status']).aggregate({f: 'sum' for f in facets}).reset_index()\n",
    "    yearly.to_csv(processed_folder / f'{name}_yearly.csv', index=False)\n",
    "    total = yearly.groupby(['status']).aggregate({f: 'sum' for f in facets}).reset_index()\n",
    "    total.to_csv(processed_folder / f'{name}_total.csv', index=False)\n",
    "    return daily, weekly, yearly, total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tying everything together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: pd.DataFrame,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'percentage.csv'\n",
    ") -> pd.DataFrame:\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    P1 = daily.loc[daily.status.str.contains('P1')].copy()\n",
    "    for i, row in percentage.iterrows():\n",
    "        if row['Uber'] == 0 and P1[(P1.date.dt.year == row.year) & (\n",
    "                P1.date.dt.month == row.month)].duration.sum() > datetime.timedelta(0):\n",
    "            print(\n",
    "                f'bad specification for {row.year}/{row.month}: activity found even though specified percentage was 0')\n",
    "        P1['duration'] = np.where(\n",
    "            (P1.date.dt.year == row.year) & (P1.date.dt.month == row.month),\n",
    "            P1['duration'] * row['Uber'],\n",
    "            P1['duration'])\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26),\n",
    "                     *pd.date_range(datetime.date(2020, 12, 21), datetime.date(2020, 12, 25)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 2, 1), datetime.date(2021, 2, 12)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 8, 16), datetime.date(2021, 8, 28)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 9, 20), datetime.date(2021, 10, 3)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 11, 25), datetime.date(2021, 12, 12)).values,\n",
    "                     *pd.date_range(datetime.date(2022, 4, 25), datetime.date(2022, 5, 13)).values]\n",
    "    P1.drop(P1[(P1.time_of_day == 'AM') &\n",
    "               (P1.day_type == 'week day') &\n",
    "               ~P1.date.apply(lambda d: d.date()).isin(dates_to_keep)].index,\n",
    "            inplace=True)\n",
    "    return P1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "periods = process_sar(raw_folder / 'new', interval_logic=main_interval_logic)\n",
    "_ = pipeline(periods, name='sar',\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             facets=['duration'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "periods = process_portal(raw_folder / 'portal' / 'Driver')\n",
    "_ = pipeline(periods, name='portal',\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             facets=['duration', 'distance_km', 'uber_pay'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
