{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Uber Driver Data Analysis\n",
    "\n",
    "(New version)\n",
    "\n",
    "Works with SAR and Portal data.\n",
    "\n",
    "Note that this notebook is not final, and more documentation is being added."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [Optional] Installing the required libraries with pip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (1.23.3)\r\n",
      "Requirement already satisfied: pandas in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (1.4.4)\r\n",
      "Requirement already satisfied: portion in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: sortedcontainers~=2.2 in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (from portion) (2.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/lstreit/anaconda3/envs/hestia/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas portion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import portion as P"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Table(pd.DataFrame):\n",
    "    \"\"\"Some Python magic to be able to type hint things like\n",
    "    df: Table['begin': datetime, 'end': datetime]\"\"\"\n",
    "\n",
    "    def __class_getitem__(cls, _):\n",
    "        return list[str]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Timestamp = datetime.datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "PeriodTable = Table['begin': Timestamp, 'end': Timestamp]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def find_file(pattern: str, folder: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Looks for a file matching the given pattern inside the given folder\n",
    "    :param pattern: a glob file pattern\n",
    "    :param folder: a path leading to a folder where the pattern should be applied\n",
    "    :return: a path to the first file matching the pattern, or a value error if none.\n",
    "    \"\"\"\n",
    "    matches = glob.glob(pattern, root_dir=folder)\n",
    "    if len(matches) == 0:\n",
    "        raise ValueError(f'Could not find file {pattern} in {folder}')\n",
    "    elif len(matches) > 1:\n",
    "        print(f'Found many matches for {pattern} in {folder}. Using the first.')\n",
    "    return folder / matches[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def find_table(pattern: str, folder: Path, usecols: Optional[list[str]] = None) -> Table:\n",
    "    \"\"\"\n",
    "    Looks for a csv-like file whose name matches the pattern and is located inside folder, using only the specified columns.\n",
    "    If found, reads it and assigns the file name as a column.\n",
    "    :param pattern: a glob file pattern\n",
    "    :param folder: a path leading to a folder where the pattern should be applied\n",
    "    :param usecols: an optional list of columns in the\n",
    "    :return: a Table (a.k.a. pandas DataFrame) having the specified columns and the file name as a column\n",
    "    \"\"\"\n",
    "    file = find_file(pattern, folder)\n",
    "    table = pd.read_csv(file, usecols=usecols)\n",
    "    if usecols is not None:\n",
    "        table = table[usecols]\n",
    "    return table.assign(file=file.name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def find_date_range(pattern: str, folder: Path, date_cols: list[str]) -> (Timestamp, Timestamp):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pattern: a glob file pattern\n",
    "    :param folder: a path leading to a folder where the pattern should be applied\n",
    "    :param date_cols: a list of column names that are dates\n",
    "    :return: the minimum and maxixmum observed dates\n",
    "    Usage:\n",
    "    find_date_range('*Driver Trip Status.csv', data_folder / 'Brice' / 'raw' / 'SAR',\n",
    "                    ['begin_timestamp_local', 'end_timestamp_local'])\n",
    "    \"\"\"\n",
    "    df = find_table(pattern, folder, usecols=date_cols)\n",
    "    for c in date_cols:\n",
    "        df[c] = pd.to_datetime(df[c])\n",
    "    return df[date_cols].min().min(), df[date_cols].max().max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def df_to_interval(df: PeriodTable) -> P.interval:\n",
    "    \"\"\"Converts a DataFrame with columns 'begin' and 'end' into a Portion interval, merging entries that overlap.\"\"\"\n",
    "    return P.Interval(*[P.closed(row['begin'], row['end']) for row in df.to_dict('records')])\n",
    "\n",
    "\n",
    "def interval_to_df(interval: P.interval) -> PeriodTable:\n",
    "    \"\"\"Converts a Portion interval into a dataframe with columns 'begin' and 'end'.\"\"\"\n",
    "    return pd.DataFrame([{'begin': begin, 'end': end} for _, begin, end, _ in P.to_data(interval)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        table: Table['t1': Timestamp, 't2': Timestamp, 't3': Timestamp],\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    :param: table: a table having a number N > 1 of time-columns and L of entries.\n",
    "    :param: columns: a list of n time-column names present in {table}.\n",
    "    :param: extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    :return: periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Usage:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(table.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def make_status_intervals(df: PeriodTable) -> dict[str, P.interval]:\n",
    "    return {s: df_to_interval(df[df.status == s]) for s in df.status.unique()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def interval_merge_logic(lt: dict[str, P.interval], oo: dict[str, P.interval]) -> dict[str, P.interval]:\n",
    "    P3 = lt['P3'] | oo['P3']\n",
    "    P2 = (lt['P2'] | oo['P2']) - P3\n",
    "    P1 = oo['P1'] - (P2 | P3)\n",
    "    return {'P1': P1, 'P2': P2, 'P3': P3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def main_interval_logic(lt: dict[str, P.interval], oo: dict[str, P.interval], P0_has_priority=False) -> Table:\n",
    "    \"\"\"Consider the following ordering of priorities: P3 > P2 > P1. P0_has_priority determines if P0 is on the left or right of inequalities.\"\"\"\n",
    "    if P0_has_priority:\n",
    "        for d in [lt, oo]:\n",
    "            for k in d.keys():\n",
    "                if k != 'P0':\n",
    "                    d[k] = d[k] - oo['P0']\n",
    "    lt['P2'] = lt['P2'] - lt['P3']\n",
    "    oo['P2'] = oo['P2'] - oo['P3']\n",
    "    oo['P1'] = oo['P1'] - (oo['P2'] | oo['P3'])\n",
    "    intervals = interval_merge_logic(lt, oo)\n",
    "    return pd.concat([interval_to_df(i).assign(status=f'{s} consistent') for s, i in intervals.items()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(table: PeriodTable, agg_dict: Optional[dict] = None) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Groups the given table by status, then sorts all intervals by begin datetime and merges overlapping entries \"efficiently\".\n",
    "    :param table: the table whose intervals should be merged\n",
    "    :param agg_dict: maps each row to the operation that should be applied to combine interval attributes when merging them\n",
    "    :return: a table with merged intervals per status\n",
    "    \"\"\"\n",
    "    agg_dict = agg_dict or {c: 'sum' for c in table.columns if c not in ['begin', 'end', 'status']}\n",
    "\n",
    "    def handle_groups(table: PeriodTable) -> PeriodTable:\n",
    "        intervals = table[['begin', 'end']].sort_values(['begin', 'end'])\n",
    "        group = 0\n",
    "        group_end = intervals.end.iloc[0]\n",
    "\n",
    "        def find_group(row):\n",
    "            nonlocal group, group_end\n",
    "            if row.begin <= group_end:\n",
    "                group_end = max(row.end, group_end)\n",
    "            else:\n",
    "                group += 1\n",
    "                group_end = row.end\n",
    "            return group\n",
    "\n",
    "        groups = intervals.apply(find_group, axis=1)\n",
    "        return table.groupby(groups).agg({'begin': 'min', 'end': 'max', **agg_dict}).reset_index(drop=True)\n",
    "\n",
    "    return table.groupby('status').apply(handle_groups).reset_index(level=1, drop=True).reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def mile2km(n_miles: float) -> float:\n",
    "    return n_miles * 1.609344"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SAR preprocessing logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def load_lifetime_trips(folder: Path, pattern: str = '*Driver Lifetime Trips.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status',\n",
    "                        'request_to_begin_distance_miles', 'trip_distance_miles', 'original_fare_local'])\n",
    "    table = table[table.status == 'completed'].drop(columns='status')\n",
    "    table.replace({r'\\N': np.nan}, inplace=True)\n",
    "    for col in ['request_to_begin_distance_miles', 'original_fare_local']:\n",
    "        table[col] = table[col].astype(float)\n",
    "    table = time_tuples_to_periods(table,\n",
    "                                   columns=['request_timestamp_local', 'begintrip_timestamp_local',\n",
    "                                            'dropoff_timestamp_local'],\n",
    "                                   extra_info=[lambda r: {'status': 'P2',\n",
    "                                                          'distance_km': mile2km(r['request_to_begin_distance_miles']),\n",
    "                                                          'file': r['file']},\n",
    "                                               lambda r: {'status': 'P3',\n",
    "                                                          'distance_km': mile2km(r['trip_distance_miles']),\n",
    "                                                          'uber_paid': r['original_fare_local'],\n",
    "                                                          'file': r['file']}])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def load_on_off(folder: Path, pattern: str = '*Driver Online Offline.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['begin_timestamp_local', 'end_timestamp_local', 'earner_state',\n",
    "                        'begin_lat', 'begin_lng', 'end_lat', 'end_lng'])\n",
    "    table.rename(columns={'begin_timestamp_local': 'begin', 'end_timestamp_local': 'end',\n",
    "                          'earner_state': 'status'}, inplace=True)\n",
    "    table = table.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def load_dispatches(folder: Path, pattern: str = 'TODO') -> Table:\n",
    "    # TODO (not finished)\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                        'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                        'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def load_trip_status(folder: Path, pattern: str = '*Driver Trip Status.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder, ['begin_timestamp_local', 'end_timestamp_local', 'status', 'end_reason'])\n",
    "    table.columns = ['begin', 'end', *table.columns[2:]]\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def load_distance_traveled(folder: Path, pattern: str = 'TODO') -> pd.DataFrame:\n",
    "    pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def load_sar(folder: Path) -> Table:\n",
    "    lifetime_trips = load_lifetime_trips(folder)\n",
    "    on_off = load_on_off(folder)\n",
    "    return pd.concat([lifetime_trips, on_off]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Portal preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def load_portal(folder: Path):\n",
    "    df = find_table('*driver_lifetime_trips*.csv', folder,\n",
    "                    ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "                     'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    df = df[df['Status'] == 'completed']\n",
    "    df = time_tuples_to_periods(df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                                             'Local Dropoff Timestamp'],\n",
    "                                extra_info=[lambda r: {'status': 'P2'},\n",
    "                                            lambda r: {'status': 'P3',\n",
    "                                                       'distance_km': mile2km(r['Trip Distance (miles)']),\n",
    "                                                       'uber_paid': r['Local Original Fare']}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Guillaume-specific logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: Table,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'Guillaume' / 'percentage.csv'\n",
    ") -> Table:\n",
    "    # TODO fix, not working anymore\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    daily['datetime'] = pd.to_datetime(daily['date'])\n",
    "    duration_P1_cols = list(filter(lambda c: 'duration_P1' in c and 'AM' in c and 'weekday' in c, daily.columns))\n",
    "    new_cols = []\n",
    "    for c in duration_P1_cols:\n",
    "        for i, row in percentage.iterrows():\n",
    "            daily[f'{c}(filtered)'] = np.where(\n",
    "                (daily.datetime.dt.year == row.year) & (daily.datetime.dt.month == row.month),\n",
    "                daily[c] * row['Uber'], daily[c])\n",
    "        new_cols.append(f'{c}(filtered)')\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26),\n",
    "                     *pd.date_range(datetime.date(2020, 12, 21), datetime.date(2020, 12, 25)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 2, 1), datetime.date(2021, 2, 12)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 8, 16), datetime.date(2021, 8, 28)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 9, 20), datetime.date(2021, 10, 3)).values,\n",
    "                     *pd.date_range(datetime.date(2021, 11, 25), datetime.date(2021, 12, 12)).values,\n",
    "                     *pd.date_range(datetime.date(2022, 4, 25), datetime.date(2022, 5, 13)).values]\n",
    "    daily.loc[daily[~daily.datetime.apply(lambda d: d.date()).isin(dates_to_keep)].index, new_cols] = 0\n",
    "    return daily[new_cols]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def split_AM_PM(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many days or many morning/afternoon periods in two.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def scaled_interval(begin: Timestamp, end: Timestamp, attributes: dict, og_duration) -> dict:\n",
    "        \"\"\"Creates an interval (a dict with begin, end and other attributes) given the original attributes and the original \"\"\"\n",
    "        return {'begin': begin, 'end': end,\n",
    "                **{k: v * (end - begin) / og_duration if isinstance(v, float) else v for k, v in attributes.items()}}\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.day != end.day:\n",
    "            rows = [scaled_interval(begin, begin.replace(hour=23, minute=59, second=59), rest, og_duration)]\n",
    "            for days in range(end.day - begin.day - 1):\n",
    "                mid = begin + datetime.timedelta(days=days)\n",
    "                rows.append(scaled_interval(mid.replace(hour=0, minute=0, second=0),\n",
    "                                            mid.replace(hour=23, minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(hour=0, minute=0, second=0), end, rest, og_duration))\n",
    "            # Call itself recursively to split resulting days into AM/PM\n",
    "            return [e for r in rows for e in rec(**r)]\n",
    "        # Check if the interval spans many AM/PM periods\n",
    "        elif begin.hour < 12 <= end.hour:\n",
    "            return [scaled_interval(begin, end.replace(hour=11, minute=59, second=59), rest, og_duration),\n",
    "                    scaled_interval(end.replace(hour=12, minute=0, second=0), end, rest, og_duration)]\n",
    "        else:\n",
    "            return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def split_hours(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many hours periods in as many intervals as hours covered by the interval.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def scaled_interval(begin: Timestamp, end: Timestamp, attributes: dict, og_duration) -> dict:\n",
    "        \"\"\"Creates an interval (a dict with begin, end and other attributes) given the original attributes and the original \"\"\"\n",
    "        return {'begin': begin, 'end': end,\n",
    "                **{k: v * (end - begin) / og_duration if isinstance(v, float) else v for k, v in attributes.items()}}\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.hour != end.hour:\n",
    "            rows = [scaled_interval(begin, begin.replace(minute=59, second=59), rest, og_duration)]\n",
    "            for hours in range(end.hour - begin.hour - 1):\n",
    "                mid = begin + datetime.timedelta(hours=hours)\n",
    "                rows.append(scaled_interval(mid.replace(minute=0, second=0),\n",
    "                                            mid.replace(minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(minute=0, second=0), end, rest, og_duration))\n",
    "            return rows\n",
    "        return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def find_week_limits(date: Timestamp) -> str:\n",
    "    week_start = date - datetime.timedelta(days=date.weekday())\n",
    "    week_end = week_start + datetime.timedelta(days=6)\n",
    "    return f'{week_start.date()} to {week_end.date()}'.replace('-', '/').replace('to', '-')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def select(d: dict[str], keep: Optional[list[str]] = None, drop: Optional[list[str]] = None) -> dict[str]:\n",
    "    assert (keep is None) != (drop is None), 'Only one of keep or drop can be specified'\n",
    "    if keep is not None:\n",
    "        return {k: v for k, v in d.items() if k in keep}\n",
    "    if drop is not None:\n",
    "        return {k: v for k, v in d.items() if k not in drop}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running the pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "all_facets = ['duration_h', 'distance_km', 'uber_paid']\n",
    "all_time_properties = {'day_of_week': lambda d: d.day_name(),\n",
    "                       'day_type': lambda d: 'weekday' if d.weekday() < 5 else 'weekend',\n",
    "                       'time_of_day': lambda d: 'AM' if d.hour < 12 else 'PM',\n",
    "                       'night': lambda d: 'night' if d.hour <= 6 or 23 < d.hour else 'day'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: PeriodTable,\n",
    "        interval_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        filtering_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        time_properties: Optional[dict[str, Callable[[Timestamp], Any]]] = None,\n",
    "        name: str = 'analysis',\n",
    "        facets: list[str] = all_facets,\n",
    "        save_at: Path = data_folder / 'results',\n",
    ") -> Tuple[pd.DataFrame, ...]:\n",
    "    # Apply interval logic if specified\n",
    "    if interval_logic is not None:\n",
    "        periods = interval_logic(periods)\n",
    "\n",
    "    # Compute the duration of a period once, in the beginning\n",
    "    periods['duration_h'] = (periods.end - periods.begin) / datetime.timedelta(hours=1)\n",
    "    # Split intervals spanning many hours\n",
    "    periods = split_hours(periods)\n",
    "\n",
    "    counts = periods[['begin', 'end']].value_counts()\n",
    "    duplicates = counts[counts > 1].reset_index()\n",
    "    display(periods[periods.begin.isin(duplicates.begin) & periods.end.isin(duplicates.end)])\n",
    "\n",
    "    # Pivot table so that each there is a single line per interval and per granularity of interest\n",
    "    periods = periods.pivot(index=['begin', 'end'], columns=['status'], values=facets).reset_index()\n",
    "    periods[periods == 0] = np.nan\n",
    "    periods.drop(columns=periods.columns[periods.isna().all()], inplace=True)  # this and previous remove columns that have only 0/nans\n",
    "    # Merges column multiindex into a single index by joining the levels\n",
    "    periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    date_info = list(time_properties.keys()) if time_properties is not None else []\n",
    "\n",
    "    # Compute these datetime properties since they will be the same for begin and end (thanks to split_hours)\n",
    "    if time_properties is not None:\n",
    "        for k, f in time_properties.items():\n",
    "            periods[k] = periods.end.apply(f)\n",
    "        periods = periods.pivot(index=['begin', 'end'], columns=date_info, values=list(agg_dict.keys())).reset_index()\n",
    "        periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "        agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "    periods['hour'] = periods.end.apply(lambda d: f'{d.hour}-{(d + datetime.timedelta(hours=1)).hour}')\n",
    "    periods['date'] = periods.end.dt.date\n",
    "    periods['week'] = periods.end.apply(find_week_limits)\n",
    "    periods['month'] = periods.end.apply(lambda d: f'{d.month:02d}. {d.month_name()}')\n",
    "    periods['year'] = periods.end.dt.year\n",
    "\n",
    "    hourly = periods.groupby(['date', 'hour']).agg(agg_dict).reset_index()\n",
    "    daily = periods.groupby(['date']).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        daily = pd.concat([daily, filtering_logic(daily)], axis=1)\n",
    "\n",
    "    weekly = periods.groupby(['week']).agg(agg_dict).reset_index()\n",
    "    monthly = periods.groupby(['year', 'month']).agg(agg_dict).reset_index()\n",
    "    yearly = periods.groupby(['year']).agg(agg_dict).reset_index()\n",
    "    total = yearly.agg(agg_dict).to_frame().T\n",
    "\n",
    "    # Writing tables to disk\n",
    "    save_at.mkdir(parents=True, exist_ok=True)\n",
    "    periods.to_csv(save_at / f'0_{name}_periods.csv', index=False, float_format='%.3f')\n",
    "    hourly.to_csv(save_at / f'1_{name}_hourly.csv', index=False, float_format='%.3f')\n",
    "    daily.to_csv(save_at / f'2_{name}_daily.csv', index=False, float_format='%.3f')\n",
    "    weekly.to_csv(save_at / f'3_{name}_weekly.csv', index=False, float_format='%.3f')\n",
    "    monthly.to_csv(save_at / f'4_{name}_monthly.csv', index=False, float_format='%.3f')\n",
    "    yearly.to_csv(save_at / f'5_{name}_yearly.csv', index=False, float_format='%.3f')\n",
    "    total.to_csv(save_at / f'6_{name}_total.csv', index=False, float_format='%.3f')\n",
    "\n",
    "    return hourly, daily, weekly, yearly, total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "_ = pipeline(load_sar(data_folder / 'Guillaume' / 'raw' / 'SAR (new)'),\n",
    "             interval_logic=lambda t: merge_overlapping_intervals(t, {c: 'sum' for c in ['uber_paid', 'distance_km']}),\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             name='sar', time_properties=select(all_time_properties, keep=['time_of_day']),\n",
    "             save_at=data_folder / 'Guillaume' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "_ = pipeline(load_sar(data_folder / 'Kidane' / 'raw' / 'SAR'),\n",
    "             interval_logic=lambda t: merge_overlapping_intervals(t, {c: 'sum' for c in ['uber_paid', 'distance_km']}),\n",
    "             name='sar', time_properties=all_time_properties,\n",
    "             save_at=data_folder / 'Kidane' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "                          begin                       end status  uber_paid  \\\n54915 2021-08-29 18:00:00+00:00 2021-08-29 18:00:38+00:00     P2   0.000000   \n54957 2021-08-31 11:00:00+00:00 2021-08-31 11:04:43+00:00     P2   0.000000   \n55026 2021-09-02 17:00:00+00:00 2021-09-02 17:02:45+00:00     P2   0.000000   \n55081 2021-09-06 16:00:00+00:00 2021-09-06 16:00:04+00:00     P2   0.000000   \n55195 2021-09-11 21:00:00+00:00 2021-09-11 21:00:09+00:00     P2   0.000000   \n73247 2021-08-29 18:00:00+00:00 2021-08-29 18:00:38+00:00     P3   1.231030   \n73274 2021-08-31 11:00:00+00:00 2021-08-31 11:04:43+00:00     P3   4.823799   \n73323 2021-09-02 17:00:00+00:00 2021-09-02 17:02:45+00:00     P3   8.644030   \n73359 2021-09-06 16:00:00+00:00 2021-09-06 16:00:04+00:00     P3   0.158475   \n73426 2021-09-11 21:00:00+00:00 2021-09-11 21:00:09+00:00     P3   0.338265   \n78214 2022-10-30 02:48:01+00:00 2022-10-30 02:01:47+00:00     P3  46.100000   \n78215 2022-10-30 02:48:01+00:00 2022-10-30 02:01:47+00:00     P3   0.000000   \n\n       distance_km  duration_h  \n54915     0.000000    0.010556  \n54957     0.000000    0.078611  \n55026     0.003945    0.045833  \n55081     0.000000    0.001111  \n55195     0.000000    0.002500  \n73247     0.248395    0.010556  \n73274     1.065423    0.078611  \n73323     1.113123    0.045833  \n73359     0.022923    0.001111  \n73426     0.056242    0.002500  \n78214     6.515524   -0.770556  \n78215     0.000000   -0.770556  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>begin</th>\n      <th>end</th>\n      <th>status</th>\n      <th>uber_paid</th>\n      <th>distance_km</th>\n      <th>duration_h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>54915</th>\n      <td>2021-08-29 18:00:00+00:00</td>\n      <td>2021-08-29 18:00:38+00:00</td>\n      <td>P2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010556</td>\n    </tr>\n    <tr>\n      <th>54957</th>\n      <td>2021-08-31 11:00:00+00:00</td>\n      <td>2021-08-31 11:04:43+00:00</td>\n      <td>P2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.078611</td>\n    </tr>\n    <tr>\n      <th>55026</th>\n      <td>2021-09-02 17:00:00+00:00</td>\n      <td>2021-09-02 17:02:45+00:00</td>\n      <td>P2</td>\n      <td>0.000000</td>\n      <td>0.003945</td>\n      <td>0.045833</td>\n    </tr>\n    <tr>\n      <th>55081</th>\n      <td>2021-09-06 16:00:00+00:00</td>\n      <td>2021-09-06 16:00:04+00:00</td>\n      <td>P2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.001111</td>\n    </tr>\n    <tr>\n      <th>55195</th>\n      <td>2021-09-11 21:00:00+00:00</td>\n      <td>2021-09-11 21:00:09+00:00</td>\n      <td>P2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <th>73247</th>\n      <td>2021-08-29 18:00:00+00:00</td>\n      <td>2021-08-29 18:00:38+00:00</td>\n      <td>P3</td>\n      <td>1.231030</td>\n      <td>0.248395</td>\n      <td>0.010556</td>\n    </tr>\n    <tr>\n      <th>73274</th>\n      <td>2021-08-31 11:00:00+00:00</td>\n      <td>2021-08-31 11:04:43+00:00</td>\n      <td>P3</td>\n      <td>4.823799</td>\n      <td>1.065423</td>\n      <td>0.078611</td>\n    </tr>\n    <tr>\n      <th>73323</th>\n      <td>2021-09-02 17:00:00+00:00</td>\n      <td>2021-09-02 17:02:45+00:00</td>\n      <td>P3</td>\n      <td>8.644030</td>\n      <td>1.113123</td>\n      <td>0.045833</td>\n    </tr>\n    <tr>\n      <th>73359</th>\n      <td>2021-09-06 16:00:00+00:00</td>\n      <td>2021-09-06 16:00:04+00:00</td>\n      <td>P3</td>\n      <td>0.158475</td>\n      <td>0.022923</td>\n      <td>0.001111</td>\n    </tr>\n    <tr>\n      <th>73426</th>\n      <td>2021-09-11 21:00:00+00:00</td>\n      <td>2021-09-11 21:00:09+00:00</td>\n      <td>P3</td>\n      <td>0.338265</td>\n      <td>0.056242</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <th>78214</th>\n      <td>2022-10-30 02:48:01+00:00</td>\n      <td>2022-10-30 02:01:47+00:00</td>\n      <td>P3</td>\n      <td>46.100000</td>\n      <td>6.515524</td>\n      <td>-0.770556</td>\n    </tr>\n    <tr>\n      <th>78215</th>\n      <td>2022-10-30 02:48:01+00:00</td>\n      <td>2022-10-30 02:01:47+00:00</td>\n      <td>P3</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.770556</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [139]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mload_sar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_folder\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBrice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mraw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSAR\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m             \u001B[49m\u001B[43minterval_logic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerge_overlapping_intervals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mc\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muber_paid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdistance_km\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m             \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msar\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_properties\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_time_properties\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m             \u001B[49m\u001B[43msave_at\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_folder\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBrice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresults\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [138]\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(periods, interval_logic, filtering_logic, time_properties, name, facets, save_at)\u001B[0m\n\u001B[1;32m     21\u001B[0m display(periods[periods\u001B[38;5;241m.\u001B[39mbegin\u001B[38;5;241m.\u001B[39misin(duplicates\u001B[38;5;241m.\u001B[39mbegin) \u001B[38;5;241m&\u001B[39m periods\u001B[38;5;241m.\u001B[39mend\u001B[38;5;241m.\u001B[39misin(duplicates\u001B[38;5;241m.\u001B[39mend)])\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Pivot table so that each there is a single line per interval and per granularity of interest\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m periods \u001B[38;5;241m=\u001B[39m \u001B[43mperiods\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpivot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbegin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstatus\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfacets\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[1;32m     25\u001B[0m periods[periods \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[1;32m     26\u001B[0m periods\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39mperiods\u001B[38;5;241m.\u001B[39mcolumns[periods\u001B[38;5;241m.\u001B[39misna()\u001B[38;5;241m.\u001B[39mall()], inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# this and previous remove columns that have only 0/nans\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/frame.py:7885\u001B[0m, in \u001B[0;36mDataFrame.pivot\u001B[0;34m(self, index, columns, values)\u001B[0m\n\u001B[1;32m   7880\u001B[0m \u001B[38;5;129m@Substitution\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   7881\u001B[0m \u001B[38;5;129m@Appender\u001B[39m(_shared_docs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpivot\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   7882\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpivot\u001B[39m(\u001B[38;5;28mself\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, values\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m   7883\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpivot\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pivot\n\u001B[0;32m-> 7885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpivot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:520\u001B[0m, in \u001B[0;36mpivot\u001B[0;34m(data, index, columns, values)\u001B[0m\n\u001B[1;32m    518\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    519\u001B[0m         indexed \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39m_constructor_sliced(data[values]\u001B[38;5;241m.\u001B[39m_values, index\u001B[38;5;241m=\u001B[39mmultiindex)\n\u001B[0;32m--> 520\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mindexed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns_listlike\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/frame.py:8428\u001B[0m, in \u001B[0;36mDataFrame.unstack\u001B[0;34m(self, level, fill_value)\u001B[0m\n\u001B[1;32m   8366\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   8367\u001B[0m \u001B[38;5;124;03mPivot a level of the (necessarily hierarchical) index labels.\u001B[39;00m\n\u001B[1;32m   8368\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   8424\u001B[0m \u001B[38;5;124;03mdtype: float64\u001B[39;00m\n\u001B[1;32m   8425\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   8426\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m unstack\n\u001B[0;32m-> 8428\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munstack\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   8430\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munstack\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:478\u001B[0m, in \u001B[0;36munstack\u001B[0;34m(obj, level, fill_value)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, DataFrame):\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mindex, MultiIndex):\n\u001B[0;32m--> 478\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_unstack_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    480\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mstack(dropna\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:505\u001B[0m, in \u001B[0;36m_unstack_frame\u001B[0;34m(obj, level, fill_value)\u001B[0m\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor(mgr)\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 505\u001B[0m     unstacker \u001B[38;5;241m=\u001B[39m \u001B[43m_Unstacker\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstructor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_constructor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m unstacker\u001B[38;5;241m.\u001B[39mget_result(\n\u001B[1;32m    507\u001B[0m         obj\u001B[38;5;241m.\u001B[39m_values, value_columns\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mcolumns, fill_value\u001B[38;5;241m=\u001B[39mfill_value\n\u001B[1;32m    508\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:140\u001B[0m, in \u001B[0;36m_Unstacker.__init__\u001B[0;34m(self, index, level, constructor)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_cells \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39miinfo(np\u001B[38;5;241m.\u001B[39mint32)\u001B[38;5;241m.\u001B[39mmax:\n\u001B[1;32m    134\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following operation may generate \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_cells\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m cells \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min the resulting pandas object.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    137\u001B[0m         PerformanceWarning,\n\u001B[1;32m    138\u001B[0m     )\n\u001B[0;32m--> 140\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_selectors\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:192\u001B[0m, in \u001B[0;36m_Unstacker._make_selectors\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    189\u001B[0m mask\u001B[38;5;241m.\u001B[39mput(selector, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex):\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndex contains duplicate entries, cannot reshape\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_index \u001B[38;5;241m=\u001B[39m comp_index\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask \u001B[38;5;241m=\u001B[39m mask\n",
      "\u001B[0;31mValueError\u001B[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "_ = pipeline(load_sar(data_folder / 'Brice' / 'raw' / 'SAR'),\n",
    "             interval_logic=lambda t: merge_overlapping_intervals(t, {c: 'sum' for c in ['uber_paid', 'distance_km']}),\n",
    "             name='sar', time_properties=all_time_properties,\n",
    "             save_at=data_folder / 'Brice' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "_ = pipeline(load_portal(data_folder / 'Guillaume' / 'raw' / 'Portal' / 'Driver'),\n",
    "             name='portal', time_properties=all_time_properties,\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             save_at=data_folder / 'Guillaume' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "_ = pipeline(load_portal(data_folder / 'Aria' / 'raw' / 'Portal' / 'Driver'),\n",
    "             name='portal', time_properties=select(all_time_properties, keep=['night', 'day_type']),\n",
    "             save_at=data_folder / 'Aria' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
