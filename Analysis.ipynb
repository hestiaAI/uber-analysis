{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Uber Driver Data Analysis\n",
    "\n",
    "(New version)\n",
    "\n",
    "Works with SAR and Portal data.\n",
    "\n",
    "Note that this notebook is not final, and more documentation is being added."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [Optional] Installing the required libraries with pip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (1.23.5)\r\n",
      "Requirement already satisfied: pandas in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: portion in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (from pandas) (2022.6)\r\n",
      "Requirement already satisfied: sortedcontainers~=2.2 in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (from portion) (2.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/lstreit/anaconda3/envs/hestia/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas portion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Table(pd.DataFrame):\n",
    "    \"\"\"Some Python magic to be able to type hint things like\n",
    "    df: Table['begin': datetime, 'end': datetime]\"\"\"\n",
    "\n",
    "    def __class_getitem__(cls, _):\n",
    "        return list[str]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Timestamp = datetime.datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "PeriodTable = Table['begin': Timestamp, 'end': Timestamp]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def find_file(pattern: str, folder: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Looks for a file matching the given pattern inside the given folder\n",
    "    :param pattern: a glob file pattern\n",
    "    :param folder: a path leading to a folder where the pattern should be applied\n",
    "    :return: a path to the first file matching the pattern, or a value error if none.\n",
    "    \"\"\"\n",
    "    matches = glob.glob(pattern, root_dir=folder)\n",
    "    if len(matches) == 0:\n",
    "        raise ValueError(f'Could not find file {pattern} in {folder}')\n",
    "    elif len(matches) > 1:\n",
    "        print(f'Found many matches for {pattern} in {folder}. Using the first.')\n",
    "    return folder / matches[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def find_table(pattern: str, folder: Path, usecols: Optional[list[str]] = None) -> Table:\n",
    "    \"\"\"\n",
    "    Looks for a csv-like file whose name matches the pattern and is located inside folder, using only the specified columns.\n",
    "    If found, reads it and assigns the file name as a column.\n",
    "    :param pattern: a glob file pattern\n",
    "    :param folder: a path leading to a folder where the pattern should be applied\n",
    "    :param usecols: an optional list of columns present in the data file, only they will be loaded\n",
    "    :return: a Table (a.k.a. pandas DataFrame) having the specified columns and the file name as a column.\n",
    "    \"\"\"\n",
    "    file = find_file(pattern, folder)\n",
    "    table = pd.read_csv(file, usecols=usecols)\n",
    "    if usecols is not None:\n",
    "        table = table[usecols]\n",
    "    return table.assign(file=file.name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def find_date_range(pattern: str, folder: Path, date_cols: list[str]) -> (Timestamp, Timestamp):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pattern: a glob file pattern\n",
    "    :param folder: a path leading to a folder where the pattern should be applied\n",
    "    :param date_cols: a list of column names that are dates\n",
    "    :return: the minimum and maximum observed dates\n",
    "    Usage:\n",
    "    find_date_range('*Driver Trip Status.csv', data_folder / 'Brice' / 'raw' / 'SAR',\n",
    "                    ['begin_timestamp_local', 'end_timestamp_local'])\n",
    "    \"\"\"\n",
    "    df = find_table(pattern, folder, usecols=date_cols)\n",
    "    for c in date_cols:\n",
    "        df[c] = pd.to_datetime(df[c])\n",
    "    return df[date_cols].min().min(), df[date_cols].max().max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently unused code\n",
    "\n",
    "def df_to_interval(df: PeriodTable) -> P.interval:\n",
    "    \"\"\"Converts a DataFrame with columns 'begin' and 'end' into a Portion interval, merging entries that overlap.\"\"\"\n",
    "    return P.Interval(*[P.closed(row['begin'], row['end']) for row in df.to_dict('records')])\n",
    "\n",
    "\n",
    "def interval_to_df(interval: P.interval) -> PeriodTable:\n",
    "    \"\"\"Converts a Portion interval into a dataframe with columns 'begin' and 'end'.\"\"\"\n",
    "    return pd.DataFrame([{'begin': begin, 'end': end} for _, begin, end, _ in P.to_data(interval)])\n",
    "\n",
    "def make_status_intervals(df: PeriodTable) -> dict[str, P.interval]:\n",
    "    return {s: df_to_interval(df[df.status == s]) for s in df.status.unique()}\n",
    "\n",
    "def interval_merge_logic(lt: dict[str, P.interval], oo: dict[str, P.interval]) -> dict[str, P.interval]:\n",
    "    P3 = lt['P3'] | oo['P3']\n",
    "    P2 = (lt['P2'] | oo['P2']) - P3\n",
    "    P1 = oo['P1'] - (P2 | P3)\n",
    "    return {'P1': P1, 'P2': P2, 'P3': P3}\n",
    "\n",
    "def main_interval_logic(lt: dict[str, P.interval], oo: dict[str, P.interval], P0_has_priority=False) -> Table:\n",
    "    \"\"\"Consider the following ordering of priorities: P3 > P2 > P1. P0_has_priority determines if P0 is on the left or right of inequalities.\"\"\"\n",
    "    if P0_has_priority:\n",
    "        for d in [lt, oo]:\n",
    "            for k in d.keys():\n",
    "                if k != 'P0':\n",
    "                    d[k] = d[k] - oo['P0']\n",
    "    lt['P2'] = lt['P2'] - lt['P3']\n",
    "    oo['P2'] = oo['P2'] - oo['P3']\n",
    "    oo['P1'] = oo['P1'] - (oo['P2'] | oo['P3'])\n",
    "    intervals = interval_merge_logic(lt, oo)\n",
    "    return pd.concat([interval_to_df(i).assign(status=f'{s} consistent') for s, i in intervals.items()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        table: Table['t1': Timestamp, 't2': Timestamp, 't3': Timestamp],\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    :param: table: a table having a number N > 1 of time-columns and L of entries.\n",
    "    :param: columns: a list of n time-column names present in {table}.\n",
    "    :param: extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    :return: periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Usage:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(table.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(table: PeriodTable, agg_dict: Optional[dict] = None) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Groups the given table by status, then sorts all intervals by begin datetime and merges overlapping entries \"efficiently\".\n",
    "    :param table: the table whose intervals should be merged\n",
    "    :param agg_dict: maps each row to the operation that should be applied to combine interval attributes when merging them\n",
    "    :return: a table with merged intervals per status\n",
    "    \"\"\"\n",
    "    agg_dict = agg_dict or {c: 'sum' for c in table.columns if c not in ['begin', 'end', 'status']}\n",
    "\n",
    "    def handle_groups(group: PeriodTable) -> PeriodTable:\n",
    "        intervals = group[['begin', 'end']].sort_values(['begin', 'end'])\n",
    "        group_number = 0\n",
    "        group_end = intervals.end.iloc[0]\n",
    "\n",
    "        def find_group(row):\n",
    "            nonlocal group_number, group_end\n",
    "            if row.begin <= group_end:\n",
    "                group_end = max(row.end, group_end)\n",
    "            else:\n",
    "                group_number += 1\n",
    "                group_end = row.end\n",
    "            return group_number\n",
    "\n",
    "        groups = intervals.apply(find_group, axis=1)\n",
    "        return table.groupby(groups).agg({'begin': 'min', 'end': 'max', **agg_dict}).reset_index(drop=True)\n",
    "\n",
    "    return table.groupby('status').apply(handle_groups).reset_index(level=1, drop=True).reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def mile2km(n_miles: float) -> float:\n",
    "    return n_miles * 1.609344"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SAR preprocessing logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def load_lifetime_trips(folder: Path, pattern: str = '*Driver Lifetime Trips.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status',\n",
    "                        'request_to_begin_distance_miles', 'trip_distance_miles', 'original_fare_local'])\n",
    "    table = table[table.status == 'completed'].drop(columns='status')\n",
    "    table.replace({r'\\N': np.nan}, inplace=True)\n",
    "    for col in ['request_to_begin_distance_miles', 'original_fare_local']:\n",
    "        table[col] = table[col].astype(float)\n",
    "    table = time_tuples_to_periods(table,\n",
    "                                   columns=['request_timestamp_local', 'begintrip_timestamp_local',\n",
    "                                            'dropoff_timestamp_local'],\n",
    "                                   extra_info=[lambda r: {'status': 'P2',\n",
    "                                                          'distance_km': mile2km(r['request_to_begin_distance_miles']),\n",
    "                                                          'file': r['file']},\n",
    "                                               lambda r: {'status': 'P3',\n",
    "                                                          'distance_km': mile2km(r['trip_distance_miles']),\n",
    "                                                          'uber_paid': r['original_fare_local'],\n",
    "                                                          'file': r['file']}])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def load_on_off(folder: Path, pattern: str = '*Driver Online Offline.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['begin_timestamp_local', 'end_timestamp_local', 'earner_state',\n",
    "                        'begin_lat', 'begin_lng', 'end_lat', 'end_lng'])\n",
    "    table.rename(columns={'begin_timestamp_local': 'begin', 'end_timestamp_local': 'end',\n",
    "                          'earner_state': 'status'}, inplace=True)\n",
    "    table = table.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def load_dispatches(folder: Path, pattern: str = 'TODO') -> Table:\n",
    "    # TODO (not finished)\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                        'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                        'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def load_trip_status(folder: Path, pattern: str = '*Driver Trip Status.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder, ['begin_timestamp_local', 'end_timestamp_local', 'status', 'end_reason'])\n",
    "    table.columns = ['begin', 'end', *table.columns[2:]]\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def load_distance_traveled(folder: Path, pattern: str = 'TODO') -> pd.DataFrame:\n",
    "    pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def load_sar(folder: Path) -> Table:\n",
    "    lifetime_trips = load_lifetime_trips(folder)\n",
    "    on_off = load_on_off(folder)\n",
    "    return pd.concat([lifetime_trips, on_off]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Portal preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def load_portal(folder: Path):\n",
    "    df = find_table('*driver_lifetime_trips*.csv', folder,\n",
    "                    ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "                     'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    df = df[df['Status'] == 'completed']\n",
    "    df = time_tuples_to_periods(df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                                             'Local Dropoff Timestamp'],\n",
    "                                extra_info=[lambda r: {'status': 'P2'},\n",
    "                                            lambda r: {'status': 'P3',\n",
    "                                                       'distance_km': mile2km(r['Trip Distance (miles)']),\n",
    "                                                       'uber_paid': r['Local Original Fare']}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Guillaume-specific logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: Table,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'Guillaume' / 'percentage.csv'\n",
    ") -> Table:\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    filtered = pd.DataFrame(index=daily.index)\n",
    "    filtered['datetime'] = pd.to_datetime(daily['date'])\n",
    "    duration_P1_cols = list(filter(lambda col: all(f in col for f in ['duration_h', 'P1']), daily.columns))\n",
    "    for c in duration_P1_cols:\n",
    "        for i, row in percentage.iterrows():\n",
    "            filtered[c] = np.where(\n",
    "                (filtered.datetime.dt.year == row.year) & (filtered.datetime.dt.month == row.month),\n",
    "                daily[c] * row['Uber'], daily[c])\n",
    "\n",
    "    def date_range(from_date: Tuple[int, ...], to_date: Tuple[int, ...]) -> list[datetime.date]:\n",
    "        return pd.date_range(datetime.date(*from_date), datetime.date(*to_date)).values\n",
    "\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26), *date_range((2020, 12, 21), (2020, 12, 25)),\n",
    "                     *date_range((2021, 2, 1), (2021, 2, 12)), *date_range((2021, 8, 16), (2021, 8, 28)),\n",
    "                     *date_range((2021, 9, 20), (2021, 10, 3)), *date_range((2021, 11, 25), (2021, 12, 12)),\n",
    "                     *date_range((2022, 4, 25), (2022, 5, 13))]\n",
    "    duration_P1_weekday_AM_cols = list(\n",
    "        filter(lambda col: all(f in col for f in ['duration_h', 'P1', 'AM', 'weekday']), daily.columns))\n",
    "    filtered.loc[\n",
    "        daily[~filtered.datetime.apply(lambda d: d.date()).isin(dates_to_keep)].index, duration_P1_weekday_AM_cols] = 0\n",
    "    return filtered[duration_P1_cols].rename(columns={col: f'{col}(filtered)' for col in duration_P1_cols})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def split_AM_PM(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many days or many morning/afternoon periods in two.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def scaled_interval(begin: Timestamp, end: Timestamp, attributes: dict, og_duration) -> dict:\n",
    "        \"\"\"Creates an interval (a dict with begin, end and other attributes) given the original attributes and the original \"\"\"\n",
    "        return {'begin': begin, 'end': end,\n",
    "                **{k: v * (end - begin) / og_duration if isinstance(v, float) else v for k, v in attributes.items()}}\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.day != end.day:\n",
    "            rows = [scaled_interval(begin, begin.replace(hour=23, minute=59, second=59), rest, og_duration)]\n",
    "            for days in range(end.day - begin.day - 1):\n",
    "                mid = begin + datetime.timedelta(days=days)\n",
    "                rows.append(scaled_interval(mid.replace(hour=0, minute=0, second=0),\n",
    "                                            mid.replace(hour=23, minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(hour=0, minute=0, second=0), end, rest, og_duration))\n",
    "            # Call itself recursively to split resulting days into AM/PM\n",
    "            return [e for r in rows for e in rec(**r)]\n",
    "        # Check if the interval spans many AM/PM periods\n",
    "        elif begin.hour < 12 <= end.hour:\n",
    "            return [scaled_interval(begin, end.replace(hour=11, minute=59, second=59), rest, og_duration),\n",
    "                    scaled_interval(end.replace(hour=12, minute=0, second=0), end, rest, og_duration)]\n",
    "        else:\n",
    "            return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def split_hours(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many hours periods in as many intervals as hours covered by the interval.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def scaled_interval(begin: Timestamp, end: Timestamp, attributes: dict, og_duration) -> dict:\n",
    "        \"\"\"Creates an interval (a dict with begin, end and other attributes) given the original attributes and the original \"\"\"\n",
    "        return {'begin': begin, 'end': end,\n",
    "                **{k: v * (end - begin) / og_duration if isinstance(v, float) else v for k, v in attributes.items()}}\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.hour != end.hour:\n",
    "            rows = [scaled_interval(begin, begin.replace(minute=59, second=59), rest, og_duration)]\n",
    "            for hours in range(end.hour - begin.hour - 1):\n",
    "                mid = begin + datetime.timedelta(hours=hours)\n",
    "                rows.append(scaled_interval(mid.replace(minute=0, second=0),\n",
    "                                            mid.replace(minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(minute=0, second=0), end, rest, og_duration))\n",
    "            return rows\n",
    "        return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def find_week_limits(date: Timestamp) -> str:\n",
    "    week_start = date - datetime.timedelta(days=date.weekday())\n",
    "    week_end = week_start + datetime.timedelta(days=6)\n",
    "    return f'{week_start.date()} to {week_end.date()}'.replace('-', '/').replace('to', '-')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def select(d: dict[str], keep: Optional[list[str]] = None, drop: Optional[list[str]] = None) -> dict[str]:\n",
    "    assert (keep is None) != (drop is None), 'Only one of keep or drop can be specified'\n",
    "    if keep is not None:\n",
    "        return {k: v for k, v in d.items() if k in keep}\n",
    "    if drop is not None:\n",
    "        return {k: v for k, v in d.items() if k not in drop}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running the pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "all_facets = ['duration_h', 'distance_km', 'uber_paid']\n",
    "all_time_properties = {'day_of_week': lambda d: d.day_name(),\n",
    "                       'day_type': lambda d: 'weekday' if d.weekday() < 5 else 'weekend',\n",
    "                       'time_of_day': lambda d: 'AM' if d.hour < 12 else 'PM',\n",
    "                       'night': lambda d: 'night' if d.hour <= 6 or 23 < d.hour else 'day'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: PeriodTable,\n",
    "        interval_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        filtering_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        time_properties: Optional[dict[str, Callable[[Timestamp], Any]]] = None,\n",
    "        name: str = 'analysis',\n",
    "        facets: list[str] = all_facets,\n",
    "        save_at: Path = data_folder / 'results',\n",
    "        compute_most_lucrative_months: bool = True,\n",
    ") -> Tuple[pd.DataFrame, ...]:\n",
    "    # Apply interval logic if specified\n",
    "    if interval_logic is not None:\n",
    "        periods = interval_logic(periods)\n",
    "\n",
    "    # Compute the duration of a period once, in the beginning\n",
    "    periods['duration_h'] = (periods.end - periods.begin) / datetime.timedelta(hours=1)\n",
    "    # Split intervals spanning many hours\n",
    "    periods = split_hours(periods)\n",
    "\n",
    "    # Pivot table so that each there is a single line per interval and per granularity of interest\n",
    "    periods = periods.pivot(index=['begin', 'end'], columns=['status'], values=facets).reset_index()\n",
    "    periods[periods == 0] = np.nan\n",
    "    periods.drop(columns=periods.columns[periods.isna().all()],\n",
    "                 inplace=True)  # this and previous remove columns that have only 0/nans\n",
    "    # Merges column multiindex into a single index by joining the levels\n",
    "    periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    date_info = list(time_properties.keys()) if time_properties is not None else []\n",
    "\n",
    "    # Compute these datetime properties since they will be the same for begin and end (thanks to split_hours)\n",
    "    if time_properties is not None:\n",
    "        for k, f in time_properties.items():\n",
    "            periods[k] = periods.end.apply(f)\n",
    "        periods = periods.pivot(index=['begin', 'end'], columns=date_info, values=list(agg_dict.keys())).reset_index()\n",
    "        periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    periods['hour'] = periods.end.apply(lambda d: f'{d.hour}-{(d + datetime.timedelta(hours=1)).hour}')\n",
    "    periods['date'] = periods.end.dt.date\n",
    "    periods['week'] = periods.end.apply(find_week_limits)\n",
    "    periods['month'] = periods.end.apply(lambda d: f'{d.month:02d}. {d.month_name()}')\n",
    "    periods['year'] = periods.end.dt.year\n",
    "\n",
    "    hourly = periods.groupby(['date', 'hour']).agg(agg_dict).reset_index()\n",
    "    daily = periods.groupby(['date', 'year', 'month', 'week']).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        filtered = filtering_logic(daily)\n",
    "        daily = pd.concat([daily, filtered], axis=1)\n",
    "        agg_dict = {**agg_dict, **{c: 'sum' for c in filtered.columns}}\n",
    "\n",
    "    weekly = daily.groupby(['week']).agg(agg_dict).reset_index()\n",
    "    monthly = daily.groupby(['year', 'month']).agg(agg_dict).reset_index()\n",
    "    yearly = daily.groupby(['year']).agg(agg_dict).reset_index()\n",
    "    total = yearly.agg(agg_dict).to_frame().T\n",
    "\n",
    "    if compute_most_lucrative_months:\n",
    "        most_lucrative_months: Table = monthly.copy()\n",
    "        most_lucrative_months['uber_paid_total'] = monthly[[c for c in monthly.columns if 'uber_paid' in c]].sum(axis=1)\n",
    "        most_lucrative_months.sort_values('uber_paid_total', ascending=False, inplace=True)\n",
    "        most_lucrative_months.to_csv(save_at / f'{name}_4_bis_most_lucrative_months.csv', index=False,\n",
    "                                     float_format='%.2f')\n",
    "\n",
    "    # Writing tables to disk\n",
    "    save_at.mkdir(parents=True, exist_ok=True)\n",
    "    periods.to_csv(save_at / f'{name}_0_periods.csv', index=False, float_format='%.2f')\n",
    "    hourly.to_csv(save_at / f'{name}_1_hourly.csv', index=False, float_format='%.2f')\n",
    "    daily.to_csv(save_at / f'{name}_2_daily.csv', index=False, float_format='%.2f')\n",
    "    weekly.to_csv(save_at / f'{name}_3_weekly.csv', index=False, float_format='%.2f')\n",
    "    monthly.to_csv(save_at / f'{name}_4_monthly.csv', index=False, float_format='%.2f')\n",
    "    yearly.to_csv(save_at / f'{name}_5_yearly.csv', index=False, float_format='%.2f')\n",
    "    total.to_csv(save_at / f'{name}_6_total.csv', index=False, float_format='%.2f')\n",
    "\n",
    "    return hourly, daily, weekly, yearly, total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "_ = pipeline(load_sar(data_folder / 'Guillaume' / 'raw' / 'SAR (new)'),\n",
    "             interval_logic=lambda t: merge_overlapping_intervals(t, {c: 'sum' for c in ['uber_paid', 'distance_km']}),\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             name='sar', time_properties=select(all_time_properties, keep=['time_of_day', 'day_type']),\n",
    "             save_at=data_folder / 'Guillaume' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "_ = pipeline(load_sar(data_folder / 'Kidane' / 'raw' / 'SAR'),\n",
    "             interval_logic=lambda t: merge_overlapping_intervals(t, {c: 'sum' for c in ['uber_paid', 'distance_km']}),\n",
    "             name='sar', time_properties=all_time_properties,\n",
    "             save_at=data_folder / 'Kidane' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mload_sar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_folder\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBrice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mraw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSAR\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m             \u001B[49m\u001B[43minterval_logic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerge_overlapping_intervals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mc\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muber_paid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdistance_km\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m             \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msar\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_properties\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_time_properties\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m             \u001B[49m\u001B[43msave_at\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_folder\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBrice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresults\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[25], line 21\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(periods, interval_logic, filtering_logic, time_properties, name, facets, save_at, compute_monthly_top_10)\u001B[0m\n\u001B[1;32m     18\u001B[0m periods \u001B[38;5;241m=\u001B[39m split_hours(periods)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Pivot table so that each there is a single line per interval and per granularity of interest\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m periods \u001B[38;5;241m=\u001B[39m \u001B[43mperiods\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpivot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbegin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstatus\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfacets\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[1;32m     22\u001B[0m periods[periods \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[1;32m     23\u001B[0m periods\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39mperiods\u001B[38;5;241m.\u001B[39mcolumns[periods\u001B[38;5;241m.\u001B[39misna()\u001B[38;5;241m.\u001B[39mall()],\n\u001B[1;32m     24\u001B[0m              inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# this and previous remove columns that have only 0/nans\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/frame.py:8564\u001B[0m, in \u001B[0;36mDataFrame.pivot\u001B[0;34m(self, index, columns, values)\u001B[0m\n\u001B[1;32m   8558\u001B[0m \u001B[38;5;129m@Substitution\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   8559\u001B[0m \u001B[38;5;129m@Appender\u001B[39m(_shared_docs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpivot\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   8560\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   8561\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpivot\u001B[39m(\u001B[38;5;28mself\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, values\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m   8562\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpivot\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pivot\n\u001B[0;32m-> 8564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpivot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:540\u001B[0m, in \u001B[0;36mpivot\u001B[0;34m(data, index, columns, values)\u001B[0m\n\u001B[1;32m    536\u001B[0m         indexed \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39m_constructor_sliced(data[values]\u001B[38;5;241m.\u001B[39m_values, index\u001B[38;5;241m=\u001B[39mmultiindex)\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;66;03m# \"Hashable\"\u001B[39;00m\n\u001B[0;32m--> 540\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mindexed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns_listlike\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/frame.py:9109\u001B[0m, in \u001B[0;36mDataFrame.unstack\u001B[0;34m(self, level, fill_value)\u001B[0m\n\u001B[1;32m   9047\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   9048\u001B[0m \u001B[38;5;124;03mPivot a level of the (necessarily hierarchical) index labels.\u001B[39;00m\n\u001B[1;32m   9049\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   9105\u001B[0m \u001B[38;5;124;03mdtype: float64\u001B[39;00m\n\u001B[1;32m   9106\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   9107\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m unstack\n\u001B[0;32m-> 9109\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munstack\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   9111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munstack\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:476\u001B[0m, in \u001B[0;36munstack\u001B[0;34m(obj, level, fill_value)\u001B[0m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, DataFrame):\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mindex, MultiIndex):\n\u001B[0;32m--> 476\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_unstack_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    478\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mstack(dropna\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:499\u001B[0m, in \u001B[0;36m_unstack_frame\u001B[0;34m(obj, level, fill_value)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_unstack_frame\u001B[39m(obj: DataFrame, level, fill_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    498\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mindex, MultiIndex)  \u001B[38;5;66;03m# checked by caller\u001B[39;00m\n\u001B[0;32m--> 499\u001B[0m     unstacker \u001B[38;5;241m=\u001B[39m \u001B[43m_Unstacker\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstructor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_constructor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_can_fast_transpose:\n\u001B[1;32m    502\u001B[0m         mgr \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39munstack(unstacker, fill_value\u001B[38;5;241m=\u001B[39mfill_value)\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:137\u001B[0m, in \u001B[0;36m_Unstacker.__init__\u001B[0;34m(self, index, level, constructor)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_cells \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39miinfo(np\u001B[38;5;241m.\u001B[39mint32)\u001B[38;5;241m.\u001B[39mmax:\n\u001B[1;32m    130\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following operation may generate \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_cells\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m cells \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min the resulting pandas object.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    133\u001B[0m         PerformanceWarning,\n\u001B[1;32m    134\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    135\u001B[0m     )\n\u001B[0;32m--> 137\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_selectors\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hestia/lib/python3.11/site-packages/pandas/core/reshape/reshape.py:189\u001B[0m, in \u001B[0;36m_Unstacker._make_selectors\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    186\u001B[0m mask\u001B[38;5;241m.\u001B[39mput(selector, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex):\n\u001B[0;32m--> 189\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndex contains duplicate entries, cannot reshape\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_index \u001B[38;5;241m=\u001B[39m comp_index\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask \u001B[38;5;241m=\u001B[39m mask\n",
      "\u001B[0;31mValueError\u001B[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "_ = pipeline(load_sar(data_folder / 'Brice' / 'raw' / 'SAR'),\n",
    "             interval_logic=lambda t: merge_overlapping_intervals(t, {c: 'sum' for c in ['uber_paid', 'distance_km']}),\n",
    "             name='sar', time_properties=all_time_properties,\n",
    "             save_at=data_folder / 'Brice' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "_ = pipeline(load_portal(data_folder / 'Guillaume' / 'raw' / 'Portal' / 'Driver'),\n",
    "             name='portal', time_properties=all_time_properties,\n",
    "             filtering_logic=guillaume_filtering_logic,\n",
    "             save_at=data_folder / 'Guillaume' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "_ = pipeline(load_portal(data_folder / 'Aria' / 'raw' / 'Portal' / 'Driver'),\n",
    "             name='portal', time_properties=select(all_time_properties, keep=['night', 'day_type']),\n",
    "             save_at=data_folder / 'Aria' / 'results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
