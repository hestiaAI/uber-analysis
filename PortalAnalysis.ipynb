{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyser les données portail, et générer la demande SAR\n",
    "\n",
    "Le chauffeur Uber doit avoir demandé ses données par l'application uber. Il le fait typiquement fait à sa première visite à la permanence, tel que décrit dans notre [mode d'emploi](https://hestiaai.fibery.io/Uber_driver_onboarding/Things-to-know-for-a-permanence-166?sharing-key=a6582d33-5ea4-44e1-be2a-cb37486cbaec) \n",
    "\n",
    "S'il nous a déjà transmis ses données, elles se trouvent dans le kdrive sous  *kDrive/Common\\ documents/HestiaLabs/Projects/Mobility/Uber\\ driver\\ data/data/01-method*\n",
    "\n",
    "Il faut dezipper ses données dans le dossier *portal-unzipped* qui se trouve à côté de ce notebook.\n",
    "Les données des chauffeurs. \n",
    "\n",
    "Si on a copié un fichier zip *XXX-Uber Data XXXX.zip* dans le dossier *portal-unzipped*, on peut par exemple le dézipper ainsi:\n",
    "```sh\n",
    "cd portal-unzipped\n",
    "unzip 'XXX-Uber Data XXXX.zip'\n",
    "```\n",
    "\n",
    "Exécuter toutes les cellules du notebook en cliquant sur le bouton aux deux triangles >> et cliquer sur le bouton rouge \n",
    "\n",
    "Le texte du SAR se trouve au bas de la page. Il faut le copier dans la demande faite selon les instructions de la  *Méthode 2) plus précise: Soumettre une demande de renseignements sur la confidentialité par le biais du formulaire d'Uber* ici:\n",
    "https://personaldata.io/nos-donnees-nos-projets/mobilite/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Installing the required libraries with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in /home/andre/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: portion in /home/andre/.local/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: openpyxl in /home/andre/.local/lib/python3.10/site-packages (3.0.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/andre/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: sortedcontainers~=2.2 in /home/andre/.local/lib/python3.10/site-packages (from portion) (2.4.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/andre/.local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas portion openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        table: Table['t1': Timestamp, 't2': Timestamp, 't3': Timestamp],\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    :param: table: a table having a number N > 1 of time-columns and L of entries.\n",
    "    :param: columns: a list of n time-column names present in {table}.\n",
    "    :param: extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    :return: periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Usage:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(table.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(table: PeriodTable, agg_dict: Optional[dict] = None) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Groups the given table by status, then sorts all intervals by begin datetime and merges overlapping entries \"efficiently\".\n",
    "    :param table: the table whose intervals should be merged\n",
    "    :param agg_dict: maps each row to the operation that should be applied to combine interval attributes when merging them\n",
    "    :return: a table with merged intervals per status\n",
    "    \"\"\"\n",
    "    agg_dict = agg_dict or {c: 'sum' for c in table.columns if c not in ['begin', 'end', 'status']}\n",
    "\n",
    "    def handle_groups(group: PeriodTable) -> PeriodTable:\n",
    "        intervals = group[['begin', 'end']].sort_values(['begin', 'end'])\n",
    "        group_number = 0\n",
    "        group_end = intervals.end.iloc[0]\n",
    "\n",
    "        def find_group(row):\n",
    "            nonlocal group_number, group_end\n",
    "            if row.begin <= group_end:\n",
    "                group_end = max(row.end, group_end)\n",
    "            else:\n",
    "                group_number += 1\n",
    "                group_end = row.end\n",
    "            return group_number\n",
    "\n",
    "        groups = intervals.apply(find_group, axis=1)\n",
    "        return table.groupby(groups).agg({'begin': 'min', 'end': 'max', **agg_dict}).reset_index(drop=True)\n",
    "\n",
    "    return table.groupby('status').apply(handle_groups).reset_index(level=1, drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hours(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many hours periods in as many intervals as hours covered by the interval.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.hour != end.hour:\n",
    "            rows = [scaled_interval(begin, begin.replace(minute=59, second=59), rest, og_duration)]\n",
    "            for hours in range(end.hour - begin.hour - 1):\n",
    "                mid = begin + datetime.timedelta(hours=hours)\n",
    "                rows.append(scaled_interval(mid.replace(minute=0, second=0),\n",
    "                                            mid.replace(minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(minute=0, second=0), end, rest, og_duration))\n",
    "            return rows\n",
    "        return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAR preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lifetime_trips(folder: Path, pattern: str = '*Driver Lifetime Trips.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status',\n",
    "                        'request_to_begin_distance_miles', 'trip_distance_miles', 'original_fare_local'])\n",
    "    table = table[table.status == 'completed'].drop(columns='status')\n",
    "    table.replace({r'\\N': np.nan}, inplace=True)\n",
    "    for col in ['request_to_begin_distance_miles', 'original_fare_local']:\n",
    "        table[col] = table[col].astype(float)\n",
    "    table = time_tuples_to_periods(table,\n",
    "                                   columns=['request_timestamp_local', 'begintrip_timestamp_local',\n",
    "                                            'dropoff_timestamp_local'],\n",
    "                                   extra_info=[lambda r: {'status': 'P2',\n",
    "                                                          'distance_km': mile2km(r['request_to_begin_distance_miles']),\n",
    "                                                          'file': r['file']},\n",
    "                                               lambda r: {'status': 'P3',\n",
    "                                                          'distance_km': mile2km(r['trip_distance_miles']),\n",
    "                                                          'uber_paid': r['original_fare_local'],\n",
    "                                                          'file': r['file']}])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_on_off(folder: Path, pattern: str = '*Driver Online Offline.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['begin_timestamp_local', 'end_timestamp_local', 'earner_state',\n",
    "                        'begin_lat', 'begin_lng', 'end_lat', 'end_lng'])\n",
    "    table.rename(columns={'begin_timestamp_local': 'begin', 'end_timestamp_local': 'end',\n",
    "                          'earner_state': 'status'}, inplace=True)\n",
    "    table = table.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dispatches(folder: Path, pattern: str = 'TODO') -> Table:\n",
    "    # TODO (not finished)\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                        'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                        'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trip_status(folder: Path, pattern: str = '*Driver Trip Status.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder, ['begin_timestamp_local', 'end_timestamp_local', 'status', 'end_reason'])\n",
    "    table.columns = ['begin', 'end', *table.columns[2:]]\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sar(folder: Path) -> Table:\n",
    "    lifetime_trips = load_lifetime_trips(folder)\n",
    "    on_off = load_on_off(folder)\n",
    "    return pd.concat([lifetime_trips, on_off]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_portal(folder: Path):\n",
    "    df = find_table('*driver_lifetime_trips*.csv', folder,\n",
    "                    ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "                     'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    df = df[df['Status'] == 'completed']\n",
    "    df = time_tuples_to_periods(df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                                             'Local Dropoff Timestamp'],\n",
    "                                extra_info=[lambda r: {'status': 'P2'},\n",
    "                                            lambda r: {'status': 'P3',\n",
    "                                                       'distance_km': mile2km(r['Trip Distance (miles)']),\n",
    "                                                       'uber_paid': r['Local Original Fare']}])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guillaume-specific logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: Table,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'Guillaume' / 'percentage.csv'\n",
    ") -> Table:\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    filtered = pd.DataFrame(index=daily.index)\n",
    "    filtered['datetime'] = pd.to_datetime(daily['date'])\n",
    "    duration_P1_cols = list(filter(lambda col: all(f in col for f in ['duration_h', 'P1']), daily.columns))\n",
    "    for c in duration_P1_cols:\n",
    "        for i, row in percentage.iterrows():\n",
    "            filtered[c] = np.where(\n",
    "                (filtered.datetime.dt.year == row.year) & (filtered.datetime.dt.month == row.month),\n",
    "                daily[c] * row['Uber'], daily[c])\n",
    "\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26), *date_range((2020, 12, 21), (2020, 12, 25)),\n",
    "                     *date_range((2021, 2, 1), (2021, 2, 12)), *date_range((2021, 8, 16), (2021, 8, 28)),\n",
    "                     *date_range((2021, 9, 20), (2021, 10, 3)), *date_range((2021, 11, 25), (2021, 12, 12)),\n",
    "                     *date_range((2022, 4, 25), (2022, 5, 13))]\n",
    "    duration_P1_weekday_AM_cols = list(\n",
    "        filter(lambda col: all(f in col for f in ['duration_h', 'P1', 'AM', 'weekday']), daily.columns))\n",
    "    filtered.loc[\n",
    "        daily[~filtered.datetime.apply(lambda d: d.date()).isin(dates_to_keep)].index, duration_P1_weekday_AM_cols] = 0\n",
    "    return filtered[duration_P1_cols].rename(columns={col: f'{col}(filtered)' for col in duration_P1_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_facets = ['duration_h', 'distance_km', 'uber_paid']\n",
    "all_time_properties = {'day_of_week': lambda d: d.day_name(),\n",
    "                       'day_type': lambda d: 'weekday' if d.weekday() < 5 else 'weekend',\n",
    "                       'time_of_day': lambda d: 'AM' if d.hour < 12 else 'PM',\n",
    "                       'night': lambda d: 'night' if d.hour <= 6 or 23 < d.hour else 'day'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: PeriodTable,\n",
    "        interval_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        filtering_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        time_properties: Optional[dict[str, Callable[[Timestamp], Any]]] = None,\n",
    "        name: str = 'analysis',\n",
    "        facets: list[str] = all_facets,\n",
    "        save_folder: Path = data_folder / 'results',\n",
    "        compute_most_lucrative_months: bool = True,\n",
    ") -> dict[str, Table]:\n",
    "    # Apply interval logic if specified\n",
    "    if interval_logic is not None:\n",
    "        periods = interval_logic(periods)\n",
    "\n",
    "    # Compute the duration of a period once, in the beginning\n",
    "    periods['duration_h'] = (periods.end - periods.begin) / datetime.timedelta(hours=1)\n",
    "    # Split intervals spanning many hours\n",
    "    periods = split_hours(periods)\n",
    "\n",
    "    # Pivot table so that each there is a single line per interval and per granularity of interest\n",
    "    periods = periods.pivot(index=['begin', 'end'], columns=['status'], values=facets).reset_index()\n",
    "    periods[periods == 0] = np.nan\n",
    "    periods.drop(columns=periods.columns[periods.isna().all()],\n",
    "                 inplace=True)  # this and previous remove columns that have only 0/nans\n",
    "    # Merges column multiindex into a single index by joining the levels\n",
    "    periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    date_info = list(time_properties.keys()) if time_properties is not None else []\n",
    "\n",
    "    # Compute these datetime properties since they will be the same for begin and end (thanks to split_hours)\n",
    "    if time_properties is not None:\n",
    "        for k, f in time_properties.items():\n",
    "            periods[k] = periods.end.apply(f)\n",
    "        periods = periods.pivot(index=['begin', 'end'], columns=date_info, values=list(agg_dict.keys())).reset_index()\n",
    "        periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    periods['hour'] = periods.end.apply(lambda d: f'{d.hour}-{(d + datetime.timedelta(hours=1)).hour}')\n",
    "    periods['date'] = periods.end.dt.date\n",
    "    periods['week'] = periods.end.apply(find_week_limits)\n",
    "    periods['month'] = periods.end.apply(lambda d: f'{d.month:02d}. {d.month_name()}')\n",
    "    periods['year'] = periods.end.dt.year\n",
    "\n",
    "    tabs = {'periods': periods,\n",
    "            'hourly': periods.groupby(['date', 'hour']).agg(agg_dict).reset_index(),\n",
    "            'daily': periods.groupby(['date', 'year', 'month', 'week']).agg(agg_dict).reset_index()}\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        filtered = filtering_logic(tabs['daily'])\n",
    "        tabs['daily'] = pd.concat([tabs['daily'], filtered], axis=1)\n",
    "        agg_dict = {**agg_dict, **{c: 'sum' for c in filtered.columns}}\n",
    "\n",
    "    tabs['weekly'] = tabs['daily'].groupby(['week']).agg(agg_dict).reset_index()\n",
    "    tabs['monthly'] = tabs['daily'].groupby(['year', 'month']).agg(agg_dict).reset_index()\n",
    "    tabs['yearly'] = tabs['daily'].groupby(['year']).agg(agg_dict).reset_index()\n",
    "    tabs['total'] = tabs['yearly'].agg(agg_dict).to_frame().T\n",
    "\n",
    "    if compute_most_lucrative_months:\n",
    "        df = tabs['monthly'].copy()\n",
    "        df['uber_paid_total'] = tabs['monthly'][[c for c in tabs['monthly'].columns if 'uber_paid' in c]].sum(axis=1)\n",
    "        df = df.sort_values('uber_paid_total', ascending=False)\n",
    "        tabs['most_lucrative_months'] = df\n",
    "        request_months = list(df.iloc[:10].sort_values(['year', 'month']).apply(\n",
    "            lambda r: f'{french_months[int(r.month.split(\".\")[0])]} {r.year}', axis=1))\n",
    "        request_months += list(\n",
    "            filter(lambda s: s not in request_months, [f'{french_months[i]} 2020' for i in range(3, 12 + 1)]))\n",
    "        complete_sar_text = sar_text.replace('{REPLACE_HERE}', ', '.join(request_months))\n",
    "        print(complete_sar_text)\n",
    "        return complete_sar_text\n",
    "       # with open(save_folder / 'sar_request_text.txt', 'w') as f:\n",
    "       #     f.write(sar_text.replace('{REPLACE_HERE}', ', '.join(request_months)))\n",
    "\n",
    "    #save_folder.mkdir(parents=True, exist_ok=True)\n",
    "    #save_excel(save_folder / f'{name}_results.xlsx', tabs, index=False, float_format='%.2f')\n",
    "\n",
    "    return tabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_data_folder = Path(os.getcwd()) / 'portal-unzipped' / 'Uber Data' / 'Driver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_results_folder = Path(os.getcwd()) / 'portal-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find file *driver_lifetime_trips*.csv in /home/andre/workspace/uber-analysis/portal-unzipped/Uber Data/Driver",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline(\u001b[43mload_portal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mportal_data_folder\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      2\u001b[0m          name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportal\u001b[39m\u001b[38;5;124m'\u001b[39m, time_properties\u001b[38;5;241m=\u001b[39mall_time_properties,\n\u001b[1;32m      3\u001b[0m          save_folder\u001b[38;5;241m=\u001b[39mportal_results_folder)\n",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m, in \u001b[0;36mload_portal\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_portal\u001b[39m(folder: Path):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfind_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*driver_lifetime_trips*.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStatus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocal Request Timestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBegin Trip Local Timestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocal Dropoff Timestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrip Distance (miles)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDuration (Seconds)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocal Original Fare\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m time_tuples_to_periods(df, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocal Request Timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBegin Trip Local Timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocal Dropoff Timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m                                 extra_info\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mlambda\u001b[39;00m r: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP2\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      9\u001b[0m                                             \u001b[38;5;28;01mlambda\u001b[39;00m r: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_km\u001b[39m\u001b[38;5;124m'\u001b[39m: mile2km(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrip Distance (miles)\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     11\u001b[0m                                                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muber_paid\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocal Original Fare\u001b[39m\u001b[38;5;124m'\u001b[39m]}])\n",
      "File \u001b[0;32m~/workspace/uber-analysis/utils.py:34\u001b[0m, in \u001b[0;36mfind_table\u001b[0;34m(pattern, folder, usecols)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_table\u001b[39m(pattern: \u001b[38;5;28mstr\u001b[39m, folder: Path, usecols: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Looks inside folder for a csv-like file whose name matches the pattern, and only reads the specified columns.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    If found, reads it and assigns the file name as a column.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    :return: a Table (a.k.a. pandas DataFrame) having the specified columns and the file name as a column.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mfind_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file, usecols\u001b[38;5;241m=\u001b[39musecols)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m usecols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/uber-analysis/utils.py:19\u001b[0m, in \u001b[0;36mfind_file\u001b[0;34m(pattern, folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m matches \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(pattern, root_dir\u001b[38;5;241m=\u001b[39mfolder)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound many matches for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Using the first.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find file *driver_lifetime_trips*.csv in /home/andre/workspace/uber-analysis/portal-unzipped/Uber Data/Driver"
     ]
    }
   ],
   "source": [
    "pipeline(load_portal(portal_data_folder),\n",
    "         name='portal', time_properties=all_time_properties,\n",
    "         save_folder=portal_results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
