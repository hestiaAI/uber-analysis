{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyser les données portail, et générer la demande SAR\n",
    "\n",
    "Le chauffeur Uber doit avoir demandé ses données par l'application uber. Il le fait typiquement fait à sa première visite à la permanence, tel que décrit dans notre [mode d'emploi](https://hestiaai.fibery.io/Uber_driver_onboarding/Things-to-know-for-a-permanence-166?sharing-key=a6582d33-5ea4-44e1-be2a-cb37486cbaec) \n",
    "\n",
    "S'il nous a déjà transmis ses données, elles se trouvent dans le kdrive sous  *kDrive/Common\\ documents/HestiaLabs/Projects/Mobility/Uber\\ driver\\ data/data/01-method*\n",
    "\n",
    "Il faut dezipper ses données dans le dossier *portal-unzipped* qui se trouve à côté de ce notebook.\n",
    "Les données des chauffeurs. \n",
    "\n",
    "Si on a copié un fichier zip *XXX-Uber Data XXXX.zip* dans le dossier *portal-unzipped*, on peut par exemple le dézipper ainsi:\n",
    "```sh\n",
    "cd portal-unzipped\n",
    "unzip 'XXX-Uber Data XXXX.zip'\n",
    "```\n",
    "\n",
    "Exécuter toutes les cellules du notebook en cliquant sur le bouton aux deux triangles >> et cliquer sur le bouton rouge \n",
    "\n",
    "Le texte du SAR se trouve au bas de la page. Il faut le copier dans la demande faite selon les instructions de la  *Méthode 2) plus précise: Soumettre une demande de renseignements sur la confidentialité par le biais du formulaire d'Uber* ici:\n",
    "https://personaldata.io/nos-donnees-nos-projets/mobilite/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Installing the required libraries with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy pandas portion openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Any\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        table: Table['t1': Timestamp, 't2': Timestamp, 't3': Timestamp],\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    :param: table: a table having a number N > 1 of time-columns and L of entries.\n",
    "    :param: columns: a list of n time-column names present in {table}.\n",
    "    :param: extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    :return: periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Usage:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(table.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(table: PeriodTable, agg_dict: Optional[dict] = None) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Groups the given table by status, then sorts all intervals by begin datetime and merges overlapping entries \"efficiently\".\n",
    "    :param table: the table whose intervals should be merged\n",
    "    :param agg_dict: maps each row to the operation that should be applied to combine interval attributes when merging them\n",
    "    :return: a table with merged intervals per status\n",
    "    \"\"\"\n",
    "    agg_dict = agg_dict or {c: 'sum' for c in table.columns if c not in ['begin', 'end', 'status']}\n",
    "\n",
    "    def handle_groups(group: PeriodTable) -> PeriodTable:\n",
    "        intervals = group[['begin', 'end']].sort_values(['begin', 'end'])\n",
    "        group_number = 0\n",
    "        group_end = intervals.end.iloc[0]\n",
    "\n",
    "        def find_group(row):\n",
    "            nonlocal group_number, group_end\n",
    "            if row.begin <= group_end:\n",
    "                group_end = max(row.end, group_end)\n",
    "            else:\n",
    "                group_number += 1\n",
    "                group_end = row.end\n",
    "            return group_number\n",
    "\n",
    "        groups = intervals.apply(find_group, axis=1)\n",
    "        return table.groupby(groups).agg({'begin': 'min', 'end': 'max', **agg_dict}).reset_index(drop=True)\n",
    "\n",
    "    return table.groupby('status').apply(handle_groups).reset_index(level=1, drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hours(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many hours periods in as many intervals as hours covered by the interval.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.hour != end.hour:\n",
    "            rows = [scaled_interval(begin, begin.replace(minute=59, second=59), rest, og_duration)]\n",
    "            for hours in range(end.hour - begin.hour - 1):\n",
    "                mid = begin + datetime.timedelta(hours=hours)\n",
    "                rows.append(scaled_interval(mid.replace(minute=0, second=0),\n",
    "                                            mid.replace(minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(minute=0, second=0), end, rest, og_duration))\n",
    "            return rows\n",
    "        return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAR preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_lifetime_trips(zf: ZipFile, pattern: str = '*Driver Lifetime Trips.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, zf,\n",
    "                       ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status',\n",
    "                        'request_to_begin_distance_miles', 'trip_distance_miles', 'original_fare_local'])\n",
    "    table = table[table.status == 'completed'].drop(columns='status')\n",
    "    table.replace({r'\\N': np.nan}, inplace=True)\n",
    "    for col in ['request_to_begin_distance_miles', 'original_fare_local']:\n",
    "        table[col] = table[col].astype(float)\n",
    "    table = time_tuples_to_periods(\n",
    "        table,\n",
    "        columns=['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local'],\n",
    "        extra_info=[\n",
    "            lambda r: {'status': 'P2', 'distance_km': mile2km(r['request_to_begin_distance_miles']), 'file': r['file']},\n",
    "            lambda r: {'status': 'P3', 'distance_km': mile2km(r['trip_distance_miles']),\n",
    "                       'uber_paid': r['original_fare_local'], 'file': r['file']}])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_on_off(zf: ZipFile, pattern: str = '*Driver Online Offline.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, zf,\n",
    "                       ['begin_timestamp_local', 'end_timestamp_local', 'earner_state',\n",
    "                        'begin_lat', 'begin_lng', 'end_lat', 'end_lng'])\n",
    "    table.rename(columns={'begin_timestamp_local': 'begin', 'end_timestamp_local': 'end',\n",
    "                          'earner_state': 'status'}, inplace=True)\n",
    "    table = table.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_dispatches(zf: ZipFile, pattern: str = 'TODO') -> Table:\n",
    "    # TODO (not finished)\n",
    "    table = find_table(pattern, zf,\n",
    "                       ['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                        'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                        'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_trip_status(zf: ZipFile, pattern: str = '*Driver Trip Status.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, zf, ['begin_timestamp_local', 'end_timestamp_local', 'status', 'end_reason'])\n",
    "    table.columns = ['begin', 'end', *table.columns[2:]]\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_sar(zip_path: Path) -> Table:\n",
    "    print(f'Zip open')\n",
    "    with ZipFile(zip_path) as zf:\n",
    "        lifetime_trips = load_lifetime_trips(zf)\n",
    "        on_off = load_on_off(zf)\n",
    "    print(f'Zip closed')\n",
    "    return pd.concat([lifetime_trips, on_off]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Portal preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_portal(zip_path: Path):\n",
    "    with ZipFile(zip_path) as zf:\n",
    "        df = find_table('*driver_lifetime_trips*.csv', zf,\n",
    "                        ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "                         'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    df = df[df['Status'] == 'completed']\n",
    "    df = time_tuples_to_periods(\n",
    "        df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp'],\n",
    "        extra_info=[lambda r: {'status': 'P2'},\n",
    "                    lambda r: {'status': 'P3', 'distance_km': mile2km(r['Trip Distance (miles)']),\n",
    "                               'uber_paid': r['Local Original Fare']}])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guillaume-specific logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: Table,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'Guillaume' / 'percentage.csv'\n",
    ") -> Table:\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    filtered = pd.DataFrame(index=daily.index)\n",
    "    filtered['datetime'] = pd.to_datetime(daily['date'])\n",
    "    duration_P1_cols = list(filter(lambda col: all(f in col for f in ['duration_h', 'P1']), daily.columns))\n",
    "    for c in duration_P1_cols:\n",
    "        for i, row in percentage.iterrows():\n",
    "            filtered[c] = np.where(\n",
    "                (filtered.datetime.dt.year == row.year) & (filtered.datetime.dt.month == row.month),\n",
    "                daily[c] * row['Uber'], daily[c])\n",
    "\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26), *date_range((2020, 12, 21), (2020, 12, 25)),\n",
    "                     *date_range((2021, 2, 1), (2021, 2, 12)), *date_range((2021, 8, 16), (2021, 8, 28)),\n",
    "                     *date_range((2021, 9, 20), (2021, 10, 3)), *date_range((2021, 11, 25), (2021, 12, 12)),\n",
    "                     *date_range((2022, 4, 25), (2022, 5, 13))]\n",
    "    duration_P1_weekday_AM_cols = list(\n",
    "        filter(lambda col: all(f in col for f in ['duration_h', 'P1', 'AM', 'weekday']), daily.columns))\n",
    "    filtered.loc[\n",
    "        daily[~filtered.datetime.apply(lambda d: d.date()).isin(dates_to_keep)].index, duration_P1_weekday_AM_cols] = 0\n",
    "    return filtered[duration_P1_cols].rename(columns={col: f'{col}(filtered)' for col in duration_P1_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_facets = ['duration_h', 'distance_km', 'uber_paid']\n",
    "all_time_properties = {'day_of_week': lambda d: d.day_name(),\n",
    "                       'day_type': lambda d: 'weekday' if d.weekday() < 5 else 'weekend',\n",
    "                       'time_of_day': lambda d: 'AM' if d.hour < 12 else 'PM',\n",
    "                       'night': lambda d: 'night' if d.hour <= 6 or 23 < d.hour else 'day'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: PeriodTable,\n",
    "        interval_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        filtering_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        time_properties: Optional[dict[str, Callable[[Timestamp], Any]]] = None,\n",
    "        name: str = 'analysis',\n",
    "        facets: list[str] = all_facets,\n",
    "        save_folder: Path = data_folder / 'results',\n",
    "        compute_most_lucrative_months: bool = True,\n",
    ") -> dict[str, Table]:\n",
    "    # Apply interval logic if specified\n",
    "    if interval_logic is not None:\n",
    "        periods = interval_logic(periods)\n",
    "\n",
    "    # Compute the duration of a period once, in the beginning\n",
    "    periods['duration_h'] = (periods.end - periods.begin) / datetime.timedelta(hours=1)\n",
    "    # Split intervals spanning many hours\n",
    "    periods = split_hours(periods)\n",
    "\n",
    "    # Pivot table so that each there is a single line per interval and per granularity of interest\n",
    "    periods = periods.pivot(index=['begin', 'end'], columns=['status'], values=facets).reset_index()\n",
    "    periods[periods == 0] = np.nan\n",
    "    periods.drop(columns=periods.columns[periods.isna().all()],\n",
    "                 inplace=True)  # this and previous remove columns that have only 0/nans\n",
    "    # Merges column multiindex into a single index by joining the levels\n",
    "    periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    date_info = list(time_properties.keys()) if time_properties is not None else []\n",
    "\n",
    "    # Compute these datetime properties since they will be the same for begin and end (thanks to split_hours)\n",
    "    if time_properties is not None:\n",
    "        for k, f in time_properties.items():\n",
    "            periods[k] = periods.end.apply(f)\n",
    "        periods = periods.pivot(index=['begin', 'end'], columns=date_info, values=list(agg_dict.keys())).reset_index()\n",
    "        periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    periods['hour'] = periods.end.apply(lambda d: f'{d.hour}-{(d + datetime.timedelta(hours=1)).hour}')\n",
    "    periods['date'] = periods.end.dt.date\n",
    "    periods['week'] = periods.end.apply(find_week_limits)\n",
    "    periods['month'] = periods.end.apply(lambda d: f'{d.month:02d}. {d.month_name()}')\n",
    "    periods['year'] = periods.end.dt.year\n",
    "\n",
    "    tabs = {'periods': periods,\n",
    "            'hourly': periods.groupby(['date', 'hour']).agg(agg_dict).reset_index(),\n",
    "            'daily': periods.groupby(['date', 'year', 'month', 'week']).agg(agg_dict).reset_index()}\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        filtered = filtering_logic(tabs['daily'])\n",
    "        tabs['daily'] = pd.concat([tabs['daily'], filtered], axis=1)\n",
    "        agg_dict = {**agg_dict, **{c: 'sum' for c in filtered.columns}}\n",
    "\n",
    "    tabs['weekly'] = tabs['daily'].groupby(['week']).agg(agg_dict).reset_index()\n",
    "    tabs['monthly'] = tabs['daily'].groupby(['year', 'month']).agg(agg_dict).reset_index()\n",
    "    tabs['yearly'] = tabs['daily'].groupby(['year']).agg(agg_dict).reset_index()\n",
    "    tabs['total'] = tabs['yearly'].agg(agg_dict).to_frame().T\n",
    "\n",
    "    if compute_most_lucrative_months:\n",
    "        df = tabs['monthly'].copy()\n",
    "        df['uber_paid_total'] = tabs['monthly'][[c for c in tabs['monthly'].columns if 'uber_paid' in c]].sum(axis=1)\n",
    "        df = df.sort_values('uber_paid_total', ascending=False)\n",
    "        tabs['most_lucrative_months'] = df\n",
    "        request_months = list(df.iloc[:10].sort_values(['year', 'month']).apply(\n",
    "            lambda r: f'{french_months[int(r.month.split(\".\")[0])]} {r.year}', axis=1))\n",
    "        request_months += list(\n",
    "            filter(lambda s: s not in request_months, [f'{french_months[i]} 2020' for i in range(3, 12 + 1)]))\n",
    "        complete_sar_text = sar_text.replace('{REPLACE_HERE}', ', '.join(request_months))\n",
    "        print(complete_sar_text)\n",
    "        return complete_sar_text\n",
    "    # with open(save_folder / 'sar_request_text.txt', 'w') as f:\n",
    "    #     f.write(sar_text.replace('{REPLACE_HERE}', ', '.join(request_months)))\n",
    "\n",
    "    #save_folder.mkdir(parents=True, exist_ok=True)\n",
    "    #save_excel(save_folder / f'{name}_results.xlsx', tabs, index=False, float_format='%.2f')\n",
    "\n",
    "    return tabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_data_folder = Path(os.getcwd()) / 'portal-unzipped' / 'Uber Data' / 'Driver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_results_folder = Path(os.getcwd()) / 'portal-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline(load_portal(portal_data_folder),\n",
    "         name='portal', time_properties=all_time_properties,\n",
    "         save_folder=portal_results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
