{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyser les données portail, et générer la demande SAR\n",
    "\n",
    "Le chauffeur Uber doit avoir demandé ses données par l'application uber. Il le fait typiquement fait à sa première visite à la permanence, tel que décrit dans notre [mode d'emploi](https://hestiaai.fibery.io/Uber_driver_onboarding/Things-to-know-for-a-permanence-166?sharing-key=a6582d33-5ea4-44e1-be2a-cb37486cbaec) \n",
    "\n",
    "S'il nous a déjà transmis ses données, elles se trouvent dans le kdrive sous  *kDrive/Common\\ documents/HestiaLabs/Projects/Mobility/Uber\\ driver\\ data/data/01-method*\n",
    "\n",
    "Il faut dezipper ses données dans le dossier *portal-unzipped* qui se trouve à côté de ce notebook.\n",
    "Les données des chauffeurs. \n",
    "\n",
    "Si on a copié un fichier zip *XXX-Uber Data XXXX.zip* dans le dossier *portal-unzipped*, on peut par exemple le dézipper ainsi:\n",
    "```sh\n",
    "cd portal-unzipped\n",
    "unzip 'XXX-Uber Data XXXX.zip'\n",
    "```\n",
    "\n",
    "Exécuter toutes les cellules du notebook en cliquant sur le bouton aux deux triangles >> et cliquer sur le bouton rouge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Installing the required libraries with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in /home/andre/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: portion in /home/andre/.local/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: openpyxl in /home/andre/.local/lib/python3.10/site-packages (3.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/andre/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: sortedcontainers~=2.2 in /home/andre/.local/lib/python3.10/site-packages (from portion) (2.4.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/andre/.local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas portion openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()) / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tuples_to_periods(\n",
    "        table: Table['t1': Timestamp, 't2': Timestamp, 't3': Timestamp],\n",
    "        columns: list[str],\n",
    "        extra_info: list[Callable[[pd.Series], dict]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe where each row has N timestamps corresponding to instants of status changes,\n",
    "    and converts each row into N-1 rows of periods in the corresponding status.\n",
    "\n",
    "    :param: table: a table having a number N > 1 of time-columns and L of entries.\n",
    "    :param: columns: a list of n time-column names present in {table}.\n",
    "    :param: extra_info: a list of functions taking a row of df and outputting a dictionary of additional information. Cannot have keys 'begin' and 'end'.\n",
    "    :return: periods: a table having L * (N-1) entries, each with a 'begin' and 'end' timestamp and associated information as specified by additional_info.\n",
    "    Usage:\n",
    "    df = pd.DataFrame([{'request_ts': '3:47 PM', 'begintrip_ts': '4:00 PM', 'dropoff_ts': '4:13 PM'}])\n",
    "    columns = ['request_ts', 'begintrip_ts', 'dropoff_ts']\n",
    "    extra_info = [lambda r: {'status': 'P2'}, lambda r: {'status': 'P3'}]\n",
    "    time_tuples_to_periods(df, columns, extra_info)\n",
    "    > begin    end      status\n",
    "    > 3:47 PM  4:00 PM  P2\n",
    "    > 4:00 PM  4:13 PM  P3\n",
    "    \"\"\"\n",
    "    assert len(columns) == len(\n",
    "        extra_info) + 1, f'The length of additional information should correspond to the number of generated periods (N-1).'\n",
    "    periods = pd.DataFrame(table.apply(\n",
    "        lambda r: [{'begin': r[b], 'end': r[e], **d(r)} for b, e, d in zip(columns, columns[1:], extra_info)],\n",
    "        axis=1\n",
    "    ).explode().to_list())\n",
    "    for col in ['begin', 'end']:\n",
    "        periods[col] = pd.to_datetime(periods[col])\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_intervals(table: PeriodTable, agg_dict: Optional[dict] = None) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Groups the given table by status, then sorts all intervals by begin datetime and merges overlapping entries \"efficiently\".\n",
    "    :param table: the table whose intervals should be merged\n",
    "    :param agg_dict: maps each row to the operation that should be applied to combine interval attributes when merging them\n",
    "    :return: a table with merged intervals per status\n",
    "    \"\"\"\n",
    "    agg_dict = agg_dict or {c: 'sum' for c in table.columns if c not in ['begin', 'end', 'status']}\n",
    "\n",
    "    def handle_groups(group: PeriodTable) -> PeriodTable:\n",
    "        intervals = group[['begin', 'end']].sort_values(['begin', 'end'])\n",
    "        group_number = 0\n",
    "        group_end = intervals.end.iloc[0]\n",
    "\n",
    "        def find_group(row):\n",
    "            nonlocal group_number, group_end\n",
    "            if row.begin <= group_end:\n",
    "                group_end = max(row.end, group_end)\n",
    "            else:\n",
    "                group_number += 1\n",
    "                group_end = row.end\n",
    "            return group_number\n",
    "\n",
    "        groups = intervals.apply(find_group, axis=1)\n",
    "        return table.groupby(groups).agg({'begin': 'min', 'end': 'max', **agg_dict}).reset_index(drop=True)\n",
    "\n",
    "    return table.groupby('status').apply(handle_groups).reset_index(level=1, drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hours(table: PeriodTable) -> PeriodTable:\n",
    "    \"\"\"\n",
    "    Splits intervals spanning many hours periods in as many intervals as hours covered by the interval.\n",
    "    If the interval is associated to numerical values (like distance or money), these values are\n",
    "    transferred to the new intervals but are weighted according to the new intervals' duration.\n",
    "    :param table: the table whose intervals should be split\n",
    "    :return: a table with no intervals spanning over AM and PM\n",
    "    \"\"\"\n",
    "\n",
    "    def rec(begin: Timestamp, end: Timestamp, **rest) -> list[dict]:\n",
    "        og_duration = end - begin\n",
    "        # Check if the interval spans many days, and split into as many days as it spans\n",
    "        if begin.hour != end.hour:\n",
    "            rows = [scaled_interval(begin, begin.replace(minute=59, second=59), rest, og_duration)]\n",
    "            for hours in range(end.hour - begin.hour - 1):\n",
    "                mid = begin + datetime.timedelta(hours=hours)\n",
    "                rows.append(scaled_interval(mid.replace(minute=0, second=0),\n",
    "                                            mid.replace(minute=59, second=59), rest, og_duration))\n",
    "            rows.append(scaled_interval(end.replace(minute=0, second=0), end, rest, og_duration))\n",
    "            return rows\n",
    "        return [{'begin': begin, 'end': end, **rest}]\n",
    "\n",
    "    return pd.DataFrame([e for d in table.to_dict('records') for e in rec(**d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAR preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lifetime_trips(folder: Path, pattern: str = '*Driver Lifetime Trips.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['request_timestamp_local', 'begintrip_timestamp_local', 'dropoff_timestamp_local', 'status',\n",
    "                        'request_to_begin_distance_miles', 'trip_distance_miles', 'original_fare_local'])\n",
    "    table = table[table.status == 'completed'].drop(columns='status')\n",
    "    table.replace({r'\\N': np.nan}, inplace=True)\n",
    "    for col in ['request_to_begin_distance_miles', 'original_fare_local']:\n",
    "        table[col] = table[col].astype(float)\n",
    "    table = time_tuples_to_periods(table,\n",
    "                                   columns=['request_timestamp_local', 'begintrip_timestamp_local',\n",
    "                                            'dropoff_timestamp_local'],\n",
    "                                   extra_info=[lambda r: {'status': 'P2',\n",
    "                                                          'distance_km': mile2km(r['request_to_begin_distance_miles']),\n",
    "                                                          'file': r['file']},\n",
    "                                               lambda r: {'status': 'P3',\n",
    "                                                          'distance_km': mile2km(r['trip_distance_miles']),\n",
    "                                                          'uber_paid': r['original_fare_local'],\n",
    "                                                          'file': r['file']}])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_on_off(folder: Path, pattern: str = '*Driver Online Offline.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['begin_timestamp_local', 'end_timestamp_local', 'earner_state',\n",
    "                        'begin_lat', 'begin_lng', 'end_lat', 'end_lng'])\n",
    "    table.rename(columns={'begin_timestamp_local': 'begin', 'end_timestamp_local': 'end',\n",
    "                          'earner_state': 'status'}, inplace=True)\n",
    "    table = table.replace({r'\\N': np.nan, 'ontrip': 'P3', 'enroute': 'P2', 'open': 'P1', 'offline': 'P0'})\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dispatches(folder: Path, pattern: str = 'TODO') -> Table:\n",
    "    # TODO (not finished)\n",
    "    table = find_table(pattern, folder,\n",
    "                       ['start_timestamp_local', 'end_timestamp_local', 'dispatches', 'completed_trips',\n",
    "                        'accepts', 'rejects', 'expireds', 'driver_cancellations', 'rider_cancellations',\n",
    "                        'minutes_online', 'minutes_on_trip', 'trip_fares'])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trip_status(folder: Path, pattern: str = '*Driver Trip Status.csv') -> PeriodTable:\n",
    "    table = find_table(pattern, folder, ['begin_timestamp_local', 'end_timestamp_local', 'status', 'end_reason'])\n",
    "    table.columns = ['begin', 'end', *table.columns[2:]]\n",
    "    for col in ['begin', 'end']:\n",
    "        table[col] = pd.to_datetime(table[col])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sar(folder: Path) -> Table:\n",
    "    lifetime_trips = load_lifetime_trips(folder)\n",
    "    on_off = load_on_off(folder)\n",
    "    return pd.concat([lifetime_trips, on_off]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_portal(folder: Path):\n",
    "    df = find_table('*driver_lifetime_trips*.csv', folder,\n",
    "                    ['Status', 'Local Request Timestamp', 'Begin Trip Local Timestamp', 'Local Dropoff Timestamp',\n",
    "                     'Trip Distance (miles)', 'Duration (Seconds)', 'Local Original Fare'])\n",
    "    df = df[df['Status'] == 'completed']\n",
    "    df = time_tuples_to_periods(df, columns=['Local Request Timestamp', 'Begin Trip Local Timestamp',\n",
    "                                             'Local Dropoff Timestamp'],\n",
    "                                extra_info=[lambda r: {'status': 'P2'},\n",
    "                                            lambda r: {'status': 'P3',\n",
    "                                                       'distance_km': mile2km(r['Trip Distance (miles)']),\n",
    "                                                       'uber_paid': r['Local Original Fare']}])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guillaume-specific logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guillaume_filtering_logic(\n",
    "        daily: Table,\n",
    "        percentage_df_path: Optional[str | Path] = data_folder / 'Guillaume' / 'percentage.csv'\n",
    ") -> Table:\n",
    "    # First, weight P1 times based on the percentage that Guillaume was working for Uber on that month\n",
    "    percentage = pd.read_csv(percentage_df_path)\n",
    "    percentage['Uber'] /= 100\n",
    "    filtered = pd.DataFrame(index=daily.index)\n",
    "    filtered['datetime'] = pd.to_datetime(daily['date'])\n",
    "    duration_P1_cols = list(filter(lambda col: all(f in col for f in ['duration_h', 'P1']), daily.columns))\n",
    "    for c in duration_P1_cols:\n",
    "        for i, row in percentage.iterrows():\n",
    "            filtered[c] = np.where(\n",
    "                (filtered.datetime.dt.year == row.year) & (filtered.datetime.dt.month == row.month),\n",
    "                daily[c] * row['Uber'], daily[c])\n",
    "\n",
    "    # Second, remove all morning weekday entries when Guillaume was working for IMAD, except for the specific dates below\n",
    "    dates_to_keep = [datetime.date(2020, 11, 26), *date_range((2020, 12, 21), (2020, 12, 25)),\n",
    "                     *date_range((2021, 2, 1), (2021, 2, 12)), *date_range((2021, 8, 16), (2021, 8, 28)),\n",
    "                     *date_range((2021, 9, 20), (2021, 10, 3)), *date_range((2021, 11, 25), (2021, 12, 12)),\n",
    "                     *date_range((2022, 4, 25), (2022, 5, 13))]\n",
    "    duration_P1_weekday_AM_cols = list(\n",
    "        filter(lambda col: all(f in col for f in ['duration_h', 'P1', 'AM', 'weekday']), daily.columns))\n",
    "    filtered.loc[\n",
    "        daily[~filtered.datetime.apply(lambda d: d.date()).isin(dates_to_keep)].index, duration_P1_weekday_AM_cols] = 0\n",
    "    return filtered[duration_P1_cols].rename(columns={col: f'{col}(filtered)' for col in duration_P1_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_facets = ['duration_h', 'distance_km', 'uber_paid']\n",
    "all_time_properties = {'day_of_week': lambda d: d.day_name(),\n",
    "                       'day_type': lambda d: 'weekday' if d.weekday() < 5 else 'weekend',\n",
    "                       'time_of_day': lambda d: 'AM' if d.hour < 12 else 'PM',\n",
    "                       'night': lambda d: 'night' if d.hour <= 6 or 23 < d.hour else 'day'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(\n",
    "        periods: PeriodTable,\n",
    "        interval_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        filtering_logic: Optional[Callable[[PeriodTable], PeriodTable]] = None,\n",
    "        time_properties: Optional[dict[str, Callable[[Timestamp], Any]]] = None,\n",
    "        name: str = 'analysis',\n",
    "        facets: list[str] = all_facets,\n",
    "        save_folder: Path = data_folder / 'results',\n",
    "        compute_most_lucrative_months: bool = True,\n",
    ") -> dict[str, Table]:\n",
    "    # Apply interval logic if specified\n",
    "    if interval_logic is not None:\n",
    "        periods = interval_logic(periods)\n",
    "\n",
    "    # Compute the duration of a period once, in the beginning\n",
    "    periods['duration_h'] = (periods.end - periods.begin) / datetime.timedelta(hours=1)\n",
    "    # Split intervals spanning many hours\n",
    "    periods = split_hours(periods)\n",
    "\n",
    "    # Pivot table so that each there is a single line per interval and per granularity of interest\n",
    "    periods = periods.pivot(index=['begin', 'end'], columns=['status'], values=facets).reset_index()\n",
    "    periods[periods == 0] = np.nan\n",
    "    periods.drop(columns=periods.columns[periods.isna().all()],\n",
    "                 inplace=True)  # this and previous remove columns that have only 0/nans\n",
    "    # Merges column multiindex into a single index by joining the levels\n",
    "    periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    date_info = list(time_properties.keys()) if time_properties is not None else []\n",
    "\n",
    "    # Compute these datetime properties since they will be the same for begin and end (thanks to split_hours)\n",
    "    if time_properties is not None:\n",
    "        for k, f in time_properties.items():\n",
    "            periods[k] = periods.end.apply(f)\n",
    "        periods = periods.pivot(index=['begin', 'end'], columns=date_info, values=list(agg_dict.keys())).reset_index()\n",
    "        periods.columns = periods.columns.map(lambda t: '.'.join(t) if t[1] else t[0])\n",
    "\n",
    "    agg_dict = {c: 'sum' for c in periods.columns if any(f in c for f in facets)}\n",
    "\n",
    "    periods['hour'] = periods.end.apply(lambda d: f'{d.hour}-{(d + datetime.timedelta(hours=1)).hour}')\n",
    "    periods['date'] = periods.end.dt.date\n",
    "    periods['week'] = periods.end.apply(find_week_limits)\n",
    "    periods['month'] = periods.end.apply(lambda d: f'{d.month:02d}. {d.month_name()}')\n",
    "    periods['year'] = periods.end.dt.year\n",
    "\n",
    "    tabs = {'periods': periods,\n",
    "            'hourly': periods.groupby(['date', 'hour']).agg(agg_dict).reset_index(),\n",
    "            'daily': periods.groupby(['date', 'year', 'month', 'week']).agg(agg_dict).reset_index()}\n",
    "\n",
    "    # Filter the data if a filtering function is specified\n",
    "    if filtering_logic is not None:\n",
    "        filtered = filtering_logic(tabs['daily'])\n",
    "        tabs['daily'] = pd.concat([tabs['daily'], filtered], axis=1)\n",
    "        agg_dict = {**agg_dict, **{c: 'sum' for c in filtered.columns}}\n",
    "\n",
    "    tabs['weekly'] = tabs['daily'].groupby(['week']).agg(agg_dict).reset_index()\n",
    "    tabs['monthly'] = tabs['daily'].groupby(['year', 'month']).agg(agg_dict).reset_index()\n",
    "    tabs['yearly'] = tabs['daily'].groupby(['year']).agg(agg_dict).reset_index()\n",
    "    tabs['total'] = tabs['yearly'].agg(agg_dict).to_frame().T\n",
    "\n",
    "    if compute_most_lucrative_months:\n",
    "        df = tabs['monthly'].copy()\n",
    "        df['uber_paid_total'] = tabs['monthly'][[c for c in tabs['monthly'].columns if 'uber_paid' in c]].sum(axis=1)\n",
    "        df = df.sort_values('uber_paid_total', ascending=False)\n",
    "        tabs['most_lucrative_months'] = df\n",
    "        request_months = list(df.iloc[:10].sort_values(['year', 'month']).apply(\n",
    "            lambda r: f'{french_months[int(r.month.split(\".\")[0])]} {r.year}', axis=1))\n",
    "        request_months += list(\n",
    "            filter(lambda s: s not in request_months, [f'{french_months[i]} 2020' for i in range(3, 12 + 1)]))\n",
    "        print(sar_text.replace('{REPLACE_HERE}', ', '.join(request_months)))\n",
    "       # with open(save_folder / 'sar_request_text.txt', 'w') as f:\n",
    "       #     f.write(sar_text.replace('{REPLACE_HERE}', ', '.join(request_months)))\n",
    "\n",
    "    #save_folder.mkdir(parents=True, exist_ok=True)\n",
    "    #save_excel(save_folder / f'{name}_results.xlsx', tabs, index=False, float_format='%.2f')\n",
    "\n",
    "    return tabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find SAR samples in the KDrive at:\n",
    "- old: `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/Lemoine_SAR_06.08.2022.zip`.\n",
    "- new: `PersonalData.IO /Lemoine_12102022-20221110T164542Z-001.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Portal sample can be found on our KDrive at `hestiaai /Common documents/HestiaLabs/PDIO- Data/Driver Data/Guillaume data/Lemoine Guillaume/202207/Uber Data F0699B53.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_data_folder = Path(os.getcwd()) / 'portal-unzipped' / 'Uber Data' / 'Driver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "portal_results_folder = Path(os.getcwd()) / 'portal-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour,\n",
      "Je m'adresse à vous pour vous demander de directement relayer mon message auprès du Responsable de la Protection des Données d'Uber, Simon Hania, comme anticipé par le Règlement Général sur la Protection des Données européen. J'utilise pour ce faire un formulaire mis à disposition de ceux qui n'ont pas de compte Uber, mais qui me semble être ma meilleure chance de contacter directement Simon Hania, étant donnés donnés les multiples problèmes que vos services d'aide présentent.\n",
      "En effet, je conduis pour Uber et cherche à obtenir une copie de mes données personnelles. La page https://help.uber.com/fr-CA/driving-and-delivering/article/demander-vos-donn%C3%A9es-personnelles-uber?nodeId=fbf08e68-65ba-456b-9bc6-1369eb9d2c44 m'informe que mes données sont accessibles via le tableau de bord partenaire. Cependant, comme décrit à\n",
      "https://forum.personaldata.io/t/transparence-sur-les-donnees-personnelles-chez-uber/307\n",
      "la transparence offerte n'est pas suffisante à mon goût. Votre page d'information m'invite à contacter le Responsable dans ce cas, ce que je fais maintenant.\n",
      "Je cherche donc par la présente à exercer mes droits prévus par le RGPD. Ceci inclut mon droit d'accès (Art 15), mon droit à la portabilité (Art 20). Je vous rappelle que tout violation des dispositions concernant les droits des personnes (Art 12 à 22) peuvent faire l'objet - en vertu de l'Article 83 - d'amendes administratives pouvant s'élever jusqu'à 20 000 000 EUR ou, dans le cas d'une entreprise, jusqu'à 4 % du chiffre d'affaires annuel mondial total de l'exercice précédent, le montant le plus élevé étant retenu.\n",
      "\n",
      "Copie de mes données personnelles\n",
      "=================================\n",
      "Cette requête couvre toutes mes données personnelles, et en particulier celles concernant:\n",
      "- ma géolocalisation (y compris les empreintes temporelles associées, et les données d'accéléromètre);\n",
      "- mes revenus;\n",
      "- les interactions partenaires;\n",
      "- les interactions clients à mon propos, dont commentaires et notes;\n",
      "- les “expériences” que Uber a mis en place et dont j'étais sujet;\n",
      "- les contrats, chartes et règles d'utilisation pour lesquels j'ai marqué mon accord, sous leurs différentes versions, et dates associées;\n",
      "- les notifications par email ou \"push\" qui m'ont été envoyées, ainsi que mes interactions avec celles-ci;\n",
      "- les offres qui m'ont été envoyées, ainsi que mes interactions avec celles-ci;\n",
      "- toute cote de performance, délivrée par Uber ou des clients, jointement ou séparément;\n",
      "- mon téléphone (y compris: niveau de batterie, système d'exploitation, adresse IP, etc);\n",
      "- mon véhicule;\n",
      "- mon accueil comme partneraire Uber;\n",
      "- le dispatching et matching des courses pour lesquelles j'ai été retenu;\n",
      "- les courses que j'ai effectuées;\n",
      "- les tickets internes Zendesk du service partenaires me concernant moi ou mes courses;\n",
      "- les tickets internes Zendesk du service clients me concernant moi ou mes courses;\n",
      "- les \"tags\" des services clients et partenaires me concernant moi ou mes courses;\n",
      "- discussions internes à mon propos;\n",
      "- toute déconnection, temporaire ou permanente;\n",
      "- mes documents d'identité, d'assurances, de validation, etc, y compris ce qui en a été extrait automatiquement;\n",
      "- toute donnée de profilage;\n",
      "- la géolocalisation, les empreintes temporelles et les données d'accéléromètre du téléphone des clients lorsque nous nous trouvions simultanément dans mon véhicule.\n",
      "\n",
      "Concernant les données de géolocalisation, je voudrais obtenir l'entièreté de ces données détenues par Uber. Néanmoins mes confrères m'informent que Uber restreint artificiellement sa réponse et n'inclut que le mois le plus récent, pour une protéger les \"rights and freedoms of others\". Uber suggère alors de préciser des périodes additionnelles qu'elle évaluera alors. Dans le but d'accélérer ce processus, et tout en acceptant pas cette limitation artificielle de mes droits par Uber, je demande en particulier mes données de géolocalisation pour les périodes couvrant décembre 2018, août 2019, décembre 2019, juin 2021, août 2021, février 2022, mars 2022, mai 2022, octobre 2022, novembre 2022, mars 2020, avril 2020, mai 2020, juin 2020, juillet 2020, août 2020, septembre 2020, octobre 2020, novembre 2020, décembre 2020.\n",
      "\n",
      "Je vous informe de l'existence de Guidelines de l'European Data Protection Board sur le Right of Access (https://edpb.europa.eu/system/files/2022-01/edpb_guidelines_012022_right-of-access_0.pdf ). Ces guidelines incluent différentes affirmations, telles que celles-ci: \"Art. 15(4) GDPR should not result in a refusal to provide all information to the data subject.  This means, for example, where the limitation applies, that information concerning others has to be rendered illegible as far as possible instead of rejecting to provide a copy of the personal data.\" ou encore \"he controller must be able to demonstrate that in the concrete situation rights or freedoms of others would factually be impacted.\" En conséquence Uber se doit de justifier son refus de rendre toutes les données de géolocalisation, et de proposer des méthodes alternatives.\n",
      "\n",
      "Article 20\n",
      "----------\n",
      "Pour les données tombant sous le coup du droit à la portabilité (RGPD Article 20), ce qui inclut toutes les données fournies par moi (Article 29 Working Party, *Guidelines on the Right to Data Portability (WP 242)*, 13 Décembre 2016, version française) et où la base juridique de traitement est le consentement ou le contrat, je souhaite:\n",
      "\n",
      "- **recevoir ces données dans un format structuré, couramment utilisé et lisible par machine**\n",
      "- accompagné avec un **description intelligible de toutes les variables**\n",
      "\n",
      "Article 15\n",
      "----------\n",
      "Pour les données tombant sous le coup du droit d'accès (RGPD Article 15), je voudrais **qu'une copie me soit envoyée dans un format électronique**. Veuillez noter que les données qui vous sont disponibles dans un format lisible par une machine doivent m'être fournies sous cette forme aussi, au vu des principes de loyauté (RGPD Article 5.1.a) et de protection des données dès la conception.\n",
      "\n",
      "Veuillez noter que toute opinion, déduction, préférence etc sont considérées comme des données personnelles (voir le Cas C‑434/16 *Peter Nowak v Data Protection Commissioner* [2017] ECLI:EU:C:2017:994, 34.)\n",
      "\n",
      "J'ai bien conscience que certaines des données que je demande concernent aussi des clients ou des employés de Uber. Je vous rappelle que dans ce cas il incombe à Uber de mettre en place des mesures pour protéger les droits de ces personnes, mais que cet argument ne peut être utilisé pour me refuser l'accès total à mes données. Pour des données de senseurs des appareils des clients lorsque ceux-ci se trouvent dans ma voiture par exemple, vous pourriez introduire un léger bruit dans les données pour masquer toute caractéristique individuelle de l'appareil.\n",
      "\n",
      "Si vous me considérez comme responsable du traitement\n",
      "-----------------------------------------------------\n",
      "De plus, si vous me considérez comme responsable du traitement de certaines de ces données et que vous agisseriez alors comme sous-traitant, veuillez me fournir alors toutes les donnéess que vous traitez en mon nom dans un format lisible par une machine, en accord avec votre obligation de respecter ma volonté sur les moyens et finalités du traitement.\n",
      "\n",
      "\n",
      "Métadonnées sur le traitement\n",
      "=============================\n",
      "\n",
      "Cette requête concerne aussi les métadonnées du traitement auxquelles j'ai droit suivant le RGPD.\n",
      "\n",
      "Information sur les responsables du traitement, sous-traitants, sources et transferts\n",
      "-------------------------------------------------------------------------------------\n",
      "Je voudrais de plus connaître\n",
      "- L'identité de tous les responsables conjoints du traitement, ainsi que grandes lignes de vos accords avec eux (RGPD Article 26).\n",
      "- Tout **destinataire à qui vous avez révélé des données**, nommées et avec leurs informations de contact en vertu de l'Article 15(1)(c). Veuillez noter que les régulateurs européens ont affirmé que par défaut les responsables de traitement doivent nommer les destinataires et pas les \"catégories\" de destinataires. Ils affirment de plus: \"Si les responsables du traitement choisissent de communiquer les catégories de destinataires, les informations devraient être les plus spécifiques possible et indiquer le type de destinataire (en fonction des activités qu’il mène), l’industrie, le secteur et le sous-secteur ainsi que l’emplacement destinataires.\" (Article 29 Working Party, ‘Guidelines on Transparency under Regulation 2016/679’ WP260 rev.01, 11 April 2018 ) Ils affirment de plus que la notion de \"destinataire\" est plus large que celle de \"tiers\", et inclut \"les autres responsables du traitement, responsables conjoints du traitement et sous-traitants auxquels les données sont transférées ou communiquée\". L'article 4 paragraphe 9 définit de plus les destinataires comme incluant toute autorité publique. Veuillez noter de plus que pour toute donnée transférée sur base du consentement, on ne peut nommer simplement la catégorie sans invalider la base juridique (Article 29 Working Party, ‘Guidelines on Consent under Regulation 2016/679’ (WP259 rev.01, 10 April 2018) 13).\n",
      "- Si tout donnée n'a pas été collectée, observée ou déduite de moi directement, je voudrais de plus obtenir des informations précises sur la **source de cette donnée**, y compris le nom et l'adresse email de contact du responsable de traitement (\"from which source the personal data originate\", Article 14(2)(f)/15(1)(g)).\n",
      "- Veuilez de plus confirmer où mes données personnelles (y compris les backups) sont stockées, et au moins si elles ont quitté l'Union Européenne (et si oui, veuillez me fournir les détails des bases légales et protections pour de tels transferts).\n",
      "\n",
      "Information sur les finalités et les bases juridiques\n",
      "-----------------------------------------------------\n",
      "- Toutes les **finalités de traitement et les bases juridiques pour ces finalités par catégorie de donnée personnelle** Cette liste doit être fournie par finalité, base juridique détaillée par finalité, et catégorie de données détaillée par finalité et base juridique. Des listes séparées sans alignement entre ces trois facteurs ne seraient pas acceptables (Article 29 Working Party, ‘Guidelines on Transparency under Regulation 2016/679’ (WP260 rev.01, 11 April 2018), page 35.). Il se peut qu'une table soit la meilleure manière de fournir cette information.\n",
      "\n",
      "- Les intérêts légitimes détaillés là où cette base juridique est utilisée (Article 14(2)(b)).\n",
      "\n",
      "Information sur les prises de décision automatisées\n",
      "---------------------------------------------------\n",
      "- Veuillez confirmer si vous avez recours ou pas à des prises de décisions automatisées (au sens de l'Article 22, RGPD). Si la réponse est oui, veuillez fournir des informations utiles sur la logique sous-jacente, ainsi que l'importance et les conséquences prévues de ce traitement pour moi (Article 15(1)(h))\n",
      "\n",
      "Information sur la conservation\n",
      "-------------------------------\n",
      "- Veuillez me confirmer pendant combien de temps chaque catégorie de données est conservée, ainsi que les critères utilisés pour prendre cette décision, en accord avec le principe de limitation de la conservation, et l'Article 15(1)(d).\n",
      "\n",
      "Je me réjouis de recevoir très vite un accusé de réception, et une réponse plus complète endéans les 30 jours, comme anticipé par le RGPD.\n"
     ]
    }
   ],
   "source": [
    "_ = pipeline(load_portal(portal_data_folder),\n",
    "             name='portal', time_properties=all_time_properties,\n",
    "             save_folder=portal_results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
